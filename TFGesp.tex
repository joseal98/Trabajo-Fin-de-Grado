\documentclass[11pt,a4paper,twoside,pdf]{article}

% Paquetes (añade otros si los necesitas):
\usepackage{latexsym}
\usepackage[utf8x]{inputenc}
\usepackage{soul}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{marvosym}
\usepackage{epsfig}
\usepackage{graphics}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{apptools}
\usepackage{titlesec}
\usepackage{amsfonts}
\usepackage{xspace}
\usepackage{color}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{xtab}
\usepackage{listings}
\usepackage{yfonts}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{verbatim}
\usepackage[colorlinks=true,urlcolor=blue,linkcolor=blue,citecolor=blue]{hyperref}
\numberwithin{equation}{section}
\usepackage[braket, qm]{qcircuit}		%% Diseño de circuitos cuánticos y escritura cómoda de símbolos

\newtheorem{theorem}{Teorema}
\newtheorem{definition}{Definición}
\newtheorem{lemma}{Lema}
\newtheorem{proposition}{Proposición}
\newtheorem{corollary}{Corolario}

% Fuente: palatino
\usepackage[sc]{mathpazo}
\linespread{1.05}

% TFG en inglés:
%\usepackage[english]{babel} 
%\addto\captionsenglish{\renewcommand{\chaptername}{}}

% TFG en español:
\usepackage[spanish,es-nodecimaldot,es-tabla,es-lcroman,es-nosectiondot,
            es-noindentfirst]{babel}
\renewcommand\spanishchaptername{}

% Mis datos %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\miNombre}{José Alberto Azorín Puche}
\newcommand{\miCurso}{2020-2021}

% Formato de la página:
\usepackage{fancyhdr}
\usepackage[top=2.88cm,bottom=2.97cm,left=2.95cm,right=2.95cm]{geometry}
\setlength{\parskip}{0.1cm}

% Pon aquí tus definiciones:

\newcommand{\dis}{\displaystyle}
\sodef\an{}{.2em}{1em plus1em}{2em plus.1em minus.1em}

\begin{document}

% Portada %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{empty}
\pagenumbering{roman}

\noindent
\begin{tabular}{r}
\includegraphics[width=8.8cm]{escudoUGRmonocromo.png} \\[-1.8ex]
\hspace{31mm}\vspace{-8mm}
\begin{tabular}{c}
\hline\\[-1ex]\hskip-2mm
{\bf Facultad de Ciencias}\hspace{18mm}
\end{tabular}
\end{tabular}

{\large
\vspace{30mm}
\hspace{25mm}
\begin{tabular}{l}
\an{\sc Doble Grado en F{\'\i}sica y Matem\'aticas}
\end{tabular}

\vspace{45mm}
\hspace{25mm}
\begin{tabular}{l}
\an{\sc Trabajo Fin de Grado}
\\[1.5ex]
\an{\LARGE\bf ESTRUCTURAS ALGEBRAICAS}
\\
\an{\LARGE\bf EN COMPUTACI{\'O}N CU{\'A}NTICA:}
\\
\an{\Large\bf DISE{\~N}O DE ALGORITMOS CL{\'A}SICOS}
\\
\an{\Large\bf DE SIMULACI{\'O}N DE CIRCUITOS}
\\
\an{\Large\bf CU{\'A}NTICOS}
\end{tabular}

\vfill
\hspace{25mm}
\begin{tabular}{l}
Presentado por:
\\
{\bf D. \miNombre}
\\[3ex]
Curso Académico \miCurso
\end{tabular}
}

\newpage

% Declaración de originalidad %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty}

\hfill\vfill

\textsc{Declaración de originalidad}\\\bigskip

D. \miNombre \\\medskip

Declaro explícitamente que el trabajo presentado como Trabajo de Fin de Grado (TFG), correspondiente al curso académico \miCurso, es original, entendida esta, en el sentido de que no ha utilizado para la elaboración del trabajo fuentes sin citarlas debidamente.
\medskip

En Granada a \today
\begin{flushleft}
Fdo: \miNombre

\end{flushleft}

\vfill

\newpage

\begin{center}

	
{\bf Summary}
\bigskip

\begin{minipage}{0.8\linewidth}
Along the last few decades, we have experienced how technology has become a fundamental part in our daily lives, turning mobile phones and computers into our biggest allies. In the later case, it is not difficult to think about an application in any conceivable task that we may face: computers appear anywher helpping us in varied fields, such as design, numerical calculation, data science, AI or simulations, among infinite others.

All these years have proven an indisputable fact: the greater are computational developments, the greater are the challenges that humans achieve. In 1965, Gordon Moore formulated what we now remember as \textit{Moore’s Law}: the number of transistors in a dense integrated circuit doubles about every two years. This statement was observed empirically and still remains verified.

However, higher quantities of components in circuits imply tinier transistors. That is the reason why quantum effects are now starting to appear and interfere in the performance of electronic devices. Science has found a solution for this issue moving to a different computing paradigm.

Quantum computation and quantum information is the study of the information processing tasks that can be accomplished using quantum mechanical systems. This brand-new point of view for computation brings countless phenomena that classical computation does not enjoy. Here are some examples:

\begin{itemize}
	\item Quantum parallelism: quantum systems with $n$ qubits represent $2^n$ states at once, whereas classical systems can only go one by one. This is the basis for Deutsch’s algorithm and Deutsch-Jozsa algorithm, which exploit this characteristic of quantum computation to accomplish an exponencial number of operations at the same time.
	\item Entanglement: a set of particles is said to be entangled when there is not an individual description for each of them, but they are described jointly. No matter how far they are from one another, actions over one of them have an impact on the rest. This is a very powerful resource in quantum information that can be used  to make improvements on communications, helping create safer channels to transmit messages.
\end{itemize}

The machines functioning according to quantum computation principles are called quantum computers: they take advantage of quantum systems’ coherence to carry out calculations. These devices have very sophisticated ways to get implemented, like ion traps or nuclear magnetic resonance. This is the source of important amount of noise, due to the interaction between the system and the environment.

\end{minipage}

\vfill

\end{center}

\newpage

\begin{center}
	
	\begin{minipage}{0.8\linewidth}
		To help avoid the fatal effects that noise might introduce, Quantum Error Correction (QEC) and fault-tolerant quantum computation were born: quantum computation is able to asume a limited quantity of noise and keep functioning with all the advantages it has to offer. Inside this domain, there exists a relevant class of codes, the stabilizer codes, whose main support is algebraic group theory.
		
		In this project, the reader will get immersed in the basics of quantum computation and the stabilizer formalism, that remains a key tool in contemporary investigations. There is also a big goal we are chasing after: presenting every aspect with the rigour that theorical physics deserve. This is why one can find and introductory section to the mathematical subjects that will apear all along the document.
		
		Once we have got at ease with all these new concepts, we expose two classical algorithms permitting simulate quantum circuits. But wait, quantum computation had promised to prevail over its classical counterpart. Why would we use classical algorithms instead of starting with quantum computers as soon as possible?
		
		As usual, our classical mind does not let us visualize beyond what we already know. Classical computers architects can debug a device by including test conditions, monitoring registers, interrupting processing at intermediate steps, and so on. However, quantum computers get bothered by this kind of resources, given the fact that they destroy coherence. Hence, we need to find a way to design and debug a quantum computer before even trying to implement it.
		
		Quantum architecture is not the only motivation to study  classical algorithms to simulate and manipulate quantum circuits. Physicists and chemists can also profit these algorithms to simulate quantum systems before new (expensive) quantum computers come into existence.
		
		We will understand simulation algorithms engendered by Gottesman and Knill \cite{NielsenChuang}, and Aaronson and Gottesman \cite{Aaronson}. After studying how they work, we will focus on the improvements in terms of efficiency that the second one introduces: the use of symplectic products to check conmutativity relations between operators leads to a decrease of computational cost, jumping from $O(n^3)$ to $O(n^2)$.
		
		This efficiency analysis is necessary when we talk about implementing new algorithms, as computational cost (number of operations executed) translates into time, energy and even space expenses.
		
		In a nutshell, this paper aims to show the reader the wonders of quantum computing and give him/her/them a global perspective of how physicists and computers scientists work, in order to develop this discipline that may change our world in the same way classical computers have changed us in these last few decades.
	\end{minipage}
	
	\vfill
	
\end{center}


% Indice %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\tableofcontents

\newpage{\ } 
\thispagestyle{empty} 

% Texto %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\pagestyle{fancy}
\fancyhead[LO,RE]{\rightmark}
\fancyhead[RO,LE]{\thepage}
\fancyfoot{}
\pagenumbering{arabic}

\section{Introducción}

En la sociedad actual, los ordenadores se han impuesto como un compañero esencial en cualquier tarea: se dejan ver en ámbitos clásicos como la ofimática, el cálculo científico o el diseño, pero también han surgido ambientes más exóticos como las simulaciones en ciencia e ingeniería, o la ciencia de datos, en los que juegan un rol fundamental. Todos estos campos tienen algo en común, y es que con el paso del tiempo han experimentado importantes mejoras ligadas al aumento de potencia que ha ofrecido la computación en las últimas décadas. 

En 1965, Gordon Moore enunciaba la conodida como \textit{ley de Moore}, que predecía cuál sería la tendencia de mejora computacional para los ordenadores. Ésta dice que cada dos años se duplica el número de transistores en un microprocesador, conllevando esto un incremento de potencia computacional. Sin embargo, al aumentar el número de componentes, el tamaño de los mismos ha de ir haciéndose más y más pequeño, de manera que los efectos cuánticos  empiezan a intervenir en el funcionamiento, y que pueden ser perjudiciales de no ser tratados con el cuidado que se merecen.

Ante la amenaza de que la ley de Moore termine por fallar, debido a las limitaciones físicas recién mencionadas, parece buena idea encontrar una solución cambiando totalmente la perspectiva desde la que se resuelven los problemas. Aquí es donde entra en juego la Computación Cuántica.

La Computación Cuántica y la Información Cuántica \cite{NielsenChuang} consisten en el estudio de tareas de procesamiento de la información mediante la utilización de sistemas físicos que se rigen por las leyes de la Mecánica Cuántica. En este trabajo, se pretende sumergir al lector en este fascinante mundo empezando desde sus elementos más básicos, el qubit y las puertas cuánticas. Esto se hace en la sección \ref{sec: intro CC}, después de haberlo introducido en \ref{sec: fundamentos matematicos} en los diferentes aspectos matemáticos a tener en cuenta durante la lectura del texto.

Ya sabemos lo que es la Computación Cuántica, pero ¿cómo se traduce esto en los ordenadores que se desarrollan a partir de ella? Podemos definir un  ordenador cuántico 
	\begin{wrapfigure}[16]{r}{0.35\columnwidth}
		\centering
		\includegraphics[width=5cm]{ibmq.jpg}	
		\captionsetup{justification=centering}	
		\caption{ Ordenador Cuántico IBM Q}
		\label{fig: IBM Q}
	\end{wrapfigure}
como una máquina que usa la coherencia de sistemas cuánticos para acelerar los cálculos que realiza.

Estas mejoras se consiguen por medio de procedimientos físicos muy diversos: trampas de iones, resonancia magnética nuclear, etc. Sin más que mirar la figura \ref{fig: IBM Q}, podemos imaginar que estos procesos son bastante complicados y que los ordenadores cuánticos que los llevan a cabo son máquinas muy sofisticadas.

El problema que presentan estos aparatos es que son muy sensibles al ruido, que aparece por la interacción con su entorno. No obstante, se ha visto que la Computación Cuántica puede tolerar ciertas cantidades de ruido, sin sacrificar con ello sus ventajas computacionales. Para tal fin se crean los códigos de corrección de errores, de los que se dará un ejemplo introductorio en el apartado \ref{subsubsec: ejemplos estabilizador}.

\newpage
Dentro de los códigos de corrección de errores aparecen los códigos estabilizadores, que se definen a partir de nociones básicas de teoría de grupos. Hablamos sobre ellos en el epígrafe \ref{subsec: formalismo estabilizador}. El formalismo que en él se deriva será la piedra angular en el desarrollo de los dos algoritmos explicados en las secciones \ref{sec: GottesmanKnill} y \ref{sec: AaronsonGottesman}.

Dichos algoritmos nos permiten simular el comportamiento de un ordenador cuántico de manera clásica. Surge ante esto una duda más que comprensible: si tan buenos son los ordenadores cuánticos, ¿para qué pasar por una simulación suya en un ordenador clásico?

Al depurar defectos en los ordenadores clásicos, podemos monitorizar diferentes magnitudes, haciendo todas las medidas que sean necesarias, o interrumpir procesos si se estima necesario, entre otros recursos. Sin embargo, si trabajamos de esta manera con un ordenador cuántico, destruiríamos la coherencia que lo caracteriza. Por consiguiente, es interesante poder diseñar y depurar una computadora cuántica por medio de herramientas clásicas antes de vernos en la necesidad de implementarla físicamente.

Además, la arquitectura de hardware no es la única motivación que podemos encontrar para el diseño de estos algoritmos. La simulación con ordenador de sistemas cuánticos puede tener aplicaciones obvias en Física y en Química, pudiendo estudiarlos sin verse obligados a esperar a la costosa contrucción de los ordenadores cuánticos.

Un último aspecto que se abordará en estas páginas suele aparecer de forma natural al hablar de algoritmos: la eficiencia. Al ejecutar un algoritmo, se consumen recursos como pueden ser tiempo, energía e incluso espacio. Por ese motivo, en la fase de diseño se buscará minimizar el consumo de alguno de ellos (que implicará con frecuencia un ahorro del resto).
En los epígrafes \ref{subsec: eficiencia GottesmanKnill} y \ref{subsec: eficiencia AaronsonGottesman} comparamos las eficiencias de los dos algoritmos estudiados, para ver que la introducción del producto simpléctico consigue reducir de manera notable el coste computacional cuando el número de qubits en el circuito simulado es elevado.

En definitiva, el objetivo de este trabajo no es otro que el de introducir al lector en una rama de la Física y la Computación todavía joven y desconocida a día de hoy para muchos científicos. A través de un par de ejemplos detallados, se intentarán reflejar las posibilidades que nos aporta esta disciplina. 
No obstante, estas posibilidades son un conjunto muy restringido de todo lo que la Computación Cuántica tiene para sorprendernos.


\newpage

\section{Fundamentos matemáticos}
	\label{sec: fundamentos matematicos}
	
	Dado que la Mecánica Cuántica y la Computación son disciplinas muy teóricas y abstractas, comenzamos presentando una serie de conceptos que necesitamos asimilar.
	
	\subsection{Teoría de grupos}
%	
%		FALTA POR HABLAR DE LA ACCIÓN DE UN GRUPO (POR CONJUGACIÓN)
%		Y SEGURAMENTE DE HOMOMORFISMOS/ISOMORFISMOS Y AUTOMORFISMOS.
%		A lo mejor tengo que incluir las definiciones de centro de un grupo y de subgrupo normal.
%		No sé si sirve de algo definir el subgrupo conmutador aquí.
%		No sé si hará falta hablar de matrices unitarias o no.
%
	
	Comenzamos esta sección de fundamentos con uno de los imprescindibles en la lectura de este trabajo, la teoría de grupos \cite{Grupos}. Daremos algunas definiciones básicas que necesitamos conocer y varias herramientas que servirán para construir los algoritmos que explicamos en las secciones \ref{sec: GottesmanKnill} y \ref{sec: AaronsonGottesman}.
	
		\begin{definition} \label{def: grupo}
			Un \textbf{grupo} es un par ordenado $(G,\ast)$, donde $G$ es un conjunto y $\ast : G\times G \longrightarrow G$ es una operación binaria que satisface las siguientes condiciones:
			
				1.  $\ast$ es asociativa, esto es, $a\ast(b\ast c)=(a\ast b)\ast c, \; \forall a,b,c \in G$.
				
				2.  Existe un elemento neutro $e\in G$ tal que $a\ast e=e\ast a=a, \; a\in G$.
				
				3. Todo elemento $a \in G$ tiene inverso: $\forall a \in G, \, \exists a^{1} / a\ast a^{-1}=a^{-1}\ast a =e $.	
				
			\noindent Si, además, se cumple que $a\ast b =b\ast a, \; \forall a,b\in G$, diremos que $(G,\ast)$ es un grupo \textbf{abeliano} o conmutativo.
		\end{definition}
	
	Un ejemplo importante que podemos dar es el grupo de todas las permutaciones sobre un conjunto $X\ne\emptyset$, que notaremos $\text{Perm}(X)$ o $S_n$, si $X$ es un conjunto finito. Es claro que las tres propiedades de \ref{def: grupo} se verifican tomando la composición de permutaciones donde antes poníamos $\ast$.
	
	Un paso clave en nuestro objetivo de tener algoritmos eficientes pasará por ser capaces de representar un grupo de la manera más compacta posible. Esto lo conseguimos por medio de los conocidos como \textbf{generadores}:
	
		\begin{definition} \label{def: generador grupo}
			Si $A$ es un subconjunto del grupo $G$, definimos el \textbf{subgrupo de $G$ generado por $A$} como \[\left\langle A \right\rangle = \displaystyle\bigcap_{A \subseteq H \le G} H. \]
			Si el generador $A=\{a_1,...,a_n\}$ es un conjunto finito, escribiremos $\left\langle A\right\rangle \equiv \left\langle a_1,...,a_n\right\rangle $, y si el subgrupo está generado por más de un conjunto, lo notaremos $\left\langle A\bigcup B\right\rangle \equiv \left\langle A,B\right\rangle $.
			%
			%Un grupo puede darse a partir de sus elementos generadores y las relaciones existentes entre ellos por medio de su \textbf{presentación}.
		\end{definition} 
	
	%Otro ejemplo importante es la familia de \textbf{grupos diédricos} o diedrales, $D_n$, que representan el conjunto de todas las rotaciones y las reflexiones respecto al eje que pasa por un determinado vértice en un polígono de $n$ lados. Este grupo , generado por las rotaciones $r$ (de $\frac{360}{n}$ grados) y las simetrías $s$, y cuya operación es la composición de movimientos, tiene la siguiente presentación:
	
	%	\begin{equation} \label{def: grupo diedrico}
	%		D_n = \left\langle r,s \, | \, r^n=1, \, s^2=1, \, sr = r^{-1}s = r^{n-1} s \right\rangle 
	%	\end{equation}
	
	El siguiente resultado revela el ahorro que supone describir un grupo en términos de sus generadores, que más tarde se traducirá en una clara ventaja computacional:
	
		\begin{proposition} \label{prop: generadores}
			Sea $G$ un grupo finito formado por $|G|$ elementos. Entonces $G$ está generado por un conjunto de, máximo, $\log_2|G|$ elementos.
		\end{proposition}
	
		\begin{proof}
			Supongamos que $g_1,...,g_l$ es un conjunto de elementos que pertenecen al grupo $G$ y $g\in G$ es otro elemento tal que $g\notin \left\langle g_1,...,g_l\right\rangle$.
		
			Tomamos entonces $f\in \left\langle g_1,...,g_l\right\rangle$, de manera que $fg\notin \left\langle g_1,...,g_l\right\rangle$ (pues de lo contrario $g=f^{-1}fg\in \left\langle g_1,...,g_l\right\rangle$, conduciendo a contradicción). Por tanto, $ \forall f \in \left\langle g_1,...,g_l\right\rangle$, hay un elemento $fg$ que está en $\left\langle g_1,...,g_l,g\right\rangle$, pero no en $\left\langle g_1,...,g_l\right\rangle$.
		
			Es por ello que, añadiendo un nuevo elemento al sistema generador de un grupo, lo que hacemos es, como mínimo, doblar el tamaño del grupo generado. De esto se deduce que $G$ ha de tener un conjunto generador de, como máximo, $\log_2|G|$.
		\end{proof}

	Otro ejemplo de grupo son los tan conocidos espacios vectoriales, teniendo a la suma como ley de composición interna. Al aprender sobre espacios vectoriales, una de las primeras nociones que nos descubren es que hay conjuntos más pequeños dentro de ellos que mantienen características iguales, los subespacios. Ahora ocurre lo mismo:
	
		\begin{definition} \label{def: subgrupo}
			Sea $G$ un grupo. Diremos que $H\subset G$ es un subgrupo de $G$ si $H$ es no vacío y cerrado bajo la operación $\ast$ y la inversión, esto es,  $a,b\in G\Rightarrow a\ast b , a^{-1}, b^{-1} \in G$. En tal caso, lo notaremos $H\le G$.
		\end{definition}
	
	Si $H$ es un conjunto finito, basta con comprobar que es no vacío y cerrado para $\ast$ para tener la certeza de que es un subgrupo.
	
	Para ilustrar la idea de subgrupos, vamos a definir varias estructuras que serán de gran utilidad más adelante. En todos los casos, comprobar la defición de subgrupo es sencillo:
	
		\begin{definition}
			Sea $G$ un grupo y $A\ne \emptyset$ un subconjunto suyo. Definimos el \textbf{centralizador} de $A$ en $G$ como el conjunto de elementos de $G$ que conmutan con todos los elementos de $A$:
			\[ C_G(A) = \{g \in G \; | \; g\ast a=a\ast g \; \forall a\in A  \}. \]
			Llamaremos \textbf{centro} de $G$ a $Z(G) = C_G(G) =  \{g \in G \; | \; g\ast a=a\ast g \; \forall a\in G  \}$. \\
			Por último, si notamos $gAg^{-1}=\{gag^{-1} | a \in A\}$, se define el \textbf{normalizador de $A$ en $G$} como
			\[ N_G(A) = \{g\in G \; | \; gAg^{-1}=A\}. \]
			%Por último, si $[a,b]=aba^{-1}b^{-1}$ es el conmutador de $a$ y $b$, llamaremos el \textbf{subgrupo conmutador} o \textbf{subgrupo derivado}  del grupo $G$ al conjunto
			%\[ G'=[G,G] = \left\langle [a,b] \; | \; a,b\in G \right\rangle \]
		\end{definition}
	
	Ahora que hemos introducido esta nueva estructura, necesitamos definir una operación que relacione unos grupos con otros:
	
		\begin{definition}
			Sean $G$ y $H$ dos grupos:
			
			\noindent Un \textbf{homomorfismo} de $G$ en $H$ es una aplicación $ f : G \longrightarrow H \;/\; f(xy) = f(x)f(y), \; \forall x, y \in G $. Llamaremos a $G$ y $H$ dominio y codominio de $f$, respectivamente. Si $H=G$, diremos que $f$ es un \textbf{endomorfismo}. 
			
			\noindent Diremos que $f$ es un \textbf{isomorfismo} (respectivamente epimorfismo, monomorfismo) si $f$ como aplicación es biyectiva (respectivamente sobreyectiva, inyectiva). Si $f$ es un endomorfismo que es, además, isomorfismo, diremos que se trata de un \textbf{automorfismo}.
		\end{definition}
	
	Terminamos esta sección hablando sobre otra aplicación que será fundamental cuando hablemos del formalismo estabilizador y del algoritmo que deriva de él.
	
		\begin{definition}
			Sea $G$ un grupo y $X\ne\emptyset$ un conjunto. Una \textbf{acción} de $G$ sobre $X$ es una aplicación $\textup{ac}: G\times X \longrightarrow X$, que verifica las dos condiciones siguientes:
			
				 1. $\textup{ac}(e,x)=x, \; \forall x \in X$.
			
				 2. $\textup{ac}\left( g , \textup{ac}(h,x) \right) = \textup{ac}(gh,x) , \; \forall g, h \in G \; \forall x \in X$.
				 
			\noindent El caso particular en que $\textup{ac}(g,x)=gx$ se conoce como \textbf{acción por traslación}, y si $\textup{ac}(g,x)=gxg^{-1}$ diremos que \textup{ac} es una \textbf{acción por conjugación}.	 
		\end{definition}	
	
	Además, resulta que dar una acción de $G$ sobre $X$ es equivalente a dar un homomorfismo de grupos de $G$ en $\text{Perm}(X)$. Es decir, aplicando una acción sobre un elemento $x_1 \in X$, estamos convirtiéndolo en otro $x_2 \in X$ de ese mismo conjunto (pudiendo ser él mismo).
	
	\subsection{Espacios de Hilbert}
	
	Como veremos en el \nameref{Par: Postulado 1} de la Mecánica Cuántica, el ambiente en que nos vamos a mover para describir los sistemas físicos será el de los espacios de Hilbert complejos. Recordemos que un espacio de Hilbert es un espacio prehilbertiano donde la norma asociada a su producto escalar $\ip{\cdot}{\cdot}$\footnote{Lo definimos de forma que sea lineal por la derecha y que sea antilineal por la izquierda.} es completa.
	
	Dicha elección es más que acertada: resultan ser una generalización de los tan conocidos espacios euclídeos reales a espacios complejos y espacios de dimensión infinita. Además, estos espacios garantizan la existencia de una base ortonormal y nos permitirán usar el Teorema espectral de operadores, herramientas esenciales en la formulación de esta rama de la Física.
	En este apartado, daremos las definiciones y resultados analíticos que aparecerán de forma continua, aunque no necesariamente de forma explícita, a lo largo de todo el trabajo. Todas las demostraciones pueden encontrarse en \cite{Hilbert}. 
	
	\begin{definition}
		Un \textbf{operador lineal}, o simplemente \textbf{operador}, es un homomorfismo entre espacios vectoriales normados. Notaremos $L(\mathcal{H})$ al espacio de los operadores lineales continuos de un espacio de Hilbert $\mathcal{H}$ en sí mismo.
		Un operador $T\in L(\mathcal{H})$ es \textbf{inversible} si $T^{-1}\in L(\mathcal{H})$.
	\end{definition}

	Dado que $L(\mathcal{H})$ es un espacio vectorial, podremos sumar y componer operadores sin que ello suponga un problema. Vayamos descubriendo algunas propiedades y cualidades que pueden presentar los operadores lineales.

	\begin{proposition} \label{prop: adjunto}
		Para cada $T\in L(\mathcal{H})$ hay un único opertador $T^\dagger \in L(\mathcal{H})$, llamado el \textbf{adjunto} de $T$, verificando que $\ip{y}{Tx}=\ip{T^\dagger y}{x}$, $\forall x, y \in \mathcal{H}$. Además, se verifican:\\
		\indent (a) $(S+\lambda T)^\dagger=S^\dagger+ \lambda^*T^\dagger $, $\: (ST)^\dagger=T^\dagger S^\dagger\:$ y $\:(T^\dagger)^\dagger=T$. \\
		\indent (b) $T$ es inversible si y solo si $T^\dagger$ es inversible, en cuyo caso $(T^\dagger)^{-1}=(T^-1)^\dagger$.
	\end{proposition}

	A partir de la proposición \ref{prop: adjunto}, se deducen varias definiciones que van a tener gran relevancia cuando hablemos de observables en la sección \ref{sec: intro CC}:
	
	\begin{definition}
		Un operador $T\in L(\mathcal{H})$ se llama \textbf{unitario} cuando $TT^\dagger=T^\dagger T =I$.\\
		Se dice que un operador $T\in L(\mathcal{H})$ es \textbf{autoadjunto} o \textbf{hermitiano} cuando $T^\dagger=T$, esto es, cuando se verifica que $\ip{y}{Tx}=\ip{Ty}{x}$ ($x,y\in\mathcal{H}$).
	\end{definition}

	Una gran ventaja que presentan los operadores, a la hora de trabajar con ellos, es que admiten una \textbf{representación matricial}. Sea $\mathcal{H}$ un espacio de Hilbert de dimensión infinita\footnote{Lo que hemos hecho para dimensión infinita puede particularizarse al caso finito: sólo hay que restringir $\mathbb{N}\times\mathbb{N}$ al conjunto $\mathcal{N}\times \mathcal{N}$, donde $\mathcal{N}=\{1,...,d\}$, siendo $d$ la dimensión del espacio.} sobre un cuerpo $\mathbb{K}$ y sea $B=\{u_n \,:\, n\in\mathbb{N}\} $una base ortonormal. A cada operador $T\in L(\mathcal{H})$ podemos asociar una función $a:\mathbb{N}\times\mathbb{N} \longrightarrow \mathbb{K}$, dada por 
		\begin{equation}
			a(i,j)=\ip{u_i}{Tu_j} \quad \left((i,j)\in \mathbb{N}\times\mathbb{N} \right) 
		\end{equation}
	Podemos así pensar en dicha función como una matriz $A=\left( a(i,j) \right)_{\mathbb{N}\times\mathbb{N}} $, que representa al operador $T$ en la base B. De hecho, $T$ está determinado de forma única por su matriz $A$. En esta representación, el adjunto viene representado por la matriz traspuesta complejo-conjugada de $A$: $A^\dagger=\left(A^* \right)^\dagger $.
	
	Gracias a esta representación, será frecuente que hablemos indistintamente de un operador y la matriz que lo representa en una cierta base ortonormal.
	
	Avanzamos en el camino hacia el Teorema Espectral definiendo algunos elementos que aparecerán explícitamente en el enunciado:
	
	Sea $\mathcal{H}$ un espacio de Hilbert sobre un cuerpo $\mathbb{K}$ y $T\in L(\mathcal{H})$. Un número $\lambda\in\mathbb{K}$ es un \textbf{valor propio} o \textbf{autovalor} de $T$ si $\ker(T-\lambda I)\neq \{0\}$. El espacio $E_\lambda=\ker(T-\lambda I)$ se llama \textbf{espacio propio} asociado al autovalor $\lambda$ y sus vectores no nulos son los \textbf{vectores propios} o \textbf{autovectores} asociados a $\lambda$.
	
	En dimensión finita, los valores propios son las raíces de la ecuación característica del operador; de ahí que el teorema fundamental del Álgebra asegure la existencia de valores propios en espacios de Hilbert complejos. Llamaremos \textbf{espectro} del operador $T$ a su conjunto de valores propios y lo notaremos $\sigma(T)$.
	
	Una \textbf{proyección ortogonal} en un espacio de Hilbert $H$ es un idempotente $P$ tal que $\ker(P)=P(\mathcal{H})^\bot$. Conocemos también un resultado que caracteriza las proyecciones sobre $P(\mathcal{H})$ como un operador idempotente autoadjunto.
	
	La siguiente proposición da sentido a asociar proyecciones ortogonales a los valores propios de un operador:
		\begin{proposition}
			Sea $T$ un operador autoadjunto en un espacio de Hilbert $\mathcal{H}$. \\
			Si $\lambda,\mu$ son valores propios distintos de $T$, entonces los espacios propios $E_\lambda=\ker(T-\lambda I)$ y $E_\mu=\ker(T-\mu I)$ son ortogonales. Por tanto, si $P_\lambda$ y $P_\mu$ son las proyecciones ortogonales de $\mathcal{H}$ sobre dichos subespacios se tiene que $P_\lambda P_\mu=0$.
		\end{proposition}
	
	Damos una última definición que aparece como una delas hipótesis del teorema:
		\begin{definition}
			Un operador \textbf{compacto} en un espacio de Banach $X$ es una aplicación lineal $T:X\rightarrow X$ tal que $\overline{T(B_X)}$ es compacto en $X$, donde $B_X=\overline{B}(0,1)$.
		\end{definition}
	
	Vamos a enunciar, al fin, el Teorema Espectral. Éste tiene dos versiones, una para el caso en que $\sigma(T)$ es un conjunto finito y otra para el caso infinito. Enunciaremos únicamente la primera, puesto que es la que se necesitará más adelante.
	
	\begin{theorem}[Teorema Espectral para operadores compactos autoadjuntos] \hspace{0.5cm}\\
	Sea $\mathcal{H}$ un espacio de Hilbert complejo y $T\in L(\mathcal{H})$ un operador compacto autoadjunto. Equivalen las afirmaciones siguientes:
		\begin{enumerate}[label=(\alph*),itemsep=0.5pt]
			\item $T(\mathcal{H})$ es de dimensión finita.
			\item $\sigma(T)$ es finito, es decir, $T$ tiene un número finito de valores propios distintos $\lambda_i$, $1\leq i\leq N$.
		\end{enumerate}
	En tal caso, poniendo $E_k=E_{\lambda_k}$, y llamando $P_k$ a la proyección ortogonal de $\mathcal{H}$ sobre $E_k$, se verifica que 
	 	\begin{equation}
	 		\mathcal{H}=\displaystyle\bigoplus_{k=1}^N E_k \; ; \quad
	 		I = \displaystyle\sum_{k=1}^{N} P_k \; ; \quad
	 		T = \displaystyle\sum_{k=1}^{N} \lambda_k P_k 
	 	\end{equation}
	 Además, si $B_k$ es una base ortonormal de $E_k$, entonces $B=\bigcup_{k=1}^N B_k$	es una base ortonormal de $\mathcal{H}$ formada por vectores propios de $T$. Y si $0\notin \sigma(T)$ entonces $\mathcal{H}$ es de dimensión finita y $T$ es inversible.
	\end{theorem}

	Un operador $T\in L(\mathcal{H})$ se dice \textbf{diagonalizable} si existe una base ortogonal de $\mathcal{H}$ formada por vectores propios de $T$. Se tiene que todo operador autoadjunto es diagonalizable.Esto se puede traducir a las matrices:
	
	 \begin{proposition}
	 	Una matriz $A\in\mathcal{M}_{d\times d}(\mathbb{K})$ es autoadjunta si, y solo si, $A=UDU^\dagger$, donde $U$ es unitaria y $D$ es diagonal con escalares reales en la diagonal.
	 \end{proposition}
	
		\subsubsection{Notación de Dirac}
		
		Al comenzar este apartado, notamos el producto escalar por $\ip{\cdot}{\cdot}$. Esta notación no fue elegida por casualidad, sino que es la que se utiliza en Mecánica Cuántica para trabajar de la forma más cómoda posible. El siguiente resultado, clave en el estudio del Análisis Funcional, nos ayudará a comprender por qué:
		
			\begin{theorem}[Riesz-Fréchet]
				Sea $\mathcal{H}$ un espacio de Hilbert y $\mathcal{J} : \mathcal{H} \rightarrow \mathcal{H}^*$ aplicación que a cada $y\in\mathcal{H}$ hace corresponder el funcional lineal $\mathcal{J}_y:\mathcal{H}\rightarrow \mathbb{K}$ definido por
					\begin{equation}
						\mathcal{J}_y(x) = \ip{y}{x} \quad (x\in \mathcal{H})
					\end{equation}
				Se verifica que $\mathcal{J}$ es una biyección conjugado-lineal e isométrica de $\mathcal{H}$ sobre $\mathcal{H}^*$.
			\end{theorem}
		
		Este teorema dice que cada vector $y \in \mathcal{H}$ está asociado a un funcional lineal en el espacio dual\footnote{Dado un espacio normado $X$ sobre $\mathbb{K}$, definimos su espacio dual topológico, $X^*$, como el espacio $L(X,\mathbb{K})$ de todos sus funcionales lineales y continuos.} $\mathcal{H}^*$ que consiste en aplicar el producto escalar por $y$. Aunque esta notación es clara, Dirac dio con la forma de escribir estos funcionales de forma multiplicativa, y que así trabajar con ellos fuera más directo e intuitivo \cite{Dirac}:
		
		Denotaremos un vector del espacio $\mathcal{H}$ como un \textbf{\textit{ket}}, $\ket{y}\in \mathcal{H}$, y al correspondiente funcional derivado del Teorema de Riesz-Fréchet lo llamaremos \textbf{\textit{bra}}, $\bra{y}\in \mathcal{H}^*$. Así, la aplicación del funcional y sus propiedades lineales se comprenden de una forma mucho más natural.
			\begin{equation}
				\mathcal{J}_y (x) \equiv \bra{y} (\ket{x}) = \bra{y} \ket{x} \equiv \ip{y}{x}
			\end{equation}
			\begin{equation}
				\mathcal{J}_y (\alpha x + \beta z) = \alpha \mathcal{J}_y(x) + \beta \mathcal{J}_y(z)
				\quad \Leftrightarrow \quad \bra{y}\left( \alpha \ket{x} + \beta \ket{z} \right) = \alpha \ip{y}{x} + \beta \ip{y}{z}
			\end{equation}
		Aunque $\mathcal{H}$ y $\mathcal{H}^*$ no son el mismo espacio (uno es un espacio de vectores o kets y el otro es un espacio de funcionales o bras), hay una correspondencia biunívoca entre ellos \cite{AlgebraII}:
			\begin{theorem}
				Dada una base $B=(v_1,...,v_n)$ de $V$, existe una única base $B^*=(\phi^1,...,\phi^n)$ de $V*$ tal que $\phi^i(v_j)=\delta_ij$. Llaremos base dual de $B$ a la base $B^*$. 
			\end{theorem}
		Además, la relación entre un ket y su correspondiente bra se describe de manera muy sencilla:
			\begin{equation} \label{eq: adjunto}
				\left( \ket{y} \right)^\dagger =\bra{y} \qquad \left( \bra{y} \right)^\dagger =\ket{y}
			\end{equation}
	
		Otro elemento curioso de la notación de Dirac es el \textbf{producto externo}, un operador de $\mathcal{H}$ en sí mismo que viene dado por la concatenación de un ket y un bra:
			\begin{equation} \label{eq: producto externo}
				\op{x}{y} : \mathcal{H} \rightarrow \mathcal{H} \: \text{ tal que } \: \op{x}{y} \left( \ket{z} \right) = \ket{x} \ip{y}{z} = \ip{y}{z} \ket{x} 
			\end{equation} 
		En \ref{subsec: tensores} hablaremos sobre su naturaleza. Un caso particular cuando el vector $\ket{x}$ es un vector de una base ortonormal de $\mathcal{H}$ son los proyectores ortogonales:
			\begin{equation}
				E=\text{Lin}\left( \ket{x_1},...,\ket{x_k} \right) \quad \Rightarrow \quad P_E = \displaystyle\sum_{j=1}^{k} \op{x_j}{x_j} \: \text{ (proyector ort. sobre $E$)}
			\end{equation}
		Atendiendo a esta definición de proyector es inmediato darse cuenta de que los proyectores son operadores autoadjuntos, sin más que fijarnos en \eqref{eq: adjunto}.
		
	\subsection{Análisis simpléctico}
				
		En este breve apartado se introduce una herramienta nueva, ahora desconocida para nosotros, pero que cada vez se utiliza más en diversos ámbitos de la Física y la Computación. Se trata de las \textbf{matrices simplécticas} \cite{deGosson}:

			\begin{definition}\label{def: Matriz simplectica}
			Definida la matriz $J = \left(\begin{array}{cc}0 & I\\-I & 0\end{array}\right) \in \mathbb{R}^{2n\times 2n}$,  decimos que una matriz $M \in \mathbb{R}^{2n\times 2n}$ es simpléctica si se verifican las siguientes igualdades:
					\begin{equation} \label{eq: Matriz simplectica}
								M^T J M =  M J M^T = J
					\end{equation}
			Notaremos el conjunto de todas las matrices simplécticas de dimensión $2n$ por $\text{Sp}(2n,\mathbb{R})$.    
			\end{definition}

		\noindent Como $J^T=J^{-1}=-J$, si $M$ es simpléctica también lo será su matriz inversa $S^{-1}$:
			\begin{equation*}
				(M^{-1})^T J M^{-1} = - (M^{-1})^T J^{-1} M^{-1} = -(M^T)^{-1}J^{-1}M^{-1} = -(MJM^T)^{-1} = -J^{-1} =J
			\end{equation*}
			\begin{equation*}	
				M^{-1} J (M^{-1})^T = - M^{-1} J^{-1} (M^{-1})^T = -M^{-1}J^{-1}(M^T)^{-1}= -(M^TJM)^{-1} = -J^{-1} =J
			\end{equation*}

		\noindent Además, es trivial reconocer que el producto de matrices simplécticas es, a su vez, una matriz simpléctica. Así, se tiene que $\text{Sp}(2n, \mathbb{R})$ es un grupo (con la multiplicación de matrices como ley de composición interna), aquel llamado \textbf{grupo simpléctico}.    

		\noindent Se puede comprobar que la definición presentada anteriormente es redundante, puesto que las igualdades \ref{eq: Matriz simplectica} son, de hecho, equivalentes:
		
			\begin{center}
				$M^TJM=J \quad \Leftrightarrow \quad (M^TJM)^{-1} = J^{-1}  \quad  \Leftrightarrow \quad
					-M^{-1}J(M^T)^{-1} = -J \quad \Leftrightarrow \quad  J = MJM^T $
			\end{center}

		\noindent Así, bastará con que se verifique una de las dos para que automáticamente se cumpla la otra, simplificándose la definición \ref{def: Matriz simplectica}.

		Pasamos a definir otra utilidad que necesitaremos para comprender en la sección \ref{sec: AaronsonGottesman} cómo funciona el algoritmo: el \textbf{producto simpléctico}.
		
			\begin{definition} \label{def: producto simplectico}
				Diremos que una forma bilineal es una \textbf{forma simpléctica} si es antisimétrica y no degenerada. Un caso especial es el conocido como \textbf{producto simpléctico} (o forma simpléctica estándar) $(\cdot | \cdot)  : \mathbb{R}^{2n} \times \mathbb{R}^{2n} \longrightarrow \mathbb{R}$, definido por
					\begin{equation} \label{eq: producto simplectico}
						 (z|z') = z'\, J\, z = p\cdot x' - p'\cdot x
					\end{equation}
				donde $z=(x,p)$ y $z'=(x',p')$, con $x, x', p, p' \in \mathbb{R}^n$	 y ``$ \cdot$'' denota el producto escalar en $\mathbb{R}^n$.
			\end{definition}	

		En efecto, se verifica que $\sigma(z,z')=-\sigma(z',z)$ (antisimetría) y esto implica que todos los vectores $z$ son \textit{isótropos}, esto es, $\sigma(z,z)=0$.
		
	\subsection{Tensores} \label{subsec: tensores}
	
	En Mecánica Cuántica, espacios tensoriales y producto tensorial son nociones más que necesarias cuando se habla de sistemas compuestos por más de un subsistema, como será nuestro caso. En este apartado, los definiremos y daremos las propiedades que se usarán posteriormente. La redacción de este apartado se apoyará, sobre todo, en \cite{AlgebraII}.
	
	\begin{definition}
		Sean $X_1,...,X_k$ y $Y$ espacios vectoriales sobre un cuerpo $\mathbb{K}$. Diremos que una aplicación $M: X_1 \times X_k \rightarrow Y$ es \textbf{multilineal} si $M$ es lineal en todos sus argumentos:
			\begin{multline}
				M(x_1,...,x_{j-1}, ax'_j + b x''_j , x_{j+1}, ... , x_k) = \\
				= a M (x_1,...,x_{j-1}, x'_j, x_{j+1}, ... , x_k) + b M(x_1,...,x_{j-1},  x''_j , x_{j+1}, ... , x_k) \qquad
			\end{multline}
		para todos $x_i\in X_i$, $\: x'_j,x''_j\in X_j$, $\:1\leq j \leq k$, $\:a,b\in \mathbb{K}$.	
	\end{definition}
	
	Dado un espacio vectorial $V(\mathbb{K})$ aparece un caso particular: 
	
	\begin{definition}
		Un \textbf{tensor} $r$ veces covariante y $s$ veces contravariante (de tipo $(r,s)$) sobre $V$ es una aplicación multilineal
			\begin{equation}
				\begin{aligned}
					T: V\times \overset{(r)}{\hdots}\times V \times V^* \times \overset{(s)}{\hdots} \times
				 	V^* \:	&  \:\rightarrow \mathbb{K} \\
				 	(u_1,...,u_r,\phi^1,...,\phi^s) \: &  \: \mapsto T(u_1,...,u_r,\phi^1,...,\phi^s)	.			 	
				\end{aligned}
			\end{equation}
	\end{definition}
	
	Definiendo de forma natural la suma y el producto por escalar de tensores (sumando y multiplicando por escalar las correspondientes imágenes), se tiene que $\mathcal{T}_{r,s}(V)$, el conjunto de tensores de tipo $(r,s)$ sobre $V(\mathbb{K})$, tiene estructura de espacio vectorial.
	
	Es claro que $\mathcal{T}_{1,0}=V^*$ y $\mathcal{T}_{0,1}=V^{**}$. El ambiente en que vamos a trabajar es con $V$ siendo un espacio de Hilbert y de dimensión finita: tanto en un caso como en otro se tiene que podemos identificar $V$ con $V^{**}$ vía un isomorfismo isométrico, de manera que podemos identificar $\mathcal{T}_{0,1}(V)=V$. Dicho con otras palabras, podemos identificar cada vector $v\in V$ con un tensor 1-contravariante
		\begin{equation*}
			v: V^* \rightarrow \mathbb{K} \quad , \quad \phi \mapsto \phi(v).
		\end{equation*}
		
	Introducimos ahora el producto tensorial, una herramienta que nos permitirá crear tensores nuevos aumentando los índices $r$ o $s$. 
		\begin{definition} \label{def: producto tensorial}
			Sean $T\in \mathcal{T}_{r,s}(V) $ y $T' \in \mathcal{T}_{r',s'}(V)$. Se define el \textbf{producto tensorial} de $T$ por $T'$ como $T\otimes T' : V\times \overset{(r+r')}{\hdots}\times V \times V^* \times \overset{(s+s')}{\hdots} \times V^* \rightarrow \mathbb{K}$ , donde
			\begin{equation*} 
				(T\otimes T')(u_1,...,u_{r+r'},\phi^1,...,\phi^{s+s'})=T(u_1,...,u_r,\phi^1,...,\phi^s)\cdot T'(u_{r+1},...,u_{r+r'},\phi^{s+1},...,\phi^{s+s'}).
			\end{equation*}
		\end{definition}
		
	\noindent Listamos a continuación algunas propiedades inmediatas deducidas de la definición:
	\begin{enumerate}[leftmargin=0.6cm,itemsep=0.8pt]
		\item $T\otimes T'$ es multilineal y, por tanto, $T\otimes T' \in \mathcal{T}_{r+r',s+s'}(V)$.
		\item $\otimes$ es lineal en cada variable: \\
			\begin{equation*} \begin{aligned}
				(aT_1+bT_2) \otimes T' = a(T_1 \otimes T')+ b (T_2 \otimes T') \: , \quad	&\forall T_1,T_2 \in \mathcal{T}_{r,s}, \forall T' \in \mathcal{T}_{r',s'}\\ 
				T \otimes (aT'_1+bT'_2) = a(T\otimes T'_1) + b (T\otimes T'_2) \: , \quad	&\forall T \in \mathcal{T}_{r,s}, \forall T'_1, T'_2 \in \mathcal{T}_{r',s'}.
			\end{aligned} \end{equation*}
		\item La operación producto tensorial es asociativa(aunque no conmutativa).	
	\end{enumerate}
		
	Un ejemplo interesante es el de los \textbf{tensores 1-covariantes, 1-contravariantes}.	Se trata de los tensores de orden 2 que constituyen el espacio vectorial $\mathcal{T}_{1,1}$ y que se definen a partir de dos tensores $\phi\in V^*$ y $u\in V$ de orden 1. Para ello, se considera su producto tensorial
		\begin{equation}
			\begin{aligned}
				\phi\otimes u : V\times V^* & \rightarrow \mathbb{K}\\
				(v, \psi) & \mapsto \phi(v)\cdot \psi (u).
			\end{aligned}
		\end{equation}
	A este espacio pertenecen, pues, los productos externos \eqref{eq: producto externo}	definidos con la notación de Dirac. De nuevo, vemos que esta notación permite trabajar de forma más intuitiva con elementos como este.
	
	Fijando una base $B=\{v_1,...,v_n\}$ de $V$, y otra $B^*=\{\varphi^1,...,\varphi^n\}$ del dual $V^*$, definimos \(B_{1,1}=\{\varphi^1,...,v_j \, | \, i,j = 1,...,n \},\)
	que es una base de $\mathcal{T}_{1,1}$, con dimensión $n^2$.
	
	Un último comentario sobre estos tensores tan especiales es que guardan una potente relación con los endomorfismos. Mientras que $\mathcal{T}_{2,0}$, $\mathcal{T}_{0,2}$ y $\mathcal{T}_{1,1}$ son todos isomorfos al espacio $\text{End}(V)$ de los endomorfismos de $V(\mathbb{K})$, sólo se tiene para el último un isomorfismo que no depende de las bases. 
	
		\begin{theorem}
			Definido el tensor $T_f(v,\phi):=\phi\left( f(v) \right) $, se tiene que la aplicación
			\[	\begin{aligned}
			\text{End}(V)	\: &\rightarrow \: \mathcal{T}_{1,1}(V) \\
			f \: & \mapsto \: T_f \qquad ,
			\end{aligned}\]
			es un isomorfismo de espacios vectoriales. Además, para cualquier base $B$ de $V$, se verifica que 
			\begin{equation}
				M_B(T_f)\footnote{Esta matriz se define coordenada a coordenada como $\left( M_B(T_f)\right)_{ij}=\left( T_f(v_j, \varphi^i)\right)_{ij} $}=M(f,B)
			\end{equation}
		\end{theorem}
	
	De este teorema se deduce, pues, que la matriz asociada a un endomorfismo y un tensor de $\mathcal{T}_{1,1}$ pueden ser identificados, sea cual sea la base en la que se trabaje.
	
	Acabamos presentado la forma más general posible de los tensores: los \textbf{tensores de tipo $(r,s)$}. Considerando una base $B=\{v_1,...,v_n\}$ de $V$ y su dual $B^*=\{\varphi^1,...,\varphi^n\}$ de $V^*$, se define
		\begin{equation} \label{eq: base tensorial}
			B_{r,s}=\{\varphi^{i_1}\otimes ... \otimes \varphi^{i_r} \otimes v_{j_1} \otimes ... \otimes v_{j_s} \: | \: i_1,...,i_r,j_1,...,j_s =1,...,n \}.
		\end{equation}
	Se puede demostrar que $B_{r,s}$ es una base de $\mathcal{T}_{r,s}(V)$ y que $\dim\left( \mathcal{T}_{r,s}(V) \right)=n^{r+s} $.
		\subsubsection{Producto de Kronecker}
		
		Ahora que ya sabemos que los tensores de orden 2 pueden ser representados por medio de matrices, vamos a revelar una forma muy cómoda de operar productos tensoriales de matrices. Un desarrollo más profundo puede encontrarse en \cite{Tensores}.
		
		Sea un conjunto de endomorfimos $a^{(j)} : V \longrightarrow V$, con $j=1,...,s$. Cada uno de ellos estará asociado a una matriz (que notaremos  $A^{(j)}=M(a^{(j)},B)$ ), ¿cómo hacer que actúen todos ellos sobre vectores del espacio $\mathcal{T}_{0,s}(V)$? La operación que estamos buscando se llama \textbf{producto de Kronecker} y viene dada por
			\[ \mathbf{A} := \displaystyle\bigotimes_{j=1}^n A^{(j)} : \mathcal{T}_{0,s}(V) \longrightarrow \mathcal{T}_{0,s}(V),\]
		donde
		\begin{equation}
		\displaystyle\bigotimes_{j=1}^n v^{(j)} \in \mathcal{T}_{0,s}(V) \quad \longmapsto \quad \mathbf{A} \left( \displaystyle\bigotimes_{j=1}^n v^{(j)}  \right) = \displaystyle\bigotimes_{j=1}^n \left( A^{(j)} v^{(j)}\right) \in \mathcal{T}_{0,s}(V).
		\end{equation} 	
		
		Obsérvese que $\mathbf{A}$ sigue siendo la matriz que representa un endomorfismo, por lo que seguirá estando asociada a tensores de orden 2. Sin embargo, la base sobre la que actúa es otra: $\mathbf{A}$ actúa sobre vectores de $\mathcal{T}_{0,s}(V)$, que tiene dimensión $n^s$, por lo que $\mathbf{A}$ será una matriz cuadrada de orden $n^s$.
		
		También hay que señalar que hemos definido el producto de Kronecker sobre elementos que son productos tensoriales de vectores, no lo hemos definido para elementos cualesquiera de $\mathcal{T}_{0,s}(V)$. Así, para poder calcular el efecto de $\mathbf{A}$ sobre cualquier vector de $\mathcal{T}_{0,s}(V)$, habrá que usar las propiedades de linealidad que dimos tras la definición \ref{def: producto tensorial}.
		
		La forma en que se definen los productos tensoriales y el producto de Kronecker nos permiten dar el producto tensorial de dos matrices de una forma muy sencilla. Dadas dos matrices cuadradas $A, B \in \mathcal{M}_n(\mathbb{C})$, su producto tensorial se calcula como
			\begin{equation} \label{eq: producto tensorial}
				A \otimes B =\left(  \begin{array}{ccc}
				a_{11}B 	 & \hdots & a_{1n}B \\
				\vdots  	 & \ddots  & \vdots \\
				a_{n1}B  & \hdots & a_{nn}B
				\end{array}						\right) 
			\end{equation}
		
		Una vez hemos aprendido lo más básico sobre producto tensorial, concluimos dando varios resultados sobre él que necesitaremos más adelante. Todos ellos atenderán a matrices cuadradas, que será lo más útil para nosotros, pero tienen una generalización a otras dimensiones y también un formalismo tensorial que ignoraremos.
		
		\begin{proposition}
			Sean $A$ y $B$ matrices cuadradas en $\mathcal{M}_n(\mathbb{C})$. Entonces:\\
			\indent 1. $(A\otimes B)^{-1} = A^{-1} \otimes B^{-1}$ $\quad$, $\quad$
			$(A\otimes B)^T = A^T \otimes B^T$ $\quad$ y $\quad$
			$(A\otimes B)^* = A^* \otimes B^*$.
			
			2. $(A\otimes B)^\dagger = A^\dagger \otimes B^\dagger$.
			
			3. $(AC)\otimes (BD) =(A\otimes B)(C\otimes D)$, donde $C,D\in \mathcal{M}_n(\mathbb{C})$.
		\end{proposition}
		
		Como consecuencia inmediata de la segunda afirmación se deduce el siguiente
		
		\begin{corollary} \label{col: matrices unitarias}
			Si $U$ y $V$ son matrices unitarias, entonces su producto tensorial $U\otimes V$ también es una matriz unitaria.
		\end{corollary}
		
		%\begin{comment}
			\begin{proof}
					Como $U$ y $V$ son unitarias se tiene que $U^{-1}=U^\dagger$ y $V^{-1}=V^\dagger$. Usando las propiedades 2 y 3, tenemos
			%\begin{equation*}
			$	\left( U\otimes V\right)^{-1} = U^{-1} \otimes V^{-1} = U^\dagger \otimes V^\dagger = (U\otimes V)^\dagger $.
			%\end{equation*}
			%quedando así demostrada la afirmación presentada.
			\end{proof}		
		%\end{comment}

		%		\begin{corollary}
		%			Sean $A$ y $B$ matrices semejantes a $C$ y $D$ (todas ellas cuadradas), respectivamente. Entonces el producto $A\otimes B$ es semejante a $C\otimes D$.
		%		\end{corollary}
		
		%		\begin{proof}
		%			Partimos de la hipótesis de que $A=P^{-1}CP$ y $B=Q^{-1}DQ$. Usando las propiedades 2 y 7,
		%			\begin{equation*}
		%				(P\otimes Q)^{-1}(C\otimes D)(P\otimes Q) = (P^{-1}\otimes Q^{-1})(CP\otimes DQ) = (P^{-1}CP)\otimes(Q^{-1}DQ) = A\otimes B ,
		%			\end{equation*}
		%			lo que prueba que $A\otimes B$ y $C\otimes D$ son matrices semejantes.
		%		\end{proof}	
		
		Presentamos ahora un teorema cuya demostración puede encontrarse en \cite{Kronecker}: 
		
		%al que llegamos pasando por un par de lemas previos que rezan: (a) el producto tensorial de matrices triangulares superiores es una matriz triangular superior; (b) el producto de Kronecker $A\otimes B$ de dos matrices $A$ y $B$, que son semejantes a $C$ y $D$ respectivamente, es semejante al producto $C\otimes D$.
		
		\begin{theorem}	\label{th: autovalores tensor}
			Sean $A\in \mathcal{M}_{n_1}(\mathbb{C})$ y $B\in \mathcal{M}_{n_2}(\mathbb{C})$ dos matrices cuadradas. Si $\{\lambda_i, \, i=1,...,n_1\}$ y $\{\mu_j, \, j=1,...,n_2\}$ son los espectros de autovalores de $A$ y $B$, respectivamente, entonces el conjunto de autovalores de $A\otimes B$ es el conjunto $\{\lambda_i\mu_j\, | \, i=1,...,n_1, \, j=1,...,n_2 \}$.
			
			Además, el vector  propio de $A\otimes B$ asociado al autovalor $\lambda_i\mu_j$ es el producto tensorial de los vectores propios asociados a $\lambda_i$ y $\mu_j$, para cada par $(i,j)$.
		\end{theorem}
		
		De este teorema, se deduce un interesante resultado:
		\begin{corollary} \label{cor: traza del producto}
			Sean $A\in \mathcal{M}_{n_1}(\mathbb{C})$ y $B\in \mathcal{M}_{n_2}(\mathbb{C})$. Se tiene entonces que \[ \textup{Tr}(A\otimes B) = \textup{Tr}(A) \textup{Tr}(B) = \textup{Tr}(B\otimes A) . \]
		\end{corollary}	
		
		\begin{proof}
			La traza de una matriz es igual a la suma de sus valores propios. Siguiendo las hipótesis del teorema \ref{th: autovalores tensor} se tiene entonces que:\\
			\hspace*{3cm}$\textup{Tr}(A\otimes B) = \displaystyle\sum_{i=1}^{n_1} \displaystyle\sum_{j=1}^{n_2} \lambda_i\mu_j = \displaystyle\sum_{i=1}^{n_1} \lambda_i \displaystyle\sum_{j=1}^{n_2} \mu_j = \textup{Tr}(A)\textup{Tr}(B)$.
		\end{proof}	
		
			
\begin{comment}		
	
	\newpage
	Es conocido que las coordenadas de un vector $v$ vienen descritas por un sólo índice ($v_i\equiv v(i)$), mientras que para las matrices se requieren dos índices distintos ($M_{ij}$). Un \textbf{tensor} surge como una generalización de estos entes matemáticos, para poder tener elementos que ``se mueven" según un número natural $n$ de índices.
	
	Para cada $j\in\{1,...,n\}$, se define un conjunto finito de índice $I_j=\{1,...,p_j\}$, de manera que las entradas de un tensor estarán determinadas de forma única por $d$-uplas de la forma $\textbf{k}=(k_1,...,k_n)\in I_1\times ... \times I_n$, esto es, $T_\textbf{k}=T[\textbf{k}]=T[k_1,...,k_n]$. Esto puede extenderse a conjuntos cualesquiera de índices, $\textbf{I}$, definiéndose\footnote{Todo lo que vamos a tratar de aquí en adelante con espacios complejos, podríamos hacerlo empleando un cuerpo cualquiera $\mathbb{K}$. Sin embargo, el uso posterior obliga a considerar el caso $\mathbb{K}=\mathbb{C}$, así que vamos a acostumbrarnos desde el principio a los espacios vectoriales complejos. Esto es algo que también tendremos que tener en cuenta, por supuesto, cuando hablemos de dimensión de un espacio vectorial.} entonces
		\[ \mathbb{C}^\textbf{I} = \left\lbrace T = \left( T_\textbf{k}\right)_{\textbf{k}\in I} \; | \; T_{\textbf{k}}\in \mathbb{C} \right\rbrace .\]
	
	Siguiendo el ejemplo de los vectores y las matrices, podemos definir las operaciones de suma y producto por un escalar coordenada a coordenada, de manera que el conjunto de tensores tiene estructura algebraica de espacio vectorial. Además, es sencillo ver que $\dim \left( \mathbb{C}^\textbf{I}\right) \equiv \dim_{\mathbb{C}}\left( \mathbb{C}^\textbf{I}\right) =|\textbf{I}|$ (el cardinal del conjunto de índices), de manera que, llamando $N=\mathbf{I}$, se tiene que $\mathbb{C}^\mathbf{I} \subset \mathbb{C}^N$.
	
	Si nos centramos en el caso en que $\textbf{I}=I_1\times ...\times I_n$ (que es el que nos ocupará más adelante), podemos relacionar los espacios $\mathbb{C}^{I_j}$ y $\mathbb{C}^\textbf{I}$ por medio del \textbf{producto tensorial}. Diremos que $T\in \mathbb{C}^\textbf{I}$ es el producto tensorial de $v^{(j)}\in \mathbb{C}^{I_j}$ ($1\le j \le n$) si 
	
		\begin{equation}
			T = v^{(1)} \otimes ... \otimes v^{(n)} = \displaystyle\bigotimes_{j=1}^n v^{(j)} \in \mathbb{C}^\textbf{I} , 
		\end{equation}
	
	siendo sus entradas
	
		\begin{equation}
			T_\textbf{k} = T[k_1,...,k_n] = v^{(1)}_{k_1} \cdot ...\cdot v^{(j)}_{k_j} \cdot ... \cdot v^{(n)}_{k_n} \qquad  \forall \textbf{k} \in \textbf{I} . 
		\end{equation}	
		
	El espacio tensorial se escribe entonces como el producto tensorial $\displaystyle\bigotimes_{j=1}^n \mathbb{C}^{I_j}= \mathbb{C}^{I_1}\otimes ... \otimes \mathbb{C}^{I_n}$ de los espacios vectoriales $\mathbb{C}^{I_j}$, y se define como el espacio generado por el sistema 
	\[ S = \left\lbrace v^{(1)}\otimes ... \otimes v^{(n)} \; : \; v^{(j)} \in \mathbb{C}^{I_j}, 1\le j \le n\right\rbrace.  \]
	
	Los elementos de $S$ son los conocidos como \textbf{tensores elementales} y a todos los elementos del espacio tensorial serán los ya presentados tensores. Así, un tensor no será, en general, un producto tensorial, sino una combinación lineal de los tensores elementales.
	
	Un par de hechos que se deducen de esta definición del espacio tensorial es que \[\displaystyle\bigotimes_{j=1}^n \mathbb{C}^{I_j} = \mathbb{C}^\textbf{I} \qquad \text{y} \qquad \dim\left(\displaystyle\bigotimes_{j=1}^n \mathbb{C}^{I_j} \right) = \displaystyle\prod_{j=1}^{n} \dim\left( \mathbb{C}^{I_j} \right).\]
	
	Obsérvese que si $|I_j|=d$, entonces $\dim(\mathbb{C}^{I_j})=d$ para $j\in\{1,...,d\}$, de manera que la dimensión del espacio tensor es $d^n$. Conviene aclarar que cuando $n=1$, esto es, cuando estamos tratando con vectores como los que conocemos de siempre, los tensores representarán vectores columnas de dimensión $d$.
	
	A continuación, listamos algunas propiedades del espacio tensorial $V\otimes W$ :
		\begin{equation} \label{eq: propiedades producto tensorial}
			\begin{array}{lll}
				(\lambda v)\otimes w = v\otimes (\lambda w)= \lambda(v\otimes w)\qquad & & \forall \lambda \in \mathbb{C}, v \in V, w\in W \\
				(v'+v'')\otimes w = v'\otimes w +v''\otimes w \qquad  &  & \forall v',v'' \in V, w\in W	 \\
				v \otimes (w'+w'')= v\otimes w' + v\otimes w'' \qquad & & \forall v\in V, w,w'' \in W \\
				0\otimes w = v \otimes 0 = 0 \qquad & &  \forall v\in V, w \in W	
			\end{array} 
		\end{equation}
		

		\subsubsection{Producto de Kronecker}
		
		Para $n=2$, puede demostrarse que el espacio de matrices $\mathbb{C}^{I_1\times I_2}$ y el espacio tensorial $\mathbb{C}^{I_1}\otimes\mathbb{C}^{I_2}$ son isomorfos. Para ello, se define el isomorfismo $\mathcal{M} : \mathbb{C}^{I_1}\otimes\mathbb{C}^{I_2} \longrightarrow \mathbb{C}^{I_1\times I_2}$, dado por $\mathcal{M}(v\otimes w) = v w^T$. En este apartado procederemos a describir cómo se multiplican las matrices, vistas como tensores.
		
		Sean $n$ pares de espacios vectoriales $V_j$ y $W_j$ ($1\le j \le n$), entre los cuales definimos sendas transformaciones lineales $A^{(j)} : V_j \longrightarrow W_j$.
		
		Si consideramos los correspondientes espacios tensoriales $\mathbf{V}=\bigotimes^n_{j=1}V_j$	y $\mathbf{W}=\bigotimes^n_{j=1}W_j$, ¿cómo se traduce esto para las aplicaciones $A^{(j)}$?
		Su producto tensorial, llamado \textbf{producto de Kronecker}, es también una transformación lineal 
		
		\[ \mathbf{A} := \displaystyle\bigotimes_{j=1}^n A^{(j)} : \mathbf{V} \longrightarrow \mathbf{W}\]
		
		definida por 
		
			\begin{equation}
				\displaystyle\bigotimes_{j=1}^n v^{(j)} \in \mathbf{V}  \quad \longmapsto \quad \mathbf{A} \left( \displaystyle\bigotimes_{j=1}^n v^{(j)}  \right) = \displaystyle\bigotimes_{j=1}^n \left( A^{(j)} v^{(j)}\right) \in \mathbf{W} 
			\end{equation} 
		
		En el caso en que $V_j=\mathbb{C}^{I_j}$ y $W_j=\mathbb{C}^{J_j}$, las transformaciones $A^{(j)}$ son matrices de $\mathbb{C}^{I_j\times J_j}$. Si tomamos el ejemplo en que $n=2$, sean $I_1=\{1,...,p_1\}$, $I_2=\{1,...,p_2\}$, $J_1=\{1,...,q_1\}$, $J_2=\{1,...,q_2\}$ e $\mathbf{I}=I_1\times I_2$, $\mathbf{J}=J_1\times J_2$. Entonces, el producto tensorial $A\otimes B \in \mathbb{C}^{\mathbf{I}\times\mathbf{J}}$ de las matrices $A\in \mathbb{C}^{I_1\times J_1}$ y $B\in \mathbb{C}^{I_2\times J_2}$ se puede escribir en forma de bloques, como
		
			\begin{equation} \label{eq: producto tensorial bis}
				A \otimes B =\left(  \begin{array}{ccc}
					a_{11}B 	 & \hdots & a_{1q_1}B \\
					\vdots  	 & \ddots  & \vdots \\
					a_{p_11}B  & \hdots & a_{p_1q_1}B
				\end{array}						\right) 
			\end{equation}
	
		Algunas propiedades \label{lab: propiedades del producto de Kronecker} esenciales que pueden deducirse de esta expresión (algunas análogas a las que vimos en \ref{eq: propiedades producto tensorial}, puesto que $A$ y $B$ no dejan de ser tensores) se recogen a continuación:
		
		1. $A \otimes 0_{q_1,q_2} = 0_{p_1,p_2}\otimes B = 0_{p_1q_1,p_2q_2} $ , donde $0_{r,s}$ es una matriz $r\times s$ llena de ceros.
		
		2. $(A\otimes B)^{-1} = A^{-1} \otimes B^{-1}$ $\quad$, $\quad$
			$(A\otimes B)^T = A^T \otimes B^T$ $\quad$ y $\quad$
			$(A\otimes B)^* = A^* \otimes B^*$.
			
		3. $(A\otimes B)^\dagger = A^\dagger \otimes B^\dagger$,  como consecuencia de lo anterior.
		
		4. $\alpha A \otimes \beta B = \alpha\beta (A\otimes B)$.
		
		\noindent Si consideramos distintas matrices en cada espacio, $A_1,A_2\in \mathbb{C}^{I_1\times J_1}$ y $B_1,B_2\in\mathbb{C}^{I_2\times J_2}$:
		
		5. $(A_1+A_2)\otimes B = (A_1 \otimes B) + (A_2\otimes B)$ y $A \otimes (B_1 + B_2) = (A\otimes B_1) + (A\otimes B_2)$.
		
		\noindent Y si, además, restringimos al caso de las matrices cuadradas, se tiene:
		
		6. 	$I_p \otimes I_q = I_{pq}$.
		
		7. $(A_1A_2)\otimes (B_1B_2) =(A_1\otimes B_1)(A_2\otimes B_2)$.
		
		8. $A \otimes B = (A\otimes I_q)(I_p\otimes B) = (I_p\otimes B)(A\otimes I_q)$.
		
	De estas propiedades se deducen varios resultados que convendrá conocer en las páginas que siguen a esta introducción matemática:
	
		\begin{corollary} \label{col: matrices unitarias II}
			Si $U$ y $V$ son matrices unitarias, entonces su producto tensorial $U\otimes V$ también es una matriz unitaria.
		\end{corollary}
	
		\begin{proof}
			Como $U$ y $V$ son unitarias se tiene que $U^{-1}=U^\dagger$ y $V^{-1}=V^\dagger$. Usando las propiedades 2 y 3, tenemos
			%\begin{equation*}
			$	\left( U\otimes V\right)^{-1} = U^{-1} \otimes V^{-1} = U^\dagger \otimes V^\dagger = (U\otimes V)^\dagger $.
			%\end{equation*}
			%quedando así demostrada la afirmación presentada.
		\end{proof}
	
%		\begin{corollary}
%			Sean $A$ y $B$ matrices semejantes a $C$ y $D$ (todas ellas cuadradas), respectivamente. Entonces el producto $A\otimes B$ es semejante a $C\otimes D$.
%		\end{corollary}
			
%		\begin{proof}
%			Partimos de la hipótesis de que $A=P^{-1}CP$ y $B=Q^{-1}DQ$. Usando las propiedades 2 y 7,
%			\begin{equation*}
%				(P\otimes Q)^{-1}(C\otimes D)(P\otimes Q) = (P^{-1}\otimes Q^{-1})(CP\otimes DQ) = (P^{-1}CP)\otimes(Q^{-1}DQ) = A\otimes B ,
%			\end{equation*}
%			lo que prueba que $A\otimes B$ y $C\otimes D$ son matrices semejantes.
%		\end{proof}	
	
	A continuación, presentamos un teorema \cite{Kronecker} al que llegamos pasando por un par de lemas previos que rezan: (a) el producto tensorial de matrices triangulares superiores es una matriz triangular superior; (b) el producto de Kronecker $A\otimes B$ de dos matrices $A$ y $B$, que son semejantes a $C$ y $D$ respectivamente, es semejante al producto $C\otimes D$.
	
		\begin{theorem}	\label{th: autovalores tensor II}
			Sean $A$ y $B$ un par de matrices cuadradas (tensores de $\mathbb{C}^{I_1\times I_1}$ y $\mathbb{C}^{J_1\times J_1}$). Si $\{\lambda_i, \, i=1,...,|I_1|\}$ y $\{\mu_j, \, j=1,...,|J_1|\}$ son los espectros de autovalores de $A$ y $B$, respectivamente, entonces el conjunto de autovalores de $A\otimes B$ es el conjunto $\{\lambda_i\mu_j, \, i=1,...,|I_1|, \, j=1,...,|J_1| \}$.
		
			Además, el vector  propio de $A\otimes B$ asociado al autovalor $\lambda_i\mu_j$ es el producto tensorial de los vectores propios asociados a $\lambda_i$ y $\mu_j$, para cada par $(i,j)\in I_1 \times J_1$.
		\end{theorem}

	De este teorema, cuya demostración puede encontrarse en \cite{Kronecker}, se deduce un interesante resultado:
		\begin{corollary} \label{cor: traza del producto II}
			Sean $A\in \mathbb{C}^{I_1\times I_1}$ y $B\in \mathbb{C}^{J_1\times J_1}$. Se tiene entonces que \[ \textup{Tr}(A\otimes B) = \textup{Tr}(A) \textup{Tr}(B) = \textup{Tr}(B\otimes A) . \]
		\end{corollary}	

		\begin{proof}
			La traza de una matriz es igual a la suma de sus valores propios. Siguiendo las hipótesis del teorema \ref{th: autovalores tensor} se tiene entonces que:\\
			\hspace*{3cm}$\textup{Tr}(A\otimes B) = \displaystyle\sum_{i=1}^{|I_1|} \displaystyle\sum_{j=1}^{|J_1|} \lambda_i\mu_j = \displaystyle\sum_{i=1}^{|I_1|} \lambda_i \displaystyle\sum_{j=1}^{|J_1|} \mu_j = \textup{Tr}(A)\textup{Tr}(B)$.
		\end{proof}
	
\end{comment}	
	
		
%		1. Para espacios de Hilbert: Si H1 y H2 tienen bases ortonormales {φk} y {ψl}, respectivamente, 
%				entonces {φk⊗ψl} son una base ortonormal para H1⊗H2.

			

	

\newpage
	
\section{Introducción a la Computación Cuántica} \label{sec: intro CC}
	  
Ahora que conocemos los conceptos matemáticos que aparecerán incesantemente, abordamos los aspectos más esenciales de la Computación Cuántica, sin los cuales no seríamos capaces de comprender los algoritmos que se presentarán posteriormente.

	\subsection{Postulados de la Mecánica Cuántica }
	
	Todo el formalismo de la Mecánica Cuántica puede describirse a partir de unos pocos postulados que resultan de la experimentación, y su validez se asume debido a que toda la teoría que deriva de ellos está en total consonancia con la Naturaleza. Estos postulados \cite{Zettili}, además de servir como puente para entender la relevancia de la sección \ref{sec: fundamentos matematicos}, suponen la base para contruir el formalismo nmatemático para describir los procesos físicos.
	
		\paragraph{Postulado 1}\label{Par: Postulado 1} \textbf{Estado de un sistema}
			
			Un sistema físico está asociado a un espacio de Hilbert complejo y separable, y un estado puro del sistema (en el instante $t$) viene descrito por un vector unitario, representado por un ket $\ket{\psi(t)} $ de dicho espacio. \footnote{ Se deduce el \textbf{Principio de Superposición}: cualquier superposición de estados puros es, a su vez, un estado puro del sistema, entendiéndose superposición como una combinación lineal $\sum_n  a_n |\psi_n(t)  \rangle $ con coeficientes complejos tales que $\sum_n |a_n|^2=1 $ .}
			  
		\paragraph{Postulado 2}\label{Par: Postulado 2} \textbf{Observables y operadores}
			
			Todo observable de un sistema físico (posición, momento, potencial, etc.) se representa por un operador lineal autoadjunto actuando en el espacio de Hilbert asociado, cuyos vectores propios forman una base completa del mismo. 	
			  
		\paragraph{Postulado 3}\label{Par: Postulado 3} \textbf{Medida de los observables}
			
			La medida de un observable $A$ (sobre un cierto estado $\ket{\psi(t)}$) se representa por la acción del operador asociado sobre el correspondiente vector unitario. Por tanto, los únicos valores posibles son los valores propios $\{a_n\}$ (que son reales) del operador. Si el resultado de la observación es $a_k$, el estado colapsa inmediatamente, proyectándose sobre el subespacio propio asociado a $a_k$, generado por los vectores propios $\left\{\ket{a_k^{(i)}}\right\}$:
			
				\begin{equation}
				  A \ket{\psi(t)} = a_k \ket{a_k^{(i)}}
			    	 	\quad \Rightarrow \quad 
			  		 \ket{\psi(t')} = \frac{P_{A,a_k} \ket{\psi(t)}}{\| P_{A,a_k} \ket{\psi(t)}\|}
				\end{equation}
									
			donde $P_{A,a_k} = \displaystyle\sum_i \ket{a_k^{(i)}} \bra{a_k^{(i)}}$, siendo discreto el espectro de $A$.
			  
		\paragraph{Postulado 4}\label{Par: Postulado 4} \textbf{Resultado probabilístico de la medida}
			
			Si el observable $A$ tiene un espectro discreto, la probabilidad al medirlo de obtener el autovalor $a_k$ (posiblemente degenerado) viene dado por
			
		   		\begin{equation}
		    		p(a_k)=\frac{ \sum_i \left| \ip{a_k^{(i)}}{\psi} \right|^2 }{\ip{\psi}{\psi}}
		   		\end{equation}	
			
		\paragraph{Postulado 5}\label{Par: Postulado 5} \textbf{Evolución temporal de un sistema}			
			
			La evolución temporal de un estado $\ket{\psi(t)}$ está regida por la ecuación de Schrödinger independiente del tiempo:
		   		\begin{equation}
		    		i \hbar \frac{ \partial \ket{\psi(t)} }{ \partial t} = \mathcal{H} \ket{\psi(t)} \; ,
 		   		\end{equation}
			  
			donde $\mathcal{H}$ es el operador hamiltoniano, que describe la energía total del sistema.
			\\
		
		\subsubsection{El operador densidad} \label{subsubsec: densidad}
			  
		El formalismo de la Mecánica Cuántica, tal y como lo hemos presentado, deriva predicciones estadísticas sobre un conjunto (una colección) de sistemas físicos idénticamente preparados, todos ellos caracterizados por un mismo estado $\ket{\psi}$. Por ejemplo, imaginemos un haz de átomos de plata, todos con el mismo estado de espín, como el que resulta del experimento de Stern-Gerlach. En este caso, el haz está polarizado; sin embargo, sabemos que previo al experimento, la orientación de los espines no está polarizada, tenemos un conjunto completamente aleatorio de estados.
		
		¿Cómo tratar, pues,  la situación en que haya distintos estados para los sistemas de la preparación? En tal caso, se definen los \textbf{estados mezcla} como una colección de sistemas dentro de la cual, una fracción de los elementos presentan un estado $\ket{\psi_1}$ y la fracción restante presenta otro, $\ket{\psi_2}$.	Matemáticamente, el estado mezcla se describe por medio de la conocida como \textbf{matriz de paridad}\footnote{ Para un estado mezcla donde aparecen $k$ estados distintos, definiríamos la matriz de paridad como $\rho = \sum_{i=1}^k \omega_i \op{\psi_i}{\psi_i}$, con la ligadura $\sum_{i=1}^k \omega_i=1$.} : 
		%
			\begin{equation}\label{eq: Paridad}
				\rho=\omega_1 \op{\psi_1}{\psi_1} + \omega_2 \op{\psi_2}{\psi_2}  \qquad \text{donde } \; \omega_1+\omega_2=1 .
			\end{equation}
		%
		Los pesos $\omega_1$ y $\omega_2$ serían las proporciones (expresadas en tanto por uno) de cada uno de los estados presentes en la mezcla. Obsérve que en caso de que sólo hubiera un estado, el operador densidad $\rho=\op{\psi_1}{\psi_1}$ estaría refieriéndose a un estado puro. Este operador es autoadjunto y cumple la condición de normalización $\text{Tr}(\rho)=1$.
		
		Ahora que contamos con esta nueva herramienta para describir una situación más general que la anterior, es necesaria una reformulación de los postulados 3 y 4 recogidos en la página anterior:
		%
		\paragraph{Postulado 3'} \label{Par: Postulado 3.2}
			Si un sistema físico está en un estado mezcla descrito por una matriz de densidad $\rho$ y medimos un observable $A$, obteniéndose el valor propio $a$, el sistema se transforma en un estado mezcla descrito por la matriz de densidad
			%
			\begin{equation}
					\rho_{A,a}=\frac{P_{A,a}\rho P_{A,a}}{\text{Tr}(\rho P_{A,a})}
			\end{equation} 
		
		\paragraph{Postulado 4'} \label{Par: Postulado 4.2}
			Si un sistema físico se encuentra en un estado descrito por una matriz de densidad $\rho$, entonces la probabilidad de obtener el valor propio $a$ de un observable $A$ es
			%
			\begin{equation}
				p_a = \text{Tr}(\rho P_{A,a})
			\end{equation}
				
			
	\subsection{Qubits y puertas cuánticas}
		
	A continuación, pasamos a describir con detalle los que serán los elementos esenciales de la Computación Cuántica: el qubit y las diferentes puertas cuánticas. Gracias a ellos, es posible realizar una analogía muy clara entre circuitos eléctricos clásicos y los circuitos cuánticos que trataremos de simular.
						
		\subsubsection{El qubit: la unidad básica de información}
			
		Es conocido que la forma más simple de almacenar información en un ordenador clásico es el \textbf{bit}, que toma valores binarios, esto es, que solo puede encontrarse en dos estados (0 y 1). Además, dichos estados son físicamente realizables con componentes muy sencillos (interruptor abierto/cerrado, bombilla encendida/apagada, etc.) y, de ahí que, para observar en qué estado se halla, baste con observar el bit. 
				
		La estructura análoga en Computación Cuántica es el \textbf{qubit}, o bit cuántico, dado por un sistema cuántico que presenta dos estados básicos $\ket{0}$ y $\ket{1}$ (pensemos, por ejemplo, en un átomo que puede estar en su estado energético fundamental o en un estado excitado, o en un electrón con dos posibles estados de espín), de los que hay que destacar que conforman un sistema ortonormal.
				
		La clave de esto es que, mientras el bit clásico (llamémoslo $b$) es un elemento $b\in\{0,1\} \cong \mathbb{Z}_2$, el qubit es un vector unitario en un espacio de Hilbert $\mathcal{H}$ complejo y separable, como rezaba el \nameref{Par: Postulado 1}. Así, el qubit puede encontrarse en uno de los estados básicos o una superposición $\ket{\psi} \in \mathcal{H}$ cualquiera de ellos:
			%	
			\begin{equation} \label{eq: Superposicion}
				\ket{\psi} = \alpha_1 \ket{0} + \alpha_2 \ket{1} \quad / \quad |\alpha_1|^2+|\alpha_2|^2=1					
			\end{equation}
			%	
		Nace así, de forma natural, el modelo matemático que nos permitirá trabajar con este nuevo elemento: los vectores $\ket{0}$ y $\ket{1}$ constituyen una base del espacio de Hilbert asociado al sistema, que en nuestro caso será $\mathcal{H} = \mathbb{C}^2$. 
				
		A partir de ahora, identificaremos la base $\mathcal{B}_1=\{\ket{0},\ket{1}\}$ con la base canónica, $ \{(1,0),(0,1)\} $, de manera que podremos escribir los estados en superposición (\ref{eq: Superposicion}) en la forma $ (\alpha_1,\alpha_2)$.
				
		Conviene, antes de continuar, presentar una segunda base de $\mathbb{C}^2$ que también es relevante y conocida. Se trata de aquella formada por los ``estados equiprobables":
			%		
			\begin{equation}\label{eq: Equiprobables}	
				\ket{+} = \frac{\ket{0}+\ket{1}}{\sqrt{2}} \overset{\mathcal{B}_1}{=} 
					\left( \begin{matrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{matrix} \right) \quad , \quad
				\ket{-}  = \frac{\ket{0}-\ket{1}}{\sqrt{2}} \overset{\mathcal{B}_1}{=} 
						\left( \begin{matrix} \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}} \end{matrix} \right)	
				\end{equation}
	
		En conclusión, los qubits pueden existir en todo un continuo de estados entre $|0\rangle$ y $|1\rangle$ hasta el instante en que son observados, cuando colapsan, pero nunca podremos conocer (por medio de la observación) las amplitudes que los describen.
						
		\subsubsection{Operando sobre los qubits: puertas y medida} \label{subsubsec: puertas}
			
		Una vez hemos presentado la forma de representar la información, es inevitable preguntarse cómo podemos manipularla: dado que los estados son vectores de $\mathbb{C}^2$, las operaciones que emplearemos con ellos vienen dadas por matrices de $\mathbb{C}^{2\times 2}$. A estas operaciones las llamaremos \textbf{puertas cuánticas}, volviendo al paralelismo con la computación clásica y sus puertas lógicas.
				 
		Sin embargo, no todas las matrices son válidas: al aplicar una puerta cuántica sobre un qubit, se obtiene un nuevo estado, de forma que la ligadura $ |\alpha_1|^2+|\alpha_2|^2=1 $ ha de cumplirse antes y después de la aplicación. Como la norma ha de conservarse (los vectores son unitarios), las puertas cuánticas sólo podrán venir dadas por matrices unitarias\footnote{ Si $U$ es una puerta cuántica y $U\ket{\psi}=\ket{\psi'}$, se tiene $ 1 = \ip{\psi'}{\psi'} = \melem{\psi}{U^{\dagger}U}{\psi} = \ip{\psi}{\psi}  \; \Rightarrow \; U^{\dagger}U=I .$}.
		Se tiene además el recíproco: toda matriz unitaria puede actuar como una puerta cuántica (en particular, la identidad).
		
		A continuación, se presentan las puertas cuánticas\footnote{Físicamente, estas puertas se implementan buscando el hamiltoniano $\mathcal{H}$ (ver \nameref{Par: Postulado 5}) que consiga modificar el sistema como nos interesa: cambiar su nivel de energía, invertir su espín, etc.} (sobre un qubit) que jugarán algún papel dentro del algoritmo con el que vamos a trabajar:
		
		\begin{flushleft}{\textbf{Matrices de Pauli }}\end{flushleft}
			Estas tres matrices, muy empleadas en Mecánica Cuántica, suponen los cimientos sobre los que se construye el formalismo estabilizador, que estudiaremos más adelante. Son las siguientes:
		%
				\begin{equation} \label{eq: Pauli}
					X \equiv \sigma_x = \left(\begin{array}{cc}0 & 1\\1 & 0\end{array}\right)		\quad , \quad
					Y \equiv \sigma_y = \left(\begin{array}{cc}0 & -i\\i & 0\end{array}\right)		 \quad , \quad
					Z \equiv \sigma_z = \left(\begin{array}{cc}1 & 0\\0 & -1\end{array}\right)		
				\end{equation}		 
			%	 
			Las puertas $X$ y $Z$ gozan de gran relevancia y  tienen una interpretación muy sencilla de comprender:
		
			$X$ es el equivalente cuántico de la puerta lógica NOT: convierte $\ket{0} $ en $\ket{1}$ y viceversa. Sus valores propios son +1 y -1, siendo sus autovectores asociados los estados equiprobables, $\ket{+} $ y $\ket{-} $, respectivamente. Resumiendo:
			%
				\begin{equation} \label{eq: Pauli X}
					X \left( \begin{matrix} \alpha_1 \\ \alpha_2 \end{matrix} \right) =  \left( \begin{matrix} \alpha_2 \\ \alpha_1 \end{matrix} \right)			\qquad , \qquad
					\begin{aligned}X\ket{+} =&+\ket{+}  \\X\ket{-} =&-\ket{-} \end{aligned}	\qquad 	
				\end{equation}	
			%
			Por su parte, lo que hace $Z$ es invertir el signo de la amplitud asociada al vector $\ket{1} $, dejando invariante la amplitud del otro. Sus vectores propios son los estados básicos, $\ket{0}$ y $\ket{1}$, correspondientes a los valores propios +1 y -1. En definitiva,
			%
			\begin{equation} \label{eq: Pauli Z}
				Z  \left( \begin{matrix} \alpha_1 \\ \alpha_2 \end{matrix} \right) =  \left( \begin{matrix} \alpha_1 \\ - \alpha_2 \end{matrix} \right)			\qquad , \qquad
				\begin{aligned}Z\ket{0} =&+\ket{0}  \\Z\ket{1} =&-\ket{1} \end{aligned}		\;\qquad.			
			\end{equation}	
			
		\begin{flushleft}{\textbf{Puerta Hadamard}}\end{flushleft}
			
			Se trata de una matriz que convierte la base de los estados básicos en la base de los estados equiprobables (\ref{eq: Equiprobables}). Como H es su propia inversa, resulta que también transforma los estados equiprobables en los estados básicos:
			%
				\begin{equation}\label{eq: Hadamard}
					H = \frac{1}{\sqrt{2}}\left(\begin{array}{cc}1 &  1\\1 & -1\end{array}\right)		\qquad , \qquad
					\begin{aligned}H\ket{0} =& \ket{+}\\H\ket{1}=& \ket{-}\end{aligned}	\qquad
					\begin{aligned}H\ket{+}=& \ket{0}\\H\ket{-}=& \ket{1}\end{aligned}
				\end{equation}	
			
		\begin{flushleft}{\textbf{Puerta de fase}}\end{flushleft}	
			
			Se trata de un caso particular de las denominadas puertas de desplazamiento de fase, $R_{\phi}$. Estas puertas dejan invariante el estado base $\ket{0}$, pero añaden una cierta fase a $\ket{1}$. No modifican la probabilidad de medir un estado básico o el otro.
			La puerta de fase $P$, aquella que nos interesa, es la que aporta una fase de $\phi=\frac{\pi}{2}$ :
			%
				\begin{equation}\label{eq: Fase}
					R_{\phi} = \left(\begin{array}{cc}1 &  0\\0 & e^{i\phi}\end{array}\right)	\quad \Rightarrow \quad
					P = \left(\begin{array}{cc}1 &  0\\0 & i\end{array}\right)				 \qquad , \qquad
					\begin{aligned} P\ket{0}=& \; \ket{0} \\ P\ket{1}=& i\ket{1}\end{aligned} \qquad
				\end{equation}		
				
			Otros ejemplo de puertas de desplazamiento de fase serían la puerta $R_{\frac{\pi}{4}}$ (que llamamos puerta $\frac{\pi}{8}$), o la puerta $Z$, que resulta ser $Z=R_{\pi})$ .	
			
		\begin{flushleft}{\textbf{Medida del qubit }}\end{flushleft}	
		
			Después de haber manipulado un qubit, aplicando sobre él las puertas cuánticas convenientes y relacionándolo con otros qubits del circuito, decidimos que queremos medir su estado. Este proceso se representará con una puerta de medida. 	
				
			Como consecuencia del \nameref{Par: Postulado 3} y el \nameref{Par: Postulado 4}, cuando observemos un qubit, éste colapsará al estado $\ket{0}$ con probabilidad $|\alpha_1|^2$ o al estado $\ket{1}$ con probabilidad $|\alpha_2|^2$.
			
			Las ``puertas'' que se emplean para medir los qubits son, por tanto, los proyectores asociados a cada uno de los estados básicos.
			Sin embargo, la medida no podrá ser considerada como puerta cuántica, ya que es singular y no es unitaria. Pero, ¿qué problema hay con que sea singular?
			Dado que una puerta viene dada por una matriz unitaria $U$, su inversa será su matriz adjunta $U^{\dagger}$; así, cualquier puerta cuántica está representada por una matriz que ha de ser forzosamente reversible. De hecho, esta es una de las grandes ventajas respecto de la Computación Clásica que se introducen con la Computación Cuántica: una vez se ha realizado una operación sobre el qubit, podemos deshacer nuestros pasos sin más que aplicar la misma operación invertida.
			
	
				
		\subsubsection{Extensión a sistemas de varios qubits}
		
		Comencemos pensando lo que ocurre con bits clásicos. Si tomamos el caso en que sólo tenemos 2 bits (cada uno de ellos con dos estados posibles $\{0,1\}$), entonces el sistema conjunto podrá adoptar cualquiera de las 4 combinaciones distintas que podemos formar con ellos : 00, 01, 10, 11 (dos interruptores abiertos, uno abierto y otro cerrado, etc.).
		
		Volviendo a los bits cuánticos, el planteamiento es el mismo: un sistema de 2 qubits podrá encontrarse en los estados básicos $\ket{00}$, $\ket{01}$, $\ket{10}$ y $\ket{11}$. Sin embargo, el Principio de Superposición vuelve a abrir una brecha entre bits clásicos y cuánticos: si el sistema puede hallarse en cualquiera de los estados básicos, también puede estar en una superposición de ellos.
		%
			\begin{equation} \label{eq: Superposicion2}\notag
				|\psi\rangle = \alpha_1 \ket{00} + \alpha_2 \ket{01} + \alpha_3 \ket{10} + \alpha_4 \ket{11} \quad / \quad |\alpha_1|^2+|\alpha_2|^2+|\alpha_3|^2+|\alpha_4|^2=1					
			\end{equation}
				
		Generalicemos esto para un sistema de $n$ qubits:
		
		Los posibles estados básicos en los que podemos encontrar el sistema se corresponden con todas las combinaciones de longitud $n$ posibles que podemos crear a partir de valores binarios. En el caso clásico, los estados son elementos de $\{0,1\}^n \cong \mathbb{Z}^n$. 
		
		Para el caso cuántico, la formulación matemática es algo más compleja: cada estado básico del sistema de n qubits vendrá dado por el producto tensorial de los diferentes estados básicos de las componentes que lo conforman. Por ejemplo, tomando $n=6$, un posible estado básico sería $\ket{001011}=\ket{0} \otimes\ket{0} \otimes\ket{1} \otimes\ket{0} \otimes\ket{1} \otimes\ket{1}$. Se deduce, por tanto, que los estados básicos de un sistema de $n$ qubits (y por tanto cualquier superposición de ellos) serán elementos del espacio vectorial complejo de dimensión $2^n$ obtenido como el producto tensorial $\mathcal{T}_{0,n}(\mathbb{C}^2)=\mathbb{C}^2 \otimes \overset{n}{...} \otimes \mathbb{C}^2 \equiv \left( \mathbb{C}^2\right)^{\otimes n} $.
		
		Siguiendo \eqref{eq: base tensorial}, sabemos que podemos obtener la base de $\mathcal{T}_{0,n}(\mathbb{C}^2)=\left( \mathbb{C}^2\right)^{\otimes n}$ calculando los productos tensoriales de las $2^n$ combinaciones posibles de estados básicos. Ordenaremos la base siguiendo la denominada ``ordenación canónica'': si empezamos a contar desde 0, cada estado se halla en la posición que le corresponde por su traducción de código binario al sistema decimal. 
		
		Lo comprenderemos mejor con un ejemplo.	Un sistema de 3 qubits se representa con el espacio 8-dimensional $\mathbb{C}^2\otimes \mathbb{C}^2 \otimes \mathbb{C}^2$. Lo describiremos con la siguiente base (ordenada):
		%
			\begin{equation} \notag
				\{\ket{000},\ket{001},\ket{010},\ket{011},\ket{100},\ket{101},\ket{101},\ket{111}\}
			\end{equation}
			%
		Así, un estado cualquiera de este sistema vendrá expresado como 
		%
			\begin{equation}  \notag
			\ket{\psi_3} = \alpha_1 \ket{000} + \alpha_2 \ket{001} + \alpha_3 \ket{010} + \alpha_4 \ket{011} + \alpha_5 \ket{100} + \alpha_6 \ket{101} + \alpha_7 \ket{110} + \alpha_8 \ket{111} \, ,
			\end{equation}
		%	
		donde los cuadrados de los módulos de los coeficientes suman 1.
		
		Es aquí donde se puede empezar a atisbar otra de las bondades de la Computación Cuántica frente al modelo clásico: con $n$ bits clásicos podemos representar un sólo estado, mientras que si empleamos $n$ qubits, podemos almacenar $2^n$ estados diferentes a la vez.
		
		\begin{flushleft}{\textbf{Puertas de varios qubits }}\end{flushleft}
		
			Al igual que antes, sentimos la necesidad de aprender a manipular los qubits con las distintas puertas cuánticas. Ahora, es normal preguntarse si hay alguna manera de relacionar los qubits que componen el sistema, haciendo que el estado de unos afecten a lo que ocurre con otros. 
		
			En Computación Clásica, son conocidas puertas lógicas como AND, OR, XOR, NAND y NOR, que toman dos bits de entrada y devuelven uno de salida aplicando una cierta operación lógica sobre los valores que reciben. El prototipo de puerta cuántica para varios qubits es \textbf{cNOT} (o \textbf{\textit{controlled}-NOT}, también llamada \textit{controlled}-$X$). Ésta se aplica sobre un par de qubits: uno de ellos, $a$, es el elemento de control, mientras que el otro, $b$, es el objetivo (o \textit{target}). La puerta cambia el valor lógico de $b$ sólamente cuando el qubit de control se encuentra en el estado básico $\ket{1}$. Por supuesto, esta puerta admite una representación matricial como cualquier otra:
				\begin{equation} \label{eq: CNOT}
					U_{CNOT} = \left(\begin{array}{cccc}
					1 & 0 & 0 & 0\\
					0 & 1 & 0 & 0\\
					0 & 0 & 0 & 1\\
					0 & 0 & 1 & 0
					\end{array}\right)										\qquad \;	,	\;\qquad
					%
					\begin{tabular}{| c | c | c | }
							\hline
							%\multicolumn{3}{ |c| }{$\textrm{CNOT}|ab\rangle=|a\ b\oplus a\rangle $} \\ \hline
								a & b & cNOT$|ab\rangle=|a\ b\oplus a\rangle$ \\ \hline
								0 & 0 & $|00\rangle$ \\
								0 & 1 &  $|01\rangle$  \\
								1 & 0 &  $|11\rangle$ \\
								1 & 1 &  $|10\rangle$ \\ \hline
					\end{tabular}			\qquad
				\end{equation}
			
			Esta puerta no es más que la versión cuántica de la puerta XOR, que aplica la suma exclusiva (o suma modulo 2) de los bits que recibe. Sin embargo, no deja de ser un caso particular de toda una familia de puertas, las \textbf{\textit{controlled-U}}, que dejan invariante el qubit de control y aplican la puerta unitaria $U$ sobre el \textit{target} cuando el primero se halla en el estado $\ket{1}$:
				\begin{equation} \label{eq: C-U}
					U = \left(\begin{array}{cc}
					x_1 & x_2 \\
					x_3 & x_4 
					\end{array}\right)											\qquad \;	\Rightarrow 	\;\qquad
					%
					cU \equiv \left(\begin{array}{cccc}
					1 & 0 &   0   &   0		\\
					0 & 1 &   0   &   0	    \\
					0 & 0 & x_1  & x_2 	   \\
					0 & 0 & x_3 & x_4 
					\end{array}\right)							 \qquad 	
				\end{equation}
				
			En esta misma línea, existen incluso puertas con varios qubits de control. Estas puertas modifican el estado del target en caso de que todos los controles se encuentren en \ket{1}. El ejemplo más destacable es la puerta de Toffoli cuántica, que no es más que una $ccZ$ (\textit{controlled-controlled-}$Z$).
		
			¿Y qué ocurre cuando haya que aplicar una \textbf{puerta sobre un sólo qubit}? Previamente, presentamos una serie de puertas cuánticas que estaban representadas por matrices de $\mathbb{C}^{2\times 2}$, adecuadas para alterar un solo qubit. Sin embargo, ahora los estados están representados por vectores de dimensión $2^n$, de manera que estas matrices dejan de ser válidas. Será necesario recurrir a matrices, todavía unitarias, de $\mathbb{C}^{2^n\times2^n}$.
		
			Cada vez que queramos aplicar alguna puerta sobre algún qubit, hemos de aplicar el producto tensorial de $n$ puertas, cada una sobre su correpondiente qubit. En caso de que queramos aplicar $U$ sobre uno y solamente uno de los $n$ qubits, digamos el $j$-ésimo, la operación consistirá en el producto tensorial de $n-1$ matrices identidad por la matriz $U$ (esta última en la posición $j$). Para operar con más qubits, hacemos lo mismo, colocando matrices identidad en las posiciones correspondientes a los qubits que queramos dejar invariantes. Veamos un ejemplo: 
		
			Dado un sistema de $n=3$ qubits y un estado $\ket{\psi_3}$ como el antes visto, queremos aplicar, a la vez, una puerta $X$ sobre el primer qubit y una Hadamard sobre el tercer qubit. Llamaremos $U$ al operador con el que modificamos el sistema completo (que será una matriz cuadrada de dimensión $2^3=8$), y escribiremos cada puerta acompañada de un subíndice denotando el qubit sobre el que actúan. $U$ viene dada, entonces, por
				\begin{equation*} \label{eq: Ejemplo Puertas}
					U = X_1 \otimes I_2 \otimes H_3 = \left[\left(\begin{array}{cc}
					0 & 1\\
					1 & 0
					\end{array}\right)\otimes\left(\begin{array}{cc}
					1 & 0\\
					0 & 1
					\end{array}\right)\right]\otimes\ H_3
					=\left(\begin{array}{cccc}
					0 & 0 & 1 & 0\\
					0 & 0 & 0 & 1\\
					1 & 0 & 0 & 0\\
					0 & 1 & 0 & 0
					\end{array}\right)\otimes\frac{1}{\sqrt{2}}\left(\begin{array}{cc}
					1 & 1\\
					1 & -1
					\end{array}\right)
				\end{equation*}
				\begin{equation*} 
					\Longrightarrow \quad
					U \ket{\psi_3} = \frac{1}{\sqrt{2}}\left(\begin{array}{cccccccc}
					0 & 0 & 0 & 0 & 1 & 1 & 0 & 0\\
					0 & 0 & 0 & 0 & 1 & -1 & 0 & 0\\
					0 & 0 & 0 & 0 & 0 & 0 & 1 & 1\\
					0 & 0 & 0 & 0 & 0 & 0 & 1 & -1\\
					1 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
					1 & -1 & 0 & 0 & 0 & 0 & 0 & 0\\
					0 & 0 & 1 & 1 & 0 & 0 & 0 & 0\\
					0 & 0 & 1 & -1 & 0 & 0 & 0 & 0
					\end{array}\right)
					%
					\left(\begin{array}{c}
					\alpha_{1}\\
					\alpha_{2}\\
					\alpha_{3}\\
					\alpha_{4}\\
					\alpha_{5}\\
					\alpha_{6}\\
					\alpha_{7}\\
					\alpha_{8}
					\end{array}\right) =
					%
					\frac{1}{\sqrt{2}} \left(\begin{array}{c}
					\alpha_{4}+\alpha_{5}\\
					\alpha_{4}-\alpha_{5}\\
					\alpha_{7}+\alpha_{8}\\
					\alpha_{7}-\alpha_{8}\\
					\alpha_{1}+\alpha_{2}\\
					\alpha_{1}-\alpha_{2}\\
					\alpha_{3}+\alpha_{4}\\
					\alpha_{3}-\alpha_{4}
					\end{array}\right)
				\end{equation*}
			
			Vemos que lo que hacen estas puertas, aunque no actúen sobre todos los qubits, es modificar las amplitudes (y por tanto las probabilidades) con las que se presentan cada uno de los vectores de la base.
		
	\subsection{Circuitos cuánticos}
	
	En las páginas que preceden, hemos descubierto los elementos que se emplean en Computación Cuántica con fines diversos (cálculo, simulaciones, etc.).  Para utilizarlos, podemos integrarlos en un ``circuito cuántico'', como en el ejemplo que se representa en la siguiente figura:
	
		\begin{figure}[htb]
			\[
			\Qcircuit @C=.7em @R=.5em @! {
				\lstick{\ket{0}} & \gate{X} & \qw & \ctrl{1} & \multigate{1}{U} &\qw & \meter & \control \cw\\
				\lstick{\ket{1}} & \qw & \gate{Z} & \targ & \ghost{U} & \qw & \meter & \cwx\\
				\lstick{\ket{\psi}} & \gate{H} & \ctrl{-1} & \qw & \qw & \qw & \gate{X} \cwx & \gate{Z} \cwx & \rstick{\ket{\psi'}} \qw 			}
			\]
			\caption{Ejemplo de circuito cuántico}
			\label{fig: Circuito Cuantico}
		\end{figure}
	
	Desgranemos su contenido:
	
	A cada qubit del sistema le hacemos corresponder una línea horizontal, sobre la que se situarán las distintas acciones que pretendemos realizar sobre él. Los circuitos cuánticos se interpretan de izquierda a derecha y, leyendo según este orden, podemos distinguir tres fases diferenciadas:

		\begin{enumerate}
			\item La preparación de estados previa a la ejecución del circuito. A la izquierda de cada una de las líneas mencionadas, se indica el estado inicial del qubit correspondiente. Es habitual partir desde el estado básico \ket{00...00}, pero conviene indicarlo siempre.
			
			\item El cuerpo del circuito, compuesto por todas las puertas lógicas que actúan sobre los qubits y las diferentes relaciones que se establecen entre ellos. Es en esta parte cuando el estado de los $n$ qubits se ve alterado.
			
			\item La salida que resulta del circuito. Aunque se parte conociendo el estado de $n$ qubits, puede interesarnos un número menor de estados como output. Esto es lo que ocurre en la Figura \ref{fig: Circuito Cuantico}, ya que solo obtenemos el estado final del tercer qubit, habiendo ``perdido'' los dos restantes.
		\end{enumerate}
	
	Sigamos con esta exploración sobre los circuitos cuánticos pasando por las diferentes puertas cuánticas que podemos representar:
	
		\begin{figure}[h]
			\centering
			 \begin{subfigure}[h] {0.3\textwidth} 
				\[
					\Qcircuit @C=.7em @R=.5em @! {
					& \gate{X} & \gate{Y} & \gate{Z} & \qw \\ }
				\]
				\caption{Matrices de Pauli}
			\end{subfigure}     
		%
			\begin{subfigure}[h] {0.2\textwidth} 
				\[
					\Qcircuit @C=.7em @R=.5em @! {
					& \gate{H} & \qw	}
				\]
				\caption{Hadamard}
			\end{subfigure}		
		%
			\begin{subfigure}[h] {0.2\textwidth} 
				\[
					\Qcircuit @C=.7em @R=.5em @! {
					& \gate{P} & \qw	}
				\]
				\caption{Fase}
			\end{subfigure}			
		%
			\begin{subfigure}[h] {0.2\textwidth} 
				\[
					\Qcircuit @C=.7em @R=.5em @! {
					& \meter & \cw	}
				\]
				\caption{Medida}
			\end{subfigure}				
			\caption{Puertas cuánticas sobre un sólo qubit.}
			\label{fig: Puertas Cuanticas 1}
		\end{figure}
	
	La puerta de medida merece especial atención, ya que aparece un elemento nuevo: un ``cable clásico''. Avanzamos en \ref{subsubsec: puertas} que, ante la medida del qubit, éste colapsa sobre uno de los dos estados básicos, \ket{0} ó \ket{1}, y esta operación es la única irreversible entre todas las que podemos encontrar. Estos hechos fuerzan la necesidad de una forma alternativa de representar el canal por el que comunicamos el estado de dicho qubit. Así, la puerta de medida siempre irá seguida de una doble línea horizontal delatando este nuevo cable.
	
		\begin{figure}[h]
			\centering
			\begin{subfigure}[h] {0.27\textwidth} 
				\[
					\Qcircuit @C=1.3em @R=1.1em @! {
					& \ctrl{1} & \qw  & \qw \\
					& \targ & \targ  & \qw \\
					& \qw & \ctrl{-1}  & \qw \\	}
				\]
				\caption{Puertas cNOT}
			\end{subfigure}	
		%
			\begin{subfigure}[h] {0.35\textwidth} 
				\[
					\Qcircuit @C=1.3em @R=0.3em @! {
					& \ctrl{2} & \qw  & \qw \\
					& \qw  & \gate{U} & \qw \\
					& \gate{U} & \ctrl{-1}  & \qw \\	}
				\]
				\caption{Puertas \textit{controlled-U} }
			\end{subfigure}	
		%
			\begin{subfigure}[h] {0.3\textwidth} 
				\[
					\Qcircuit @C=1em @R=.2em {
					& \multigate{2}{U} & \qw  & \qw & \qw & \multigate{3}{V} & \qw \\
					& \ghost{U}& \qw & \qw  & \qw & \ghost{V} & \qw  \\
					& \ghost{U} & \qw  & \multigate{1}{A} & \qw  & \ghost{V} & \qw \\
					& \qw & \qw & \ghost{A}  & \qw & \ghost{V} & \qw 			}
				\]
				\caption{\centering Puertas sobre varios qubits }
			\end{subfigure}	
			\caption{Puertas cuánticas sobre varios qubits}
		\end{figure}

	En las puertas controladas se observa que hay una línea vertical uniendo las líneas correspondientes a los qubits que asocia. El punto negro se coloca sobre el cable correspondiente al qubit de control, mientras que el otro extremo es el \textit{target}. La operación realizada por la puerta cNOT se representa por $\oplus$, aunque podríamos poner una puerta $X$ en su lugar.

		
	\subsection{El formalismo estabilizador} \label{subsec: formalismo estabilizador}
	
	La comunicación de información, codificada en qubits, está sujeta a un ruido que puede alterar el estado de alguno de ellos, provocando que el mensaje de llegada difiera de aquel que envió el emisor. Para tratar de corregir estos posibles errores, existe multitud de códigos de detección y corrección de errores.
	
	Dentro de estos códigos, podemos encontrar un tipo de gran relevancia, que va a suponer el centro en torno al cual gira este trabajo: los códigos estabilizadores. Detrás de ellos, se esconde todo un formalismo que encuentra en la teoría de grupos su principal apoyo.
	
		\subsubsection{El grupo estabilizador}
		
		Volviendo a las definiciones que dimos en la base teórica sobre grupos, hay una que no dimos entonces porque era oportuno reservarla para este apartado:
		
			\begin{definition} \label{def: estabilizador}
				Dado un grupo $G$ y un conjunto $X$, podemos considerar para cada $x \in X$
					\[ \textup{Stab}(x)=\{g \in G \; | \; \text{ac}(g,x)=x\}, \]
				al que llamaremos \textbf{estabilizador} de $x$ en $G$ (o grupo de isotropía de $G$).
			\end{definition}
		
		Esta definición es directamente aplicable a la situación que necesitamos abordar: dado un estado puro $\ket{\psi}$, decimos que la matriz unitaria $U$ \textbf{estabiliza} a $\ket{\psi}$ si $\ket{\psi}$ es un vector propio de $U$ con valor propio 1, esto es, $U\ket{\psi}=\ket{\psi}$. 
		Esto será coherente con la definición \ref{def: estabilizador} si consideramos la acción por traslación de $U_\mathbb{C}(2^n)\equiv \mathcal{U}_n$ sobre $\left( \mathbb{C}^2\right)^{\otimes n}$.
		
		El estabilizador de un cierto estado puro $\ket{\psi}$ en $\mathcal{U}_n$ constituye un subgrupo dentro del grupo unitario $\mathcal{U}_n$ al que llamaremos \textbf{grupo estabilizador} y notaremos $\text{Stab}\left(\ket{\psi}\right).$ Comprobemos que cumple los requisitos para ser un subgrupo:
		
		Es claro que la matriz identidad, $I$, estabiliza cualquier estado puro. Obsérvese también que si dos matrices $U$ y $V$ estabilizan un estado $\ket{\psi}$, también lo harán su producto y sus matrices inversas:
			\begin{equation*}
				\left.\begin{aligned} U\ket{\psi} =& \ket{\psi}  \\ V\ket{\psi}=&\ket{\psi} \end{aligned}\right\} \quad \Rightarrow \quad UV\ket{\psi}=U\left( V\ket{\psi} \right)=U \ket{\psi}= \ket{\psi}	
			\end{equation*}
			\begin{equation*}
				U\ket{\psi} = \ket{\psi} \quad \Rightarrow \quad \ket{\psi} = U^{-1}U\ket{\psi} = U^{-1} \left( U \ket{\psi} \right) = U^{-1} \ket{\psi}
			\end{equation*}
	
		Esta idea puede extenderse: un grupo puede estabilizar más de un estado. Dado un grupo estabilizador $G$, denominaremos \textbf{código estabilizador} asociado a $G$ al subespacio $\mathcal{V}:= \{ \ket{\psi} : \sigma\ket{\psi}=\ket{\psi} \; \forall \sigma \in G \}$ formado por los estados que deja invariante.
	
		La idea fundamental del formalismo estabilizador es abandonar la representación de un estado cuántico $\ket{\psi}$ a partir de un vector de amplitudes (como hemos estado haciendo hasta ahora), para pasar a representarlo a partir de su grupo estabilizador.\footnote{En lugar de hacer evolucionar el vector que describe al sistema, pasamos a interesarnos por la evolución de los operadores que actúan sobre él. Así, abandonamos una representación de Schrödinger para adoptar una representación de Heisenberg.} Esto podría parecer poco práctico, puesto que se pasa de necesitar $2^n$ coeficientes, en el primer caso, a $2^{2n}$ para representar cada una de las matrices en el segundo. Sin embargo, vamos a ver que somos capaces de encontrar una representación mucho más compacta. 
		
		Es el momento de recordar las matrices de Pauli que conocimos en \ref{eq: Pauli}. Es sencillo comprobar que se cumplen las siguientes igualdades:
	
			\begin{equation} \label{eq: Pauli igualdades}
				\begin{aligned} X^2 = & +I \\ XY = & +iZ \\  YX = & -iZ \end{aligned}
													\qquad \qquad		
				\begin{aligned} Y^2 = & +I \\ YZ = & +iX \\  ZY = & -iX \end{aligned}
													\qquad \qquad
				\begin{aligned} Z^2 = & +I \\ ZX = & +iY \\  XZ = & -iY \end{aligned}
			\end{equation}
	
		En particular, se tiene que las matrices de Pauli conmutan o anticonmutan dos a dos. A la vista de estas igualdades, resulta evidente que la identidad, junto con las matrices de Pauli, multiplicadas por $\{\pm1,\pm i\}$, forman un subgrupo de $U_\mathbb{C}(2)$ (con el producto de matrices).
	
		En caso de que tratemos con un sistema de $n$ qubits, los operadores que se pueden formar a partir de ellas vienen dados por los diferentes productos tensoriales de $n$ matrices de Pauli (o identidad) posibles. A estos operadores los llamaremos \textbf{operadores de Pauli sobre $n$  qubits}. Por las propiedades del producto tensorial, se garantiza que cualquier producto de estos operadores da lugar a otro operador de Pauli. En definitiva, la identidad y los operadores de Pauli sobre $n$ qubits, multiplicados por un factor de $\{\pm 1,\pm i\}$  también forman un grupo:
	
			\begin{equation} \label{eq: Grupo de Pauli}
				\mathcal{P}_n = \left\{ u \cdot \displaystyle\bigotimes_{i=1}^{n} P_i 		\; / \;
				u\in\{\pm1,\pm i\} \; , \,	P_i \in \{I,X,Y,Z\}  \ \forall i \in \{1,...,n\} \right\}		\,
			\end{equation}
		
		A este grupo lo llamaremos \textbf{Grupo de Pauli} sobre $n$ qubits, que consta, claramente, de $|\mathcal{P}_n|=4^{n+1}$ elementos. La ley de composición interna con la que se define este grupo no es otra que la composición de operadores, esto es, el producto de matrices unitarias de dimensión $2^n$.
	
		Después de estas definiciones, estamos en condiciones de presentar el siguiente teorema \cite{Aaronson}, que nos aporta el grupo de operadores con el que vamos a trabajar.
		
			\begin{theorem} \label{th: Caracterizacion}
				Dado un estado $\ket{\psi}$ de n qubits, las siguientes afirmaciones son equivalentes:
				\begin{enumerate}[label=(\roman*),itemsep=1pt]
					\item \label{th: CaracterizacionI}$\ket{\psi}$ se puede obtener a partir del estado $\ket{0}^{\otimes n}$ por medio de únicamente la acción de puertas cNOT, Hadamard y de fase.
					\item \label{th: CaracterizacionII}$\ket{\psi}$ se puede obtener a partir del estado $\ket{0}^{\otimes n}$ por medio de únicamente la acción de puertas cNOT, Hadamard, de fase y de medida.
					\item \label{th: CaracterizacionIII} $\ket{\psi}$ es estabilizado por exactamente $2^n$ operadores de Pauli.
					\item \label{th: CaracterizacionIV} $\ket{\psi}$ queda completamente determinado por $S \left( \ket{\psi} \right) = \text{Stab}\left( \ket{\psi}\right) \bigcap \mathcal{P}_n$ (grupo de los operadores de Pauli que estabilizan a $\ket{\psi}$) .
				\end{enumerate}
			\end{theorem}
		
		Las afirmaciones \ref{th: CaracterizacionIII} y \ref{th: CaracterizacionIV} resumen, pues, la representación que tomaremos a partir de ahora: para hablar de un estado de $n$ qubits, usaremos el grupo de $2^n$ operadores de Pauli que lo estabilizan. Como un grupo $G$ cualquiera tiene un sistema generador de tamaño $\log_2|G|$ (proposición \ref{prop: generadores}), nos bastará con tener $n$ operadores para describir $S\left( \ket{\psi}\right) $ por completo. Veámoslo con un ejemplo para el caso $n=3$:
		
		Tomando el estado $\ket{011}$, por ejemplo, es sencillo darse cuenta que el grupo que lo estabiliza es 
			\begin{equation*}
				S\left( \ket{011} \right) =\{ I,\, Z_1,\, -Z_2,\, -Z_3,\, -Z_1Z_2,\, -Z_1Z_3,\, Z_2Z_3,\, Z_1Z_2Z_3 \},
			\end{equation*}
		
		pero es sufiente con conocer únicamente tres de sus elementos, ya que el resto se pueden generar a partir de ellos: $S\left( \ket{011} \right) = \left\langle Z_1, \, -Z_2, \, -Z_3 \right\rangle$.
		
		Prestemos atención a la notación que vamos a emplear a partir de ahora: se omitirán los símbolos de producto tensorial, $\otimes$ y se indicará a qué qubit está afectando la puerta que se escriba. Además, en caso de que sea la identidad la que está actuando sobre un qubit, también será omitida. Veamos algunos ejemplos para comparar y comprender mejor esta novedad:
			\begin{equation*} 
			Z_1 \equiv Z_1\otimes I_2 \otimes I_3 \quad , \quad
					-Z_1Z_3 \equiv Z_1\otimes I_2 \otimes (-Z_3) \quad , \quad I \equiv I_1\otimes I_2 \otimes I_3 
			\end{equation*}
		
		Antes de acabar, hay que destacar que el único código estabilizado por la matriz $-I$ es el subespacio trivial, por lo que nunca la incluiremos en el grupo estabilizador de los estados que representaremos. También es interesante darse cuenta de que \textbf{$\mathcal{S} \equiv S\left( \ket{\psi}\right)$ es un grupo abeliano}, sin importar el estado $\ket{\psi}$ que consideremos \cite{Bermejo VandenNest}.  Para ver esto, pensemos en un estado $\ket{\psi}\ne 0$ que es invariante bajo la acción por traslación de todo elemento de $\mathcal{S}$ y consideremos $\sigma,\tau \in \mathcal{S}$ dos operadores de Pauli arbitrarios del estabilizador. Por las igualdades \ref{eq: Pauli igualdades}, sabemos que $\sigma \tau = u \tau \sigma$, donde $u\in \{\pm1,\pm i\}$. Aplicando dos veces la definición de estabilizador se sigue $\ket{\psi}=\sigma\tau\ket{\psi}=u\tau\sigma\ket{\psi}=u\ket{\psi}$, de manera que $\ket{\psi}=u\ket{\psi}$ y se tiene forzosamente que la fase $u=+1$. Se tiene por tanto que $\sigma\tau=\tau\sigma$. 
		
			\subsubsection*{Puertas de Clifford}
			
			En el Teorema \ref{th: Caracterizacion}, en \ref{th: CaracterizacionI} y \ref{th: CaracterizacionII} concretamente, se entiende que serán las puertas cNOT, Hadamard y de fase aquellas que vamos a emplear de aquí en adelante. ¿Por qué éstas y no otras? Resulta que, al aplicar estas puertas, conseguimos que los operadores de Pauli que representan nuestro estado cuántico permanezcan dentro del grupo de Pauli (ahondaremos sobre esto en la sección \ref{sec: GottesmanKnill}), de ahí su capital importancia para los códigos estabilizadores.
			
			Debido a su relevancia, les pondremos nombre: las puertas cNOT, Hadamard y de fase son las conocidas como \textbf{puertas de Clifford}, y los circuitos cuánticos que cuentan únicamente con ellas para operar serán los circuitos de Clifford. Si, además, incluimos también puertas de medida, se obtienen los denominados \textbf{circuitos estabilizadores}, llamando 
			\textbf{estado estabilizador} a cualquier estado obtenido desde $\ket{0}^{\otimes n}$ por medio de un circuito estabilizador. Veremos algunos ejemplos en \ref{subsubsec: ejemplos estabilizador}.
			
			Un apunte necesario sobre las puertas de Clifford tiene que ver con la \textbf{complejidad} de los circuitos. El siguiente resultado supone una pieza fundamental en la simulación de circuitos cuánticos:
			
			\begin{theorem}[Gottesman-Knill] \label{th: GottesmanKnill}
				Sea un circuito cuántico constituido únicamente por puertas de Clifford y medidas dentro del grupo de Pauli, siendo la preparación uno de los estados básicos del sistema. Dicho circuito puede simularse de forma eficiente en un ordenador clásico.
			\end{theorem}
			
			Por tanto, este teorema nos garantiza que toda simulación que hagamos de circuitos estabilizadores implicará tiempos de orden polinomial, siendo realista plantearse la realización clásica de dichas simulaciones.

			Además de los circuitos estabilizadores, hay otro ámbito en el que las puertas de Clifford adquieren relevancia: la \textbf{universalidad}. En computación clásica, decimos que una puerta lógica es universal si toda operación puede ser implementada usándola sólamente a ella. Ejemplos de puertas lógicas universales son la puerta NAND, que es irreversible, o la puerta de Toffoli para computación reversible. En Computación Cuántica, esta definición se verá modificada:
			
			\begin{definition}
				Un conjunto de puertas se dice \textbf{universal para la computación cuántica} si toda operación unitaria puede ser aproximada con precisión arbitraria por un circuito cuántico compuesto únicamente por puertas de dicho conjunto.
			\end{definition}
		
			Si bien es cierto que las puertas de Clifford no constituyen un conjunto de puertas universal, sí que forman parte de los conjuntos más sencillos que se pueden formar: las puertas de Clifford, junto con la puerta $\pi/8$ (que, recordemos, es la rotación $R_\frac{\pi}{4}$), forman un conjunto capaz de aproximar tanto como se quiera cualquier operación unitaria. 
			
			Otro ejemplo de conjunto de puertas universal lo constituyen la puerta de Toffoli (algo más compleja que la rotación $\pi/8$) acompañada, también ahora, por las puertas de Clifford. En definitiva, podemos imaginar que, a pesar de no poder representar cualquier circuito, el formalismo estabilizador nos permite estudiar un amplio abanico de ejemplos interesantes. 
			
				
		\subsubsection{Ejemplos de circuitos estabilizadores} \label{subsubsec: ejemplos estabilizador}
		
		El objetivo de este apartado es el de transmitir al lector la importancia de las puertas de Clifford y el formalismo estabilizador. Podría parecer que, restringiendo tanto las operaciones a usar, estaríamos sacrificando algunas de las ventajas fundamentales de la Computación Cuántica, pero vamos a ver que estas herramientas son, de hecho, el origen de muchas de estas ventajas. 
		
		
			\paragraph{1. Generación de estados entrelazados} \hspace{1pt}
		
			Un sistema de varias partículas se dice que está en estado de \textbf{entrelazamiento} cuando es imposible describir cada partícula del sistema de forma independiente sin que ello suponga una pérdida de información. Matemáticamente, esto se traduce en términos de separabilidad: un estado en entrelazamiento no puede expresarse como producto tensorial de los estados de cada uno de los subsistemas.
			
			El primer ejemplo de circuito estabilizador a mostrar consigue generar estados (de $n$ qubits) en entrelazamiento empleando únicamente una puerta Hadamard y $n-1$ puertas cNOT:
			
			\begin{figure}[htb]
				\[ \qquad \qquad \qquad
				\Qcircuit @C=.7em @R=.5em @! { 
					\lstick{\ket{0}} & \gate{H} & \ctrl{1} & \ctrl{2} & \qw &\qw & \dots & & \qw & \ctrl{4} & \ctrl{5} & \qw & \\
					\lstick{\ket{0}} & \qw		  & \targ 	 & \qw		 & \qw &\qw & \dots & & \qw & \qw 	  & \qw       & \qw & \\
					\lstick{\ket{0}} & \qw 		  & \qw 	 & \targ	 & \qw &\qw & \dots & & \qw & \qw 	  & \qw       & \qw & \\ 
					\vdots	 			&				&			  &			    &		 &		 &			& &        &			 &			    &		 & \\
					\lstick{\ket{0}} & \qw		  & \qw		 & \qw 		& \qw &\qw & \dots & & \qw & \targ	  & \qw       & \qw & \\
					\lstick{\ket{0}} & \qw		  & \qw		 & \qw 		& \qw &\qw & \dots & & \qw & \qw	  & \targ     & \qw & 
				{\gategroup{1}{13}{6}{13}{0.8em}{\}} }	}
				\begin{tabular}{c} 
			      \ket{GHZ_n} \vspace{-3.8cm}
				\end{tabular} 
				\]
				\caption{Circuito generador de estados GHZ}
				\label{fig: Generacion GHZ}
			\end{figure}
			
			Este circuito toma como input el estado básico $\ket{0}^{\otimes n}$ y lo transforma en el estado GHZ (Greenberger-Horne-Zeilinger) de $n$ qubits:
			%
				\begin{equation} \label{eq: GHZ}
					\ket{GHZ_n}=\frac{\ket{0}^{\otimes n}+\ket{1}^{\otimes n}}{\sqrt{2}}, \qquad n\ge 2
				\end{equation}
			
			Estos estados son la forma más simple de observar entrelazamiento en sistemas de $n$ qubits. Además de la definición explícita dada en \eqref{eq: GHZ}, podemos dar una descripción muy elegante de ellos gracias a los generadores de su grupo estabilizador:
			%
				\begin{equation}
					S^{(GHZ_n)}_1 = \prod_{k=1}^{n}X_k \qquad , \qquad  S^{(GHZ_n)}_k=Z_{k-1}Z_k \; \text{ para } k\in\{2,...,n\}.
				\end{equation}
			
			En el caso de un sistema de $n=2$ qubits, el circuito presentado nos permite conseguir un conjunto importante de estados, conocido como la base de estados de Bell (o pares EPR\footnote{En honor a Eintein, Podolsky y Rosen, por la ``paradoja'' con el mismo nombre, que ponía de manifiesto las peculiaridades del entrelazamiento cuántico.}). El circuito transforma cada uno de los cuatro estados básicos en un par EPR:
			%
				\begin{equation} \label{eq: Bell}
					\begin{aligned}\ket{00} \mapsto \ket{\beta_{00}} = \frac{\ket{00}+\ket{11}}{\sqrt{2}} 
						\\ \ket{10} \mapsto \ket{\beta_{10}} = \frac{\ket{00}-\ket{11}}{\sqrt{2}}     \end{aligned}	\qquad \qquad
					\begin{aligned}\ket{01} \mapsto \ket{\beta_{01}} = \frac{\ket{01}+\ket{10}}{\sqrt{2}} 
						\\ \ket{11} \mapsto \ket{\beta_{11}} = \frac{\ket{01}-\ket{10}}{\sqrt{2}}     \end{aligned}
				\end{equation}
			
			El entrelazamiento es un recurso muy importante de la Computación Cuántica, ya que son precisamente las correlaciones entre estados las que conducen a su potencia. Además, elementos como los estados de Bell tienen aplicaciones incluso en transmisión de la información, intuyéndose futuras mejoras en criptografía y comunicación.
				
				
			\paragraph{2. Teleportación Cuántica} \hspace{1pt}
			
			Este sorprendente fenómeno, se introdujo en 1993 por medio de una historia \cite{Teleportacion}. Imaginemos que un par de amigos, Alice y Bob, crearon un par EPR $\ket{\beta_{00}}$ antes de que Bob se mudara a un lugar muy lejano. Como recuerdo del otro, cada amigo se quedó con un qubit de los que componen el par EPR: Alice con el primero y Bob con el segundo.
			
			Después de un tiempo, Alice necesita enviar un estado cuántico cualquiera a Bob (por el motivo que sea, no nos metemos en sus asuntos) de la forma $\ket{\psi}=a\ket{0}+b\ket{1}$. Alice no conoce el estado exacto que ha de enviar (no sabe las amplitudes $a$ y $b$), y sólo puede enviarle a Bob información clásica, como cadenas de ceros y unos. ¿Qué puede hacer Alice?
			
			Cuando se repartieron el estado entrelazado, Alice y Bob quedaron conectados para siempre: es el momento de aprovecharlo. Para triunfar en la comunicación, Alice diseña un circuito con el cual Bob podrá decodificar el mensaje a partir de los bits que reciba:
			
				\begin{figure}[htb]
					\[
					\Qcircuit @C=.7em @R=.4em @! {
						& \lstick{\ket{\psi}} & \qw & \qw & \qw & \ctrl{1} & \gate{H} & \qw & \meter & \cw &\control \cw\\
						& \lstick{\ket{0}} & \qw & \targ & \qw & \targ & \qw & \qw & \meter & \control  \cw & \cwx\\
						\text{Bob}\quad & \lstick{\ket{0}} & \gate{H} & \ctrl{-1} & \qw & \qw & \qw & \qw  & \qw & \gate{X}  \cwx & \gate{Z} \cwx & \rstick{\ket{\psi}} \qw \\
						&& \; \qquad \text{Creación de }\ket{\beta_{00}} &&& \overset{\uparrow}{\ket{\psi_1}} &\overset{\uparrow}{\ket{\psi_2}} &&&
						{\gategroup{2}{3}{3}{4}{.7em}{--} }
						\inputgroupv{1}{2}{0.25em}{1.1em}{\text{Alice}}
					}
					\]
					\caption{Circuito para teleportar el estado $\ket{\psi}$.}
					\label{fig: Teleportacion}
				\end{figure}
			
			Veamos cómo este circuito puede ayudarles a teleportar el estado $\ket{\psi}$ como muestra la figura \ref{fig: Teleportacion}. El estado inicial del sistema completo es 
				%
				\begin{equation*}
					\ket{\psi_0} = \ket{\psi}\ket{\beta_{00}} = \frac{1}{\sqrt{2}}\left[ a\ket{0}(\ket{00}+\ket{11}) + b\ket{1}(\ket{00}+\ket{11})\right].
				\end{equation*}
				
			Alice hace pasar sus qubits por una puerta cNOT y una Hadamard, quedando el estado del sistema como sigue:
				%
				\begin{equation*}
						\ket{\psi_1}  = \frac{1}{\sqrt{2}}\left[ a\ket{0}(\ket{10}+\ket{01}) + b\ket{1}(\ket{00}+\ket{11})\right]
				\end{equation*}	
				%
				\begin{equation*}
					\begin{aligned}
						\ket{\psi_2} &= \frac{1}{2}\left[a(\ket{0}+\ket{1})(\ket{00}+\ket{11})+b(\ket{0}-\ket{1})(\ket{10}+\ket{01})\right]  \\
										   &= \frac{1}{2}\left[ \ket{00}(a\ket{0}+b\ket{1}) + \ket{01}(a\ket{1}+b\ket{0}) + \ket{10}(a\ket{0}-b\ket{1}) + \ket{11}(a\ket{1}-b\ket{0}) \right] .
					\end{aligned}
				\end{equation*}
		
			En esta última expresión es donde se ve la \textit{magia} del entrelazamiento: al medir los qubits de Alice (obteniéndose $A_1,A_2$), sabemos en qué estado se encuentra el de Bob y qué operación deberíamos usar para transformarlo en \ket{\psi}. En la tabla \ref{tab: AliceBob}, se estudian los distintos casos que podemos encontrar.
			
				\begin{table}[h!]
					\centering
					\begin{tabular}{| c | c | c | }
						\hline
						%\multicolumn{3}{ |c| }{$\textrm{CNOT}|ab\rangle=|a\ b\oplus a\rangle $} \\ \hline
						$(A_1,A_2)$ & $\ket{\psi_B}$ & Operación a realizar sobre $\ket{\psi_B}$\\ \hline
						00 & $a\ket{0}+b\ket{1}$ & Dejarlo igual. \\
						01 & $a\ket{1}+b\ket{0}$ &  Aplicar una puerta X.  \\
						10 & $a\ket{0}-b\ket{1}$ &  Aplicar una puerta Z. \\
						11  & $a\ket{1}-b\ket{0}$ &  Aplicar ambas puertas X y Z. \\ \hline
					\end{tabular}
					\caption{Transformaciones sobre el qubit de Bob según las medidas de Alice.}
					\label{tab: AliceBob}
				\end{table}
			
			Es inmediato darse cuenta de que las acciones que se explicitan en la tabla se resumen con un un par de puertas controladas que tienen $\ket{\psi_B}$ como target: una $cX$ controlada por el segundo qubit de Alice y una $cZ$ controlada por el primero. Queda así comprobado que el circuito creado por Alice cumple su cometido y puede enviar el estado cuántico que Bob tanto necesitaba. 
			
			Un lector exigente podría ahora quejarse de que en el circuito de la figura \ref{fig: Teleportacion} aparece una puerta $cZ$, que no forma parte del conjunto de puertas de Clifford. Sin embargo, de las igualdades de \eqref{eq: Pauli X}, \eqref{eq: Pauli Z} y \eqref{eq: Hadamard} que aplicar una puerta $cZ$ es equivalente a aplicar la secuencia $H_3 \rightarrow$ cNOT $\rightarrow H_3$, por lo que la promesa de que este circuito fuera estabilizador no se ha roto.
			
			
			\paragraph{3. Códigos de corrección de errores: código de inversión del qubit} 	\label{ej: QEC} \hspace{1cm}
			
			El circuito que nos disponemos a emplear a continuación no es estrictamente un código estabilizador, pero sí da gran protagonismo a las puertas cNOT, además de ser un ejemplo muy ilustrativo para entender cómo el formalismo estabilizador puede aplicarse en códigos de corrección de errores.
			
			Contextualicemos primero con un planteamiento más general, que puede encontrarse en \cite{QEC II}. Supongamos que queremos transmitir un mensaje a través de un canal con ruido. Este canal altera el estado del sistema que queremos enviar de diversas formas, aplicando una puerta $Z$ sobre algún qubit o cambiando un 0 por un 1 (aplicándose una puerta $X$ sobre ese qubit), entre otras. En definitiva, el conjunto de errores que pueden aparecer forma parte del grupo de Pauli, $\mathcal{E}=\{E_a\} \subset \mathcal{P}_n$.
			
			Recordemos que un código estabilizador asociado a un grupo $\mathcal{S}$ venía dado por $\mathcal{V}=\{ \ket{\psi} : \sigma \ket{\psi} = \ket{\psi} , \forall \sigma \in \mathcal{S} \}$, esto es, se trata del subespacio propio asociado al valor propio $+1$ de todos los elementos de $\mathcal{S}$. Considerando $\mathcal{S}$ generado por un sistema $\{\sigma_i\}$, la detección de errores se realiza fijándonos en las relaciones de conmutación de los generadores con cada error:
			
			\begin{itemize}[leftmargin=0.6cm]
			
				\item Si $E_a$ anticonmuta con un cierto generador $\sigma_i \in \mathcal{S}$, se tiene que 
								\[ \sigma_i E_a \ket{\psi} = - E_a\sigma_i \ket{\psi}  =-E_a \ket{\psi},  \]
				por lo que $E_a\ket{\psi}$ es un vector propio de $\sigma_i$ con valor propio $-1$. Se entiende, por tanto, que $E_a\ket{\psi}$	es ortogonal al código $\mathcal{V}$. Así, midiendo cada generador $\sigma_i$, se puede definir un coeficiente $s_{i,a}\in\{0,1\}$, según si $\sigma_i$ y $E_a$ conmutan o anticonmutan:
								\[ \sigma_i E_a = (-1)^{s_{i,a}} E_a \sigma_i \]
				Se obtiene así el vector $\overline{s_a}=(s_{1,a},...,s_{n-k,a})$, que representa el síndrome del error $E_a$ ($k$ es el número de qubits codificados y $n$ el número de qubits empleados en la codificación).			
				
				\item Si $E_a \in \mathcal{S}$, el error no corrompe el sistema y deja el estado dentro del código $\mathcal{V}$.
				
				\item Si $E_a \notin \mathcal{S}$, pero conmuta con todos sus elementos, es decir, $E_a \in Z(\mathcal{S})-\mathcal{S}$. Este caso es el más delicado, pues $E$ modifica los elementos del código $\mathcal{V}$ pero no los saca de él. Así, $E$ será un error indetectable para este código.
				
			\end{itemize}	
		
			Con todo ello, podemos concluir que un código $\mathcal{V}$ puede corregir un conjunto de errores $\mathcal{E}$ si y solo si $E^\dagger_a E_b \in \mathcal{S} \bigcup (\mathcal{P}_n-Z(\mathcal{S}))$, para todo $E_a,E_b \in \mathcal{E}$.
			
			\vspace{0.5cm}
			
			Aprovechando las nociones recién explicadas, proponemos un circuito \cite{YouTube} capaz de corregir un único error del tipo aplicar una puerta $X_i$ sobre un qubit $i$:
			
				\begin{figure}[htb]
				\[
					\Qcircuit @C=.1em @R=.4em @! {
						& \lstick{\ket{\psi}} & \ctrl{1} & \ctrl{2}& \qw & \multigate{2}{E_a} & \qw  & \ctrl{3} & \qw	  &\qw 	   		 & \qw & \qw & \qw & \multigate{2}{\mathcal{C}} & \qw & \ctrl{2} & \ctrl{1} & \rstick{\ket{\psi}} \qw \\	
						%					
						& \lstick{\ket{0}} 	  & \targ    & \qw     & \qw & \ghost{E_a} 			 & \qw  & \qw 		& \ctrl{2} & \ctrl{3}   & \qw & \qw & \qw & \ghost{\mathcal{C}} & \qw & \qw & \targ & \qw &\\\
						%
						& \lstick{\ket{0}}    & \qw     & \targ    & \qw & \ghost{E_a}			 & \qw  & \qw 		& \qw	   & \qw  & \ctrl{2} & \qw & \qw & \ghost{\mathcal{C}} & \qw & \targ & \qw & \qw &  \\
						%
						&& \; \qquad \text{Codificación} &&&  & \lstick{\ket{0}} & \targ & \targ & \qw & \qw & \meter & \push{ \, s_1 \, } \cw &  \control \cw \cwx && \qquad \text{Decodificación}\\
						%
						& & & & & & \lstick{\ket{0}} & \qw & \qw & \targ & \targ & \meter & \push{\, s_2 \, } \cw & \control \cw \cwx &
						%
						{\gategroup{1}{3}{3}{4}{.9em}{--} }
						{\gategroup{1}{16}{3}{17}{.9em}{--} }
					}
					\]
					\caption{Circuito de corrección del error ``invertir un qubit".}
					\label{fig: QEC}
				\end{figure}
			
			Queremos enviar un estado $\ket{\psi}=\alpha \ket{0}+\beta \ket{1}$ a través de un canal con ruido, con la garantía de que dicho estado llegue inalterado al destino. Lo primero que hacemos para conseguirlo es codificar un qubit ``lógico'' con la ayuda de 3 qubits físicos. La codificación que se lleva a cabo es la siguiente:
			%
				\begin{equation*}
					\ket{0}_L = \ket{000}  \; , \;  \ket{1}_L = \ket{111} \qquad \Longrightarrow \qquad \alpha \ket{0}+ \beta \ket{1} \longrightarrow \alpha \ket{000}+ \beta \ket{111} = \ket{\psi}_L.
				\end{equation*}
			
			$\{\ket{0}_L,\ket{1}_L\}$ es la base del código estabilizador $\mathcal{V}$ que vamos a usar. Es inmediato ver que el código está asociado al grupo $\mathcal{S}=\left\langle Z_1 Z_2 , Z_2 Z_3  \right\rangle $ (y tenemos la suerte de que el único error que conmuta con los generadores del grupo es la identidad).
			
			Al atravesar el error (que consideramos localizado entre la codificación y la corrección), puede ocurrir que el estado se quede dentro del código $\mathcal{V}$ o que sea transformado en un vector $E_a\ket{\psi}$ perteneciente a un subespacio $\mathcal{V}^*$(de dimensión 2) ortogonal a él. Midiendo los generadores de $\mathcal{S}$, $Z_1Z_2$ y $Z_2Z_3$, obtendremos dos valores $s_1$ y $s_2$ que constituyen el síndrome que nos revelará cuál ha sido el error. En la tabla \ref{tab: QEC} se valoran las distintas posibilidades.
			
				\begin{table}[h!]
					\centering
					\begin{tabular}{| c | c | c | c | }
						\hline
						Error $E_a$   & Subespacio $\mathcal{V}^*$    										 & $s_1$ & $s_2$\\ \hline
						$I_1I_2I_3$   & $\text{Lin}(\ket{000},\ket{111})= \mathcal{V}$ & 	0	  &  	0     \\
						$X_1I_2I_3$  & $\text{Lin}(\ket{100},\ket{011})$ 					  & 	1	  & 	 0	   \\
						$I_1X_2I_3$  & $\text{Lin}(\ket{010},\ket{101})$ 					  & 	1      & 	 1 	   \\
						$I_1I_2X_3$  & $\text{Lin}(\ket{001},\ket{110})$ 					  &  	0      &  	 1		\\ \hline
					\end{tabular}
					\caption{Efecto de los posibles errores sobre el código.}
					\label{tab: QEC}
				\end{table}		
			
			Conocido el síndrome del error $E_a$, solo falta aplicar una puerta unitaria que vuelva a aplicar el error $E_a$ según proceda, atendiendo al síndrome $(s_1,s_2)$ que recibe por medio de los controles. Ahora ya tendríamos el estado $\ket{\psi}_L$ como antes de sufrir la transformación $E_a$, lo que nos permite recuperar el estado $\ket{\psi}$ que queríamos enviar gracias a una simple decodificación (inversión de las operaciones que aplicamos durante la codificación).
			
			De forma muy similar, podemos construir un circuito capaz de corregir un error del tipo ``cambiar de fase un estado'', que se manifiesta como el paso de un qubit a través de una puerta $Z$. En este caso, tendríamos que cambiar la base a medir a $\{X_1X_2,X_2X_3\}$, cosa que en el circuito se traduciría en poner una puerta Hadamard para cada qubit antes y después del error $E_a$, ya que esto reduce el nuevo problema al recién resuelto.
			
			
			
		\subsubsection{La matriz estabilizadora}
		
		Terminamos la introducción al formalismo estabilizador con una nueva forma de representar el estado en que se halle el sistema. Para ello, hemos de pasar por una reformulación del grupo de Pauli, basada en un recurso al que llamaremos \textit{etiquetas} \cite{Bermejo VandenNest}.
		
		Recordemos que un sistema de $n$ qubits estaba asociado al espacio de Hilbert $\left( \mathbb{C}^2\right) ^{\otimes n}$. Este espacio está, a su vez, asociado al grupo $G=\mathbb{Z}_2^n$ (del que obtenemos los estados básicos), de dimensión $\textfrak{g}=2^n$, por medio de la relación:
		%
		\begin{equation}
			\ket{g} = \ket{g(1)} \otimes ... \otimes \ket{g(n)} \qquad (g \in G),
		\end{equation}
		%
		\noindent donde $g=(g(1),...,g(n))$ es un elemento del grupo. Teniendo esto en mente, podemos escribir las matrices de Pauli $X$ y $Z$ como los siguientes operadores que actúan sobre elementos de $\left( \mathbb{C}^2\right) ^{\otimes n}$:
		%
		\begin{equation}\label{eq: Pauli etiquetas}
			X(g) := \displaystyle\sum_{h \in G} \op{h+g}{h}	\qquad , \qquad Z(g) := \displaystyle\sum_{h \in G} \chi_g(h)\op{h}{h}
		\end{equation}
		%
		donde $g \in G$ y $\chi_g : G \longrightarrow \{-1,+1\}$ es un homomorfismo  definido por
		\begin{equation}\label{eq: character function}
			\chi_g(h) = \exp\left( 2\pi i \displaystyle\sum_{i=1}^n \frac{g(i)h(i)}{2}\right) . 
		\end{equation}
		
		Con esta notación, las componentes $g(i)=1$ se traducirán en que la puerta $X$ o $Z$ actúa sobre el qubit $i$, mientras que $g(j)=0$ quiere decir que no se aplican, esto es, la matriz identidad se encuentra en su lugar. Tomemos, por ejemplo, el elemento $g=(0,1)\in \mathbb{Z}_2^2$ para ver esta nomeclatura en acción:
		%
		\begin{equation*}
			\begin{aligned}
				X(0,1) &= \op{00+01}{00} + \op{01+01}{01} +\op{10+01}{10} + \op{11+01}{11} \\ 
				&= \op{01}{00} + \op{00}{01} + \op{11}{10} + \op{10}{11} \\
				&= \left(\begin{array}{cccc}
					0 & 1 & 0 & 0\\
					1 & 0 & 0 & 0\\
					0 & 0 & 0 & 1\\
					0 & 0 & 1 & 0
				\end{array}\right)	
				= \left(\begin{array}{cc} 1 & 0 \\ 0 & 1	\end{array}\right) \otimes
				\left(\begin{array}{cc} 0 & 1 \\ 1 & 0	\end{array}\right) = I_1 \otimes X_2 \\	
			\end{aligned}						
		\end{equation*}
		%
		\begin{equation*}
			\begin{aligned}
				Z(0,1) &= e^{i\pi \cdot 0}\op{00}{00} + e^{i\pi \cdot 1} \op{01}{01} + e^{i\pi \cdot 0}\op{10}{10} +  e^{i\pi\cdot 1}\op{11}{11} \\ 
				&= +\op{00}{00} - \op{01}{01} + \op{10}{10} - \op{11}{11} \\
				&= \left(\begin{array}{cccc}
					1 & 0 & 0 & 0\\
					0 & -1 & 0 & 0\\
					0 & 0 & 1 & 0\\
					0 & 0 & 0 & -1
				\end{array}\right)	
				= \left(\begin{array}{cc} 1 & 0 \\ 0 & 1	\end{array}\right) \otimes
				\left(\begin{array}{cc} 1 & 0 \\ 0 & -1	\end{array}\right) = I_1 \otimes Z_2
			\end{aligned}			
		\end{equation*}
		
		De esto se deduce que cualquier operador de Pauli podrá reescribirse de la siguiente forma
		%
		\begin{equation}\label{eq: Operador Pauli etiqueta 1}
			T_a = i^{- x_a \cdot z_a} X(x_a) Z(z_a) \; , \qquad x_a, z_a \in \mathbb{Z}_2^n  
		\end{equation}
		%
		o lo que es lo mismo:
		%
		\begin{equation}\label{eq: Operador Pauli etiqueta 2}
			T_a = i^{- x_a \cdot z_a} \left( X_1^{x_{a1}} \otimes ... \otimes X_n^{x_{an}}\right) 
			\left( Z_1^{z_{a1}} \otimes ... \otimes Z_n^{z_{an}}\right) \; , \qquad x_a, z_a
			\in \mathbb{Z}_2^n  
		\end{equation}
		
		Notaremos $(x_a,z_a) \in \mathbb{Z}_2^{2n}$ al vector de etiquetas que determina de forma única al correspondiente operador de Pauli, $T_a$.
		
		Una vez hemos asimilado esta escritura de los operadores de Pauli por medio de etiquetas, podemos presentar el ente matemático con el que vamos a representar el sistema de $n$ qubits y que haremos evolucionar a conveniencia. Ante la falta de convenio en la bibliografía para apodarlo (\textit{check matrix} en \cite{NielsenChuang} o \textit{tableau} en \cite{Aaronson}), el nombre \textbf{matriz estabilizadora} será suficientemente representativo.
		
		La idea es utilizar esta matriz para representar los distintos generadores del grupo $S\left( \ket{\psi}\right)$. Para el caso en que tengamos $n$ qubits, ya sabemos que es suficiente con $n$ generadores para que el grupo quede completamente representado. La matriz será, pues, de dimensión $n\times 2n$: una fila por cada generador, que viene representado por su correspondiente etiqueta. Por ejemplo, en el caso de un sistema de $n=3$ qubits, la matriz estabilizadora que reúne los generadores $T_1$, $T_2$ y $T_3$ vendrá dada por
	
			\begin{equation} \label{eq: matriz estabilizadora 1}
			\begin{array}{cc}
			T_1 &\rightarrow \\ T_2 &\rightarrow \\ T_3 &\rightarrow
			\end{array}
			\left(\begin{array}{ccccccc}
			x_{11}  	  & x_{12}   &  x_{13} 	      & \vline & z_{11}       &  z_{12} &  z_{13}    \\
			x_{21}  	 & x_{22}  &  x_{23} 	   	 & \vline & z_{21}       &  z_{22} &  z_{23}  	  \\
			x_{31}  	 & x_{32}  &  x_{33} 	  	& \vline & z_{31}       &  z_{32} &  z_{33}\\ 
			\end{array}\right)		
			\end{equation}

\newpage

\section{Algoritmo de Gottesman-Knill}	\label{sec: GottesmanKnill}

En esta sección, se propone un método para poder ir averiguando el estado en que se encuentra un sistema a lo largo de su evolución, pasando por puertas cuánticas y sufriendo medidas. A pesar de funcionar correctamente, en la siguiente sección veremos que este algoritmo puede ser mejorado, atendiendo a relaciones de conmutatividad entre operadores.
	
		\subsection{Puertas unitarias en el formalismo estabilizador} 
			\label{subsec: puertas}
		
		En el apartado anterior se desveló una nueva forma de representar los sistemas por medio de su grupo estabilizador. El problema que se plantea con ello es la necesidad de adaptar todo lo que habíamos aprendido hasta ahora, ya que los procesos de medida y aplicación de puertas cuánticas se habían diseñado para trabajar según las amplitudes que presentara cada uno de los vectores de la base.
		
		Supongamos que se aplica una puerta unitaria $U$ sobre el estado $\ket{\psi}$ que, recordemos, estará descrito por el grupo de matrices $S\left( \ket{\psi}\right) \equiv \mathcal{S}$. Entonces, para cualquier elemento $T \in \mathcal{S} $, se tiene
		
			\begin{equation}
				U\ket{\psi} = U T\ket{\psi}=UTU^{\dagger}U\ket{\psi} \, ,
			\end{equation}  
		
		\noindent de manera que el vector $U\ket{\psi}$ es estabilizado por la matriz $UTU^{\dagger}$, de donde se deduce que el grupo $U\mathcal{S}U^{\dagger}=\left\{UTU^{\dagger} / T\in \mathcal{S} \right\}$ estabiliza el estado $U\ket{\psi}$. Es más, si $\mathcal{S}=\left< T_1,T_2,...,T_l\right> $, se tiene que $UT_1U^{\dagger},...,UT_lU^{\dagger}$ generan $U\mathcal{S}U^{\dagger}$. Así, para conocer cómo afectan las puertas sobre el estabilizador, será suficiente averiguar cómo modifican a los generadores del estabilizador.
		
		Necesitamos que la acción por conjugación de una puerta, sobre los generadores del grupo de Pauli, los mantenga dentro del mismo, esto es, $\text{ac}(U,P)=UPU^\dagger \in \mathcal{P}_n, \; \forall P\in\mathcal{P}_n,  \; \forall U\in \mathcal{U}_n$. Por ello, $U$ está en el normalizador $N_{\mathcal{U}_n}(\mathcal{P}_n)$, al cual llamaremos \textbf{grupo de Clifford}. Se puede demostrar \cite{Gottesman}, en el caso $n=1$, que dicho grupo está generado por puertas de Hadamard, fase y cNOT, extendiéndose con productos tensoriales para $\mathcal{P}_n$.
		Podemos ahora comprender que, en el Teorema \ref{th: Caracterizacion}, las puertas mencionadas en \ref{th: CaracterizacionI} y \ref{th: CaracterizacionII} no fueron escritas de forma arbitraria.
		
		Veamos algunos ejemplos de las puertas unitarias que actúan sobre un sólo qubit. Por ejemplo, la puerta de Hadamard afecta a cada una de las matrices de Pauli como se muestra a continuación:
		
			\begin{equation}\label{eq: HadamardX}
				HXH^{\dagger} = \frac{1}{2}  \left(\begin{array}{cc}1 & 1\\1 & -1\end{array}\right)
																 \left(\begin{array}{cc}0 & 1\\1 & 0\end{array}\right)
																 \left(\begin{array}{cc}1 & 1\\1 & -1\end{array}\right)
										   = \frac{1}{2}  \left(\begin{array}{cc}2 & 0\\0 & -2\end{array}\right) = Z					 
			\end{equation}
			
			\begin{equation}\label{eq: HadamardY}
				HYH^{\dagger} = \frac{1}{2}  \left(\begin{array}{cc}1 & 1\\1 & -1\end{array}\right)
																\left(\begin{array}{cc}0 & -i\\i & 0\end{array}\right)
																\left(\begin{array}{cc}1 & 1\\1 & -1\end{array}\right)
										  = \frac{1}{2}  \left(\begin{array}{cc}0 & 2i\\-2i & 0\end{array}\right) = -Y					 
			\end{equation}
		
			\begin{equation}\label{eq: HadamardZ}
				HZH^{\dagger} = \frac{1}{2} \left(\begin{array}{cc}1 & 1\\1 & -1\end{array}\right)
																\left(\begin{array}{cc}1 & 0\\0 & -1\end{array}\right)
																\left(\begin{array}{cc}1 & 1\\1 & -1\end{array}\right)
										   = \frac{1}{2} \left(\begin{array}{cc}0 & 2\\2 & 0\end{array}\right) = X					 
			\end{equation}
			
		Para ver el efecto de las puertas de fase, el procedimiento sería muy similar; y para las puertas de Pauli, podemos utilizar las igualdades \ref{eq: Pauli igualdades} simplificando los cálculos enormemente. Resumimos todos los resultados en la tabla \ref{tab: Estabilizadores Pauli}.
		
		Estos resultados son directamente aplicables al caso en que nuestro sistema esté formado por $n$ qubits. Por ejemplo, el estado $\ket{0}^{\otimes n}$ tiene al grupo $\left\langle Z_1,...,Z_n\right\rangle $ por estabilizador y si hacemos pasar cada uno de los qubits por una puerta Hadamard, el grupo estabilizador del sistema pasaría a ser $\left\langle X_1,...,X_n\right\rangle$.
		
		Avanzando hacia al caso en que la puerta unitaria $U$ relaciona más de un qubit (digamos $k$), hemos de entender que la operación a realizar es $UAU^{\dagger}=B$, donde tanto $A$ como $B$ son matrices de dimensión $2^k$ resultado del producto tensorial de $k$ matrices de Pauli. El ejemplo más obvio es el de la puerta \textit{controlled}-NOT (que representaremos simplemente por $U$):
		
			\begin{equation}\label{eq: CNOT estabilizador}
				\begin{aligned}
				UX_1U^{\dagger} =& 
					\left(\begin{array}{cccc}
						1 & 0 & 0 & 0\\
						0 & 1 & 0 & 0\\
						0 & 0 & 0 & 1\\
						0 & 0 & 1 & 0
					\end{array}\right)	
					\left(\begin{array}{cccc}
						0 & 0 & 1 & 0\\
						0 & 0 & 0 & 1\\
						0 & 1 & 0 & 0\\
						1 & 0 & 0 & 0
					\end{array}\right)	
					\left(\begin{array}{cccc}
						1 & 0 & 0 & 0\\
						0 & 1 & 0 & 0\\
						0 & 0 & 0 & 1\\
						0 & 0 & 1 & 0
					\end{array}\right)	\\ =& 
					\left(\begin{array}{cccc}
						0 & 0 & 0 & 1\\
						0 & 0 & 1 & 0\\
						0 & 1 & 0 & 0\\
						1 & 0 & 0 & 0
					\end{array}\right)	=
					\left(\begin{array}{cc}
						0 & 1 \\
						1 & 0 
					\end{array} \right)	\otimes
					\left(\begin{array}{cc}
						0 & 1 \\
						1 & 0 
					\end{array} \right)	= X_1 \otimes X_2
				\end{aligned}	
			\end{equation}
		
		Podríamos proceder de igual manera con cualquier operador de Pauli que quisiéramos utilizar. Esto es aplicable a cualquier matriz unitaria $U$ que queramos aplicar sobre varios qubits, pero el Teorema \ref{th: Caracterizacion} asegura que sólo vamos a necesitar las puertas cNOT de este tipo. De nuevo, el resto de resultados a considerar se encuentran recogidos en las tablas que presentamos a continuación:
			
			\begin{table}[h]
				\centering
					\begin{tabular}{| c | c | c | }
						\hline
							Puerta & Entrada & Salida \\ 						\hline
							\multirow{3}{*}{$X$}	& $X$ & $X$ \\
																& $Y$ & $-Y$ \\
																& $Z$ & $-Z$  \\	\hline
							\multirow{3}{*}{$Y$}	& $X$ & $-X$ \\
																& $Y$ & $Y$	\\
																& $Z$ & $-Z$ \\		\hline
							\multirow{3}{*}{$Z$}	& $X$ & $-X$ \\
																& $Y$ & $-Y$ \\
																& $Z$ & $Z$ \\		\hline	
							\multirow{3}{*}{$H$}	& $X$ & $Z$ \\
																& $Y$ & $-Y$ \\
																& $Z$ & $Z$ \\ 		\hline												
					\end{tabular}		
				\qquad
					\begin{tabular}{| c | c | c | }
						\hline
							Puerta & Entrada & Salida \\ 						\hline
							\multirow{3}{*}{$P$}	& $X$ & $Y$ \\
																& $Y$ & $-X$ \\
																& $Z$ & $Z$  \\	\hline
							\multirow{9}{*}{cNOT} 	 & $X_1$ & $X_1X_2$ \\ 
																	& $X_2$ & $X_2$ \\ 
																	& $X_1X_2$ & $X_1$ \\ \cline{2-3} 
																	& $Y_1$ & $Y_1X_2$ \\  
																	& $Y_2$ & $Z_1Y_2$ \\ 
																	& $Y_1Y_2$ & $-X_1Z_2$ \\ \cline{2-3} 
																	& $Z_1$ & $Z_1$ \\ 
																	& $Z_2$ & $Z_1Z_2$ \\ 
																	& $Z_1Z_2$ & $Z_2$ \\ \hline																						
					\end{tabular}	
				\caption{Acción de puertas unitarias sobre las matrices de Pauli}
				\label{tab: Estabilizadores Pauli}
			\end{table}		
	
	
		Obsérvese que en la tabla hay información redundante: debido a las relaciones entre las matrices de Pauli (\ref{eq: Pauli igualdades}), conociendo lo que ocurre con dos de ellas se puede calcular lo que ocurre con la tercera. En nuestro algoritmo, se empleará sólamente la información referente a las puertas $X$ y $Z$ sin que esto suponga pérdida alguna de información. 
	
			
		\subsection{La medida en el formalismo estabilizador}\label{subsubsec: medida}
	
		\begin{wrapfigure}[8]{r}{0.34\columnwidth}
			\[
			\Qcircuit @C=.7em @R=.5em @! {
				& \lstick{\ket{0}} & \gate{H} & \ctrl{1} & \gate{H} & \meter\\
				&						& \qw 		 & \gate{T}	&	\qw & \qw 	}
			\]
			\caption{Medida de $T$}
			\label{fig: Medida T}
		\end{wrapfigure}
		
		Asociado a todo operador de Pauli $T \in \mathcal{P}_n$, puede realizarse la medida del mismo sobre su base de vectores propios. Consideramos para ello la descomposición espectral $T=\sum \lambda P_\lambda$. Dado un estado cuántico $\ket{\psi}$, la medida que obtiene el valor propio $\lambda$ ocurre con probabilidad $\left\| P_\lambda \ket{\psi} \right\|^2 $, transformando $\ket{\psi}$ en el estado $P_\lambda\ket{\psi}$.
		
		En la figura \ref{fig: Medida T}, se muestra el circuito empleado para medir (sobre un qubit auxiliar) un qubit en la base de autovectores de un operador $T$ cualquiera.
		
		Pretendemos, por tanto, medir un operador $T \in \mathcal{P}_n$\footnote{Suponemos, sin pérdida de generalidad, que no va acompañado de factor multiplicativo $u\in\{-1,\pm i\}$.}, cuyos valores propios sólo podrán ser $\pm1$. El sistema está en un estado $\ket{\psi}$ estabilizado por $\mathcal{S}=S\left( \ket{\psi}\right) = \left\langle T_1,...T_l\right\rangle$, pueden entonces ocurrir dos cosas:
		
			\begin{flushleft}\textbf{a.} $T$ conmuta con todos los generadores del estabilizador:	\end{flushleft} 
			
			Como $T_jT\ket{\psi}=TT_j\ket{\psi}=T\ket{\psi} \, \forall j=1,...,n$, $T\ket{\psi}$ está también estabilizado por $S\left( \ket{\psi}\right) $ y es, por tanto, un múltiplo de $\ket{\psi}$. Como $T^2=I$, forzosamente $T\ket{\psi}=\pm\ket{\psi}$, de modo que $T$ o $-T$ es un elemento del estabilizador.
			
			Si $+T\in S\left( \ket{\psi}\right) $, se tiene que $T\ket{\psi}=\ket{\psi}$ y de ahí que al medir $T$ se vaya a obtener el valor $+1$ con probabilidad 1. Además, como $\ket{\psi}$ ya se encuentra en el subespacio que proyecta $T$, la medida no alterará el estado del sistema, dejando el grupo estabilizador invariante. Lo mismo ocurriría si fuera $-T$ el que está en el estabilizador, obteniéndose en este caso el valor $-1$ al medir.
			
			En definitiva, si $\mathcal{C}_\mathcal{S}(T)=\mathcal{S}$, su medida será un procedimiento determinista que no altera al grupo estabilizador: $\mathcal{S}'=\mathcal{S}$.
			
			\begin{flushleft}\textbf{b.} $T$ anticonmuta con uno o más generadores del estabilizador y conmuta con el resto:	\end{flushleft} 
			
			Antes de nada, veamos que todo se reduce al caso en que $T$ anticonmuta con sólo uno de los generadores del estabilizador: si $T$ anticonmuta con dos generadores $T_1$ y $T_2$, por ejemplo, resulta que $T$ conmuta con su producto $T_1T_2$. Así, podemos sustituir uno de los dos generadores por el producto (que también es un elemento del grupo estabilizador), dejando sólo un generador que anticonmute con $T$. Podríamos repetir este paso las veces que hiciera falta según cuántos generadores anticonmutasen con $T$.
			
			$T$ tiene dos valores posibles que pueden resultar de su medida, $\pm1$, cuyos proyectores son $\frac{I\pm T}{2}\op{\psi}{\psi}$, respectivamente. Por tanto, las probabilidades (según el \nameref{Par: Postulado 4.2}) de medir cada uno de estos valores propios viene dada por:
			 	%
			 	\begin{equation}
			 		p_{+1} = \text{Tr} \left( \frac{I+T}{2}\op{\psi}{\psi} \right) 	\qquad		p_{-1}=\text{Tr} \left( \frac{I-T}{2}\op{\psi}{\psi} \right) 
			 	\end{equation}
			 
			 Y usando que $T_1\ket{\psi}=\ket{\psi}$ y $TT_1=-T_1T$ llegamos a que 
			 %
			 	\begin{equation}
					p_{+1} = \text{Tr} \left( \frac{I+T}{2}T_1\op{\psi}{\psi} \right) = \text{Tr} \left( T_1 \frac{I-T}{2}\op{\psi}{\psi} \right)
				\end{equation}			 	
		
			Por último, empleamos que $\text{Tr}(ABC)=\text{Tr}(BCA)$ para llevar $T_1$ a la derecha de la expresión y absorberlo con $\bra{\psi}$ (puesto que $T_1=T_1^{\dagger}$). Con ello, deducimos que
			%
				\begin{equation}
					p_{+1} = \text{Tr} \left( \frac{I-T}{2}\op{\psi}{\psi} \right) = p_{-1} 	\qquad		\Rightarrow		\qquad		p_{+1}=p_{-1}=\frac{1}{2}
				\end{equation}
	
			Concluimos, por tanto, que en este caso el resultado de la medida será aleatorio y que, una vez medido $T$ obteniéndose $\pm1$, el estado colapsará convirtiéndose en $\ket{\psi^{\pm}}=\frac{I\pm T}{\sqrt{2}}\ket{\psi}$. Su grupo estabilizador pasará a ser $\mathcal{S}' = \left\langle \pm T, T_2,...,T_n \right\rangle = \left\langle \pm T, \mathcal{C}_\mathcal{S}(T) \right\rangle  $.
		
	\subsection{Implementación y eficiencia del algoritmo} \label{subsec: eficiencia GottesmanKnill}
	
	Antes de nada, es imprescindible introducir algunas nociones sobre complejidad y eficiencia que se emplearán a continuación.
	
	Para determinar si un algoritmo es o no eficiente, necesitamos cuantificar los recursos que consume este, es decir, el número de operaciones que se han de ejecutar para poder llevarlo a cabo. Esto se hace comparando dicho número con el comportamiento asintótico de una cierta función, que suelen ser polinomios, logaritmos y exponenciales.
	
	 \begin{definition}
	 	Sean $f(n)$ y $g(n)$ dos funciones sobre $\mathbb{N}$. Decimos que ``$f(n)$ se encuentra en la clase de funciones $O(g(n))$'', o simplemente ``$f(n)$ es $O(g(n))$'', si existen $c\in\mathbb{R}^+$ y $n_0\in\mathbb{N}$ tales que $f(n)\leq cg(n), \, \forall n > n_0$.
	 \end{definition}
	
	Como es de esperar, la complejidad de un algoritmo en términos de operaciones se traduce en la cantidad de tiempo requerido para ejecutarlo. Diremos que un algoritmo se ejecuta en tiempo $poly(n)$ si, en la definición anterior, $g(n)$ es un polinomio de cualquier grado en n, mientras que tiempo $polylog(n)$ hará a referencia a polinomios en $(\log n)$.\footnote{Esta notación trata con logaritmos en cualquier base, ya que la diferencia no es más que una constante multiplicativa.}
	
	\begin{definition}
		Un algoritmo clásico de simulación es \textbf{eficiente} si se puede llevar a cabo en tiempo $poly(n)$, donde $n$ es el número del qubits del circuito cuántico que se está representando.
	\end{definition}

	Los mejores algoritmos son, entonces, aquellos que presentan un número de operaciones lineal con el número de qubits e incluso inferior (no será el caso con los nuestros). Con respecto a los algoritmos ineficientes, destacaremos que los más frecuentes pertenecen a la clase $O(e^n)$. Tras esta introducción, ya estamos en condiciones de ver lo que ocurre con este algoritmo.
 	
 	\vspace{.35cm}
	
	Comenzando con las puertas cuánticas, poco queda por decir acerca de su manipulación. En \ref{subsec: puertas} ya veíamos que sólo interesa la aplicación de puertas de Clifford, y que al aplicar una puerta $U$ necesitamos \textbf{actualizar todos los generadores} del grupo estabilizador según lo reflejado en la tabla \ref{tab: Estabilizadores Pauli}.
	
	Por tanto, a cada puerta que actúe sobre el sistema, el algoritmo contempla modificar las $n$ etiquetas que representan a los distintos generadores. El siguiente resultado, demostrado en \cite{VandenNest}, aclara el tiempo que se consume durante este proceso:

		\begin{theorem}
			Sea $U$ una puerta normalizadora y $\sigma$ un operador de Pauli expresado en términos de su etiqueta. Se tiene entonces que la etiqueta correspondiente a $U\sigma U^\dagger$ puede ser calculada en tiempo $polylog(\textfrak{g})$.
		\end{theorem}
	
	El teorema se formula para puertas normalizadoras\footnote{Las puertas normalizadoras se definen a partir de ciertos recursos que escapan el contenido de este trabajo. Por ello, sólo diremos que suponen una generalización de las puertas de Clifford.}, que son una generalización de las puertas de Clifford, de manera que se puede aplicar en este contexto. Recordemos que $\textfrak{g}=2^n$ era la dimensión del grupo $\mathbb{Z}^n_2$ y el espacio de Hilbert asociado $\left( \mathbb{C^2}\right)^{\otimes n}$. Por tanto, se deduce que la acción de una puerta sobre un generador se consigue llevar a cabo en un tiempo lineal con el número de qubits. Para completar el proceso con todos los generadores, se requiere un tiempo $O(n^2)$.
	
	\vspace{0.5cm}
	
	Pasemos al proceso de medida. Dado que los vectores propios de la matriz $Z$ son los estados básicos $\ket{0}$ y $\ket{1}$, asociados a los valores propios $+1$ y $-1$ respectivamente, la medida de un qubit $a$ equivale a la medida del operador $Z_a$ (puerta $Z$ actuando sobre dicho qubit). La etiqueta correspondiente a dicho operador es el vector $e_{a+n}\in \mathbb{Z}_2^{2n}$, que tiene todas sus coordenadas nulas salvo la $(a+n)$-ésima, que está ocupada por un 1.
	
	De forma análoga a lo expuesto en la figura \ref{fig: Medida T}, se tiene que el circuito empleado para medir la puerta $Z$ es el siguiente:
	
	\begin{figure}[h!]
		\[
		\Qcircuit @C=.7em @R=.5em @! {
			& \lstick{\ket{0}} & \gate{H} & \ctrl{1} & \gate{H} & \meter&\\
			&						& \qw 		 & \gate{Z}	&	\qw & \qw &	}
		\quad = \quad
		\Qcircuit @C=1.5em @R=0.5em @! {
			& \lstick{\ket{0}}  & \targ  & \meter\\
			&								 & \ctrl{-1} & \qw 	}
		\]
		\caption{Medida de $Z$}
		\label{fig: Medida Z}
	\end{figure}
	
	En el apartado \ref{subsubsec: medida}, vimos que el problema de la medida se reduce a averiguar si el operador $Z_a$ pertenece o no al grupo estabilizador del estado, $\left\langle T_1,...,T_n \right\rangle $. La forma que Gottesman y Knill plantean es encontrando la solución $(c_1,...,c_n)\in \mathbb{Z}^n_2$ de la ecuación
		%
		\begin{equation} \label{eq: medidaGK}
			\pm Z_a = T^{c_1}_1 \cdot ...  \cdot T^{c_n}_n
		\end{equation}
		
	La ecuación \eqref{eq: medidaGK}  se puede traducir en un sistema de ecuaciones cuya matriz de coeficientes es la matriz estabilizadora del estado:
	%
		\begin{equation} \label{eq: sistema}
			(0\; ... \; 0 \; \overset{(a+n)}{1} \; 0 \; ... \; 0)
			 =
			\left( \begin{array}{ccc} c_1 & \hdots & c_n \end{array} \right) 
			\left(\begin{array}{ccccccc}
			x_{11}  	  & \hdots   &  x_{1n} 	      & \vline & z_{11}       &  \hdots &  z_{1n}    \\
			\vdots  	 & \ddots  &  \vdots	   	 & \vline & \vdots       &  \ddots &  \vdots  	  \\
			x_{n1}  	 & \hdots  &  x_{nn} 	  	& \vline & z_{n1}       & \hdots &  z_{nn}\\ 
			\end{array}\right)		
		\end{equation}
	
%		\begin{equation} \label{eq: sistema}
%			\left( 	\begin{array}{c}
%			0  \\ \vdots \\ 0 \\ \hline \vdots \\ 1 \\ \vdots
%			\end{array}	\right) 
%			=
%			\left(\begin{array}{ccc}
%			x_{11}  	 & \hdots & x_{n1}  \\
%			\vdots 	 	& \ddots & \vdots  \\
%			x_{1n}  	 & \hdots & x_{nn}  \\ \hline
%			z_{11} 		  & \hdots & x_{n1}   \\
%			\vdots 		& \ddots  & \vdots  \\
%			x_{(1n}  & \hdots  & x_{(nn}  \\ 
%			\end{array}\right)	
%			%
%			\left( 	\begin{array}{c}
%			c_1  \\ \vdots  \\ c_n
%			\end{array}	\right) 	
%		\end{equation}
	
	Si \eqref{eq: sistema} tiene solución, nos encontramos en el caso en que $Z_a\in\mathcal{S}$ y no hay que actualizar el sistema. Sin embargo, para averiguar cuál es el signo que acompaña a $Z_a$ (signo del autovalor medido), se ha de resolver el sistema, y esto implica emplear una reducción gaussiana que es un proceso en $O(n^3)$.
	
	En caso contrario, la medida del operador es un proceso aleatorio, de manera que se decide si el autovalor es $+1$ o $-1$ ``lanzando una moneda''. Además, tenemos que actualizar el grupo estabilizador: reemplazamos un generador $T_j$ que anticonmute con $Z_a$ por él, y multiplicamos por $T_j$ el resto de generadores que anticonmuten con $Z_a$. Este es un proceso que emplea $O(n^2)$ operaciones \cite{Aaronson}.
	
	Si sumamos todos los procesos implicados, concluimos que el algoritmo está en la clase $O(n^3)$ y es, por ende, eficiente. Sin embargo, aún podemos mejorarlo.
		
\newpage		
\section{Algoritmo de Aaronson-Gottesman} \label{sec: AaronsonGottesman}

En esta sección, se explica el algoritmo desarrollado por Scott Aaronson y Daniel Gottesman \cite{Aaronson}, que introduce mejoras de eficiencia respecto al que tratábamos anteriormente. La clave para conseguirlo se encuentra en explotar las relaciones de conmutatividad (y anticonmutatividad) que presentan las matrices de Pauli \eqref{eq: Pauli igualdades}. 

En este caso, necesitamos cambiar la representación del sistema, ampliando la matriz estabilizadora. Consideraremos también otros $n$ generadores del ``destabilizador", esto es, operadores que no forman parte del estabilizador pero que, junto con él, generan el grupo de Pauli completo. Además, ahora la matriz también incluye una columna extra para tener en cuenta la fase de cada operador. Para no confundir la notación con todo lo visto anteriormente, notaremos $R_i =\pm P_1...P_n\footnote{Sólo consideramos $\pm1$ para la fase del operador $R_i$ y nos olvidamos de $\pm i$. De lo contrario, se daría el caso en que $R^2_i=-I$, cosa que sabemos imposible para códigos no triviales.} \: (i = 1,...,2n)$ a cada fila (generador) que aparece en esta nueva versión de la matriz.


El algoritmo representa un estado por la matriz de variables binarias $x_{ij}$, $z_{ij}$, $r_{i}$ para cada $i,j \in \{1,...,2n\}$:

\begin{equation} \label{eq: matriz estabilizadora}
\begin{array}{cc}
R_1 &\rightarrow \\ \vdots \\ R_n &\rightarrow \\ R_{n+1} &\rightarrow \\ \vdots \\ R_{2n} &\rightarrow 
\end{array}
%			\begin{array}{cc}
%				R_1  \\ \vdots \\ R_n  \\ R_{n+1}  \\ \vdots \\ R_{2n} 
%			\end{array}
\left(\begin{array}{cccccccccc}
x_{11}  	 & \hdots & x_{1n} 	    	& \vline & z_{11}       & \hdots & z_{1n}  		 & \vline & r_{1}       \\
\vdots 	 	& \ddots & \vdots 	      & \vline & \vdots 	 & \ddots & \vdots 		 & \vline & \vdots	  \\
x_{n1}  	 & \hdots & x_{nn}    	   & \vline & z_{n1}  	   & \hdots & z_{nn}	   & \vline & r_{n}		  \\ \hline
x_{(n+1)1} & \hdots & x_{(n+1)n}   & \vline & z_{(n+1)1} & \hdots & z_{(n+1)n} & \vline & r_{n+1}    \\
\vdots 		& \ddots  & \vdots 		  & \vline & \vdots 	 & \ddots & \vdots 	 	 & \vline & \vdots	   \\
x_{(2n)1}  & \hdots  & x_{(2n)n}    & \vline & z_{(2n)1}  & \hdots & z_{(2n)n} 	 & \vline & r_{2n}		\\ 
\end{array}\right)		
\end{equation}

Los elementos destabilizadores están representados por las filas $\{1,...,n\}$ mientras que los generadores del grupo estabilizador se corresponderán con las filas $\{n+1,...,2n\}$. Las $2n$ primeras entradas en cada fila serán la etiqueta del operador de Pauli correspondiente, mientras que $r_i$ denota la fase que lo acompaña: $r_i=0$ para fases positivas y $r_i=1$ para fases negativas.


Ahora que ya conocemos la nueva matriz estabilizadora, solo nos queda conocer qué hemos de hacer para actualizarla ante de la acción de las posibles puertas de Clifford y de medida que podamos aplicar sobre el sistema.
	
Primero, necesitamos ver cómo implementaremos la composición de operadores (o producto de matrices de dimensión $2^n$) que, recordemos, otorgaba a los operadores de Pauli el atributo de grupo. La denotaremos por el símbolo $+$, pues la composición de dos operadores tiene una etiqueta que recuerda a la suma de etiquetas de los sumandos. Se define, pues, una subrutina encargada de computar la recién mencionada operación:
	%
	\paragraph{rowsum(h,i)} Esta subrutina toma las filas $h$ e $i$ de la matriz estabilizadora (generadores $R_h$ y $R_i$) y reescribe sobre la fila $h$, transformándola en el generador $R_i+R_h$. 
		
	Lo primero que hace es ver qué ocurre con la fase del nuevo generador. Para ello, definimos la función $g:\mathbb{Z}_2^4 \longrightarrow \{-1,0,+1\}$, que acepta como parámetros las etiquetas $(x_1,z_1,x_2,z_2)$ correspondiente a dos matrices de Pauli y devuelve un coeficiente $r$ tal que $i^r$ es la fase resultante cuando dichas matrices han sido multiplicadas. 
	
	Es sencillo comprobar que la forma explícita de esta función será:
		
				\begin{equation}
					g(x_1,z_1,x_2,z_2)=\left\lbrace 
						\begin{aligned}
							0  					\qquad	   \text{si } (x_1,z_1) &= (0,0)      \\
							x_2 (1-2z_2)  \qquad	 \text{si } (x_1,z_1) &= (0,1)		  \\
							z_2 (2x_2-1)  \qquad	 \text{si } (x_1,z_1) &= (1,0) 		  \\
							z_2 - x_2		\qquad	   \text{si } (x_1,z_1) &= (1,1)      
						\end{aligned}
				 	\right. 
				\end{equation}
		
	Gracias a ella, seremos ahora capaces de establecer cuál es el nuevo valor para $r_h$. Nos fijamos en el valor de la siguiente expresión, en la que se reúnen las aportaciones de las fases de cada uno de los generadores y las que ofrecen los $n$ productos de matrices de Pauli que se van a llevar a cabo: 
		
				\begin{equation*}
					d = 2r_h + 2r_i + \displaystyle\sum_{j=1}^n g(x_{ij},z_{ij},x_{hj},z_{hj})
				\end{equation*}
		
	Entonces, asignaremos $r_h:=0$ si $d \equiv 0 \text{ (mod 4)}$ y $r_h:=1$ cuando $d \equiv 2 \text{ (mod 4)}$ (nunca se dará el caso en que $d$ sea congruente a 1 o 3).
		
	Ahora que ya tenemos la fase, podemos modificar las etiquetas que denotan al generador $R_h$. Esta etapa es más sencilla, simplemente tenemos que ejecutar $x_{hj} := x_{ij} \oplus x_{hj}$ $z_{hj} := z_{ij} \oplus z_{hj}$ para cada columna $j\in\{1,...,n\}$, donde la operación $\oplus$ no es más que la suma módulo 2.
			
			
	\subsection{Representación de estados puros}
		
	Llegamos al tan esperado algoritmo mejorado para simular circuitos cuánticos. Comenzamos ilustrando el caso en que lo que hacemos evolucionar es un estado puro, para generalizarlo posteriormente al caso de estados mezcla.
		
	El estado de partida será $\ket{0}^{\otimes n}$, de manera que $S \left( \ket{0}^{\otimes n} \right) = \left\langle Z_1,...,Z_n \right\rangle $. Por tanto, añadiendo una fila auxiliar $R_{2n+1}$ a la matriz (su utilidad se verá más tarde), su configuración inicial vendrá dada por $r_i=0$ para todo $i \in \{1,..,2n+1\}$ y $x_{ij}=\delta_{ij}$, $z_{ij}=\delta_{(i-n)j}$, $\forall i\in\{1,...,2n+1\}$ $\forall j \in \{1,...,n\}$. Por ejemplo, si $n=2$ la matriz estabilizadora en el instante inicial adoptará la forma
			
			\begin{equation} \label{ec: matriz inicial ejemplo}
				\left( 
					\begin{array}{ccccccc}
						1 & 0 & \vline & 0 & 0 & \vline & 0 \\
						0 & 1 & \vline & 0 & 0 & \vline & 0	\\ 		\hline
						0 & 0 & \vline & 1 & 0 & \vline & 0 \\
						0 & 0 & \vline & 0 & 1 & \vline & 0	\\		\hline
						0 & 0 & \vline & 0 & 0 & \vline & 0
					\end{array}
				\right) 
			\end{equation}
		
	Durante la simulación, modificaremos el estado del sistema por medio de puertas de Clifford, que actúan según lo recogido en la Tabla \ref{tab: Estabilizadores Pauli}. ¿Cómo interpretará el algoritmo la acción de cada una de ellas?
			
			\begin{itemize}
				\item \textbf{cNOT sobre $b$ controlado por $a$:} Para cada fila $i \in \{1,...,2n\}$, se tienen $r_i := r_i \oplus x_{ia}z_{ib}(x_{ib} \oplus z_{ia} \oplus 1)$ y $x_{ib}:=x_{ib}\oplus x_{ia}$, $z_{ia} := z_{ia} \oplus z_{ib}$.
				
				\item \textbf{Hadamard sobre el qubit $a$:} Para todo $i \in \{1,...,2n\}$, asignamos $r_i := r_i \oplus x_{ia}z_{ia}$ e intercambiamos $x_{ia}$ y $z_{ia}$.
				
				\item \textbf{Puerta de fase sobre el qubit $a$:} Cambiaremos $r_i := r_i \oplus x_{ia}z_{ia}$ y  $z_{ia} := z_{ia} \oplus x_{ia}$ para todas las filas $i\in\{1,...,2n\}$.
			\end{itemize}	
	
		
	Lo que tiene más enjundia de este algoritmo es el proceso de medida, cuya esencia son las consideraciones que expusimos en \ref{subsubsec: medida}. Comentaremos primero lo que hace el algoritmo y, seguido de ello, daremos la justificación teórica:
		
	\textbf{Medida del qubit $a$ en la base canónica:} Comprobamos si hay algún índice $p \in \{n+1,..., 2n\}$ tal que $x_{pa}=1$. Se abre la puerta a dos posibilidades distintas:
			
			\begin{itemize}[leftmargin=0.6cm]
				
				\item[] \textbf{Caso 1 : tal $p$ existe.} En caso de haber más de uno, tomaremos el mínimo de ellos.
					
					La medida será aleatoria, así que el estado requiere ser actualizado. Para tal fin, llamamos a $\text{rowsum}(i,p)$ para todas las filas $i \in \{1,...,2n\}$ tales que $i\ne p$ y $x_{ia}=1$. Tras ello, guardamos la fila $R_{p}$ en la $R_{p-n}$ y llenamos $R_p$ de ceros, excepto $r_p$ (que tomará los valores 0 o 1 con igual probabilidad) y $z_{pa}=1$. Por último, devolvemos el valor $r_p$ como resultado de la medida.
							
				\item[] \textbf{Caso 2 : no existe tal $p$.}
				
					Ahora la salida será determinista, así que la medida no alterará el estado. Sólo tenemos que ver si se observa 0 o 1. Para ello, rellenamos de ceros la fila $R_{n+1}$ completa y sobre ella, sumamos con $\text{rowsum}(2n+1,i+n)$ todas las filas $R_{i+n}$ con $i\in\{1,...,n\}$ tales que $x_{ia}=1$. Finalizamos devolviendo $r_{2n+1}$ como salida de la medida.
				
			\end{itemize}
		
	\noindent Todo lo anterior parece muy abstracto, pero vamos a ver cómo todo cobra sentido después de varias  apreciaciones:
		
	\noindent \textbf{1. } Empezamos por definir el producto simpléctico pertinente para trabajar en estas situaciones. Recordando la definición \ref{def: producto simplectico}, se entiende que, considerando dos filas de la matriz como $R_i \equiv (x_i,z_i) \in \mathbb{Z}_2^{2n}$, el producto simpléctico de generadores se define como
		
		\begin{equation} \label{def: producto simplectico algoritmo}
			(R_h|R_i) = z_h \cdot x_i - z_i \cdot x_h \; (\text{mod }2) = 
								\displaystyle\bigoplus_{j=1}^n (x_{ij}z_{hj} \oplus x_{hj}z_{ij})
		\end{equation}
		
	De esta definición se deducen un par de resultados importantes:
		
		\begin{lemma} \label{lema: conmutacion matrices}
			Sean $A,B$ dos matrices de Pauli representadas, respectivamente, por las etiquetas $a\equiv(x_a,z_a)$, $b\equiv(x_b,z_b)\in \mathbb{Z}^2_2$. Se verifican entonces las siguientes afirmaciones:
			\begin{enumerate} [label=(\roman*),itemsep=1pt]
				\item $A$ y $B$ conmutan si, y solo si, $(a|b)=0$.
				\item $A$ y $B$ anticonmutan si, y solo si, $(a|b)=1$.
			\end{enumerate}
		\end{lemma}
		
		\begin{proof}
			Nos hallamos en el caso de la definición \ref{def: producto simplectico algoritmo} en que $n=1$.
			Por tanto, el producto simpléctico que nos interesa vendrá dado por $(a|b)=x_az_b\oplus x_bz_a$. Es inmediato, con una prueba por casos (considerando las 16 posibles combinaciones de ceros y unos para la 4-upla $(x_a,z_a,x_b,z_b)$) demostrar las afirmaciones del lema.
		\end{proof}	
		
		\begin{lemma}\label{lema: conmutacion generadores}
			Sean $R_h$ y $R_i$ dos generadores de la matriz estabilizadora. Entonces $R_h$ conmuta con $R_i$ si $(R_h|R_i)=0$, y anticonmuta con él si $(R_h|R_i)=1$.
		\end{lemma}
		
		\begin{proof}
			Sean dos operadores de Pauli $P=i^kP_1...P_n$ y $Q=i^lQ_1...Q_n$ (donde $P_i$ y $Q_i$ son matrices de Pauli). Es claro que $[P,Q]=0$ si, y solo si, el número de índices tales que $\{P_j,Q_j\}=0$ es par. De forma análoga, $\{P,Q\}=0$ si, y solo si, dicho número de índices es impar. A partir de ahora, identificamos $P$ con $R_h$ y $Q$ con $R_i$.
				
			Supongamos que $(R_h|R_i)=0$, esto es, $\bigoplus_{j=1}^n (x_{hj}z_{ij} \oplus x_{ij}z_{hj})=0$. Tenemos entonces que el número de sumandos $(x_{hj}z_{ij} \oplus x_{ij}z_{hj})$ que no se anulan es par (ya que reduciendo módulo 2 se anula). Por el lema \ref{lema: conmutacion matrices}, esto equivale a que el número de parejas $\left( P_j,Q_j \right)$ que anticonmutan es par. Por lo mencionado al comienzo de la demostración, esto equivale, a su vez, a la primera afirmación del lema.
				
			Supongamos ahora que $(R_h|R_i)=1$ y, de forma análoga a lo anterior, deducimos que el número de sumandos $(x_{hj}z_{ij} \oplus x_{ij}z_{hj})$ que resultan 1 ha de ser impar (para que no se anule reduciendo módulo 2). Por tanto, el número de parejas $\left( P_j,Q_j \right)$ que anticonmutan es impar, lo que equivale a que $R_h$ y $R_i$ anticonmuten.
		\end{proof}
		
	\noindent \textbf{2. } Recogemos las relaciones entre los distintos generadores de la matriz en la siguiente
			
		\begin{proposition}\label{prop: proposicion Aaronson}
			Las siguientes afirmaciones sobre la matriz estabilizadora se cumplen siempre:
			\begin{enumerate}[label=(\roman*),itemsep=1pt]
				\item $R_{n+1},...,R_{2n}$ generan $S\left( \ket{\psi} \right)$, y $R_1,...,R_{2n}$ generan $\mathcal{P}_n$.
				\item \label{prop: Aaronson II}Los generadores $R_1,...,R_n$ conmutan entre ellos.
				\item \label{prop: Aaronson III}  $\forall h \in \{1,...,n\}$, $R_h$ anticonmuta con $R_{h+n}$.
				\item \label{prop: Aaronson IV}Para todos $ i,h \in \{1,...,n\}$ tales que $i\ne h$, $R_i$ conmuta con $R_{h+n}$.
			\end{enumerate}	
		\end{proposition}
	
	Es fácil comprobar que estas afirmaciones se cumplen para el estado inicial. Además, el algoritmo fuerza que todas estas afirmaciones sigan siendo verdad  tras cada actualización del grupo estabilizador que hagamos.
		
	\vspace{0.35cm}
	
	\noindent \textbf{3. }  Supongamos que la medida del qubit $a$ da lugar a una salida determinista. Esto ocurría en caso en que $Z_a$ pertenecía al estabilizador. Con la nueva notación (usando $+$ para el producto de operadores), la ecuación \eqref{eq: medidaGK} se convierte en 
	
		\begin{equation}\label{eq: medida Z}
			\displaystyle\sum_{h=1}^n c_h R_{h+n} = \pm Z_a .
		\end{equation}
	
	Nos proponemos averiguar el valor de los coeficientes, ya que sumando los $R_{h+n}$ adecuados, podremos saber si la fase del output es positiva o negativa. Obsérvese que, para todo $i \in \{1,...,n\}$,
	
		\begin{equation}
			c_i \underset{P\ref{prop: proposicion Aaronson}}{\overset{L\ref{lema: conmutacion generadores}}{=}}
			\displaystyle\sum_{h=1}^n c_h ( R_i | R_{h+n} ) =
			\left( R_i \, \vline \displaystyle\sum_{h=1}^n c_h R_{h+n} \right) = (R_i|Z_a)
		\end{equation}
	
	Así, comprobando si $R_i$ anticonmuta con $Z_a$ (cosa que ocurrirá si y solo si $x_{ia}=1$, puesto que esto denotará que $R_i$ tiene una matriz $X$ o $Y$ en la posición $a$), podemos saber si $c_i=1$ y, por tanto, si rowsum$(2n+1,i+n)$ ha de ser llamada.
		
	\vspace{0.35cm}
	
	\noindent \textbf{4. }  Si nos ponemos en el caso en que $Z_a$ no está en el estabilizador, multiplicamos algunos operadores por uno que anticonmuta con él. El siguiente lema aclara cómo altera esta operación las relaciones de conmutatividad de los distintos operadores:
	
		\begin{lemma} \label{lema: conmutacion producto}
			Sean $A$, $B$ y $C$ tres operadores de Pauli sobre un mismo espacio de Hilbert. Entonces: \\
			%\begin{enumerate}[label=(\alph*),itemsep=1pt]
				\indent (a) Si A y C conmutan, $[A,B]=0$ si y solo si $[A,BC]=0$. \\
				\indent (b) Si A y C anticonmutan, $[A,B]=0$ si y solo si $\{A,BC\}=0$.
			%\end{enumerate}
		\end{lemma}	
	
		\begin{proof}
			Comenzamos con el caso (a), suponiendo que $AC=CA$: 
			\begin{center}
				$AB=BA \quad \Rightarrow \quad A(BC)=BAC=(BC)A  \quad \Rightarrow \quad [A,BC]=0 \newline$
				$A(BC)=(BC)A \: \Rightarrow \: A(BC)C=(BC)AC \: \Rightarrow \: AB=BCCA=BA \: \Rightarrow \: [A,B]=0$
			\end{center}
			Vamos con (b), suponiendo que $AC=-CA$:
			\begin{center}
				$AB=BA \quad \Rightarrow \quad A(BC)=BAC=-(BC)A  \quad \Rightarrow \quad \{A,BC\}=0 \newline$
				$A(BC)=-(BC)A \: \Rightarrow \: A(BC)C=-(BC)AC \: \Rightarrow \: AB=-BC(-CA)=BA \: \Rightarrow \: [A,B]=0$
			\end{center}
			Tratándose de matrices de Pauli, sabemos que si no conmutan, entonces anticonmutan. Así, pueden deducirse nuevas implicaciones sin más que negar alguna de las tesis.
		\end{proof}
		
 \vspace{0.2cm}
		
	Ya disponemos de todos los ingredientes necesarios para comprender lo que hacemos al medir el qubit $a$, es decir, al medir el operador $Z_a$: 
	\begin{itemize}[leftmargin=0cm]	
	\item[] \textbf{Caso 1}. Al tomar un $p\in\{n+1,...,2n\}$ tal que $x_{pa}=1$, estamos seleccionando un generador del estabilizador, $R_p$, que anticonmuta con $Z_a$. Al ``sumar'' este generador con el resto de generadores $R_i \: (i\in \{1,...,2n\})$  que también anticonmutan con $Z_a$ (los que cumplen $x_{ia}=1$), conseguimos que los correspondientes productos conmuten con $Z_a$. Llevamos, finalmente, el generador $R_p$, que anticonmuta con $Z_a$, al destabilizador (posición $p-n$) y registramos $Z_a$ como el generador $R_p$ del estabilizador.
	 
	Se tiene entonces que $R_p=Z_a$ conmuta con todas los generadores de la matriz salvo con $R_{p-n}$, cumpliéndose entonces \ref{prop: Aaronson III} y \ref{prop: Aaronson IV} en la proposición \ref{prop: proposicion Aaronson}. Por su parte, el lema \ref{lema: conmutacion producto} garantiza que se verifique \ref{prop: Aaronson II}.
		
	\item[] \textbf{Caso 2}. Al no tener ningún $x_{pa}=1$, $Z_a$ conmuta con todos los generadores del estabilizador y, de ahí,  $+Z_a$ o $-Z_a$ pertenece a él (aunque no tiene por qué ser necesariamente unos de los generadores). Sumamos aquellos generadores del estabilizador correspondientes a los generadores del destabilizador que anticonmutan con $Z_a$. Con ello, estamos computando la suma \eqref{eq: medida Z} de aquellos términos con $c_h=1$. De este modo, sabiendo el signo de la fase en el primer miembro de la igualdad, seremos conocedores del signo que aparece en el segundo miembro: un signo $+$ significará que la salida es el estado $\ket{0}$ y $-$ dará a entender que el output es $\ket{1}$.
	
	\end{itemize}
 	
 	\vspace{.3cm}
	
	En las explicaciones para justificar la medida en el algoritmo, descubrimos estaba diseñada para verificar las condiciones de la proposición \ref{prop: proposicion Aaronson}. ¿Qué ocurre con ellas cuando aplicamos una puerta de Clifford $U$? Veamos que conserva conmutatividad:
		\begin{equation*}
			AB=BA \: \Leftrightarrow \: UABU^\dagger = U BA U^\dagger  \: \Leftrightarrow \: (UAU^\dagger)( UBU^\dagger) = (UBU^\dagger)( UAU^\dagger)
		\end{equation*}	
	Cuando aplicamos una puerta sobre el sistema, esta se traduce en la acción por conjugación sobre todos los operadores. Por tanto, estos seguirán conservando las relaciones de conmutatividad previas a su aplicación.	
	
	
		
	\subsection{Representación de estados mezcla}
		
	Si recordamos lo visto en \ref{subsubsec: densidad}, la matriz densidad supone una generalización de los estados puros. En este apartado, buscamos adaptar el algoritmo para un cierto tipo de estados mezcla: los \textbf{estados mezcla estabilizadores}, es decir, estados concebidos como una distribución uniforme sobre todos los estados que se encuentran en el código $\mathcal{V}$, un subespacio estabilizado por un grupo descrito a partir de $r<n$ generadores. Sabemos, por \cite{Bermejo VandenNest}, que el código asociado tendrá dimensión $|\mathcal{V}|=\frac{\textfrak{g}}{|\mathcal{S}|}=2^{n-r}$.
		
	Será necesario conocer la forma en que se escribe la matriz densidad de un estado mezcla en términos de su estabilizador. Si $M$ es un operador de Pauli, $\frac{I+M}{2}$ resulta ser el proyector sobre el espacio propio de $M$ asociado al autovalor $+1$. Por tanto, la matriz densidad que representa a un estado mezcla estabilizador (estando su estabilizador generado por lo operadores $M_1,...,M_r$) vendrá dada por \footnote{Como la traza de un producto es el producto de las trazas (tanto para el producto usual de matrices como para el tensorial), es sencillo ver que se cumple que $\text{Tr}(\rho)=1$. Además, $\rho$ es un operador autoadjunto por serlo las matrices de Pauli, y de ahí, cada $I+M_i$. Así, podemos afirmar que $\rho$ es una matriz densidad.}
		%
			\begin{equation} \label{ec: matriz densidad}
				\rho = \frac{1}{2^r} \displaystyle\prod_{i=1}^r \left( I+M_i \right) 
			\end{equation}
	
	En definitiva, la matriz densidad del sistema resulta ser el proyector sobre su código estabilizador. Un ejemplo nos ayudará a comprenderlo mejor. Tomemos el código estabilizador que usamos en el ejemplo \nameref{ej: QEC}. En ese caso, $\mathcal{V}$ era un subespacio vectorial generado por la base $\{\ket{000},\ket{111}\}$ y estaba estabilizado por el grupo $\left\langle Z_1Z_2,Z_2Z_3 \right\rangle $.
	
	Calculando la matriz densidad como la distribución uniforme sobre los estados generadores, $\frac{1}{2}\left( \op{000}{000}+ \op{111}{111}\right)$, obtenemos (en la base de estados básicos para $n=3$) una matriz llena de ceros salvo en la primera y la última posición de la diagonal, que tienen un 1. Si hacemos los cálculos según la expresión \eqref{ec: matriz densidad}, el resultado coincide con lo anterior:
		\begin{equation*}
			\begin{aligned}
				\rho &= \frac{1}{2^2} (I+Z_1 \otimes Z_2 \otimes I_3)(I+I_1 \otimes Z_2 \otimes Z_3)\\
						&= \text{diag}(1,1,0,0,0,0,1,1)\cdot \text{diag}(1,0,0,1,1,0,0,1) = \text{diag}(1,0,0,0,0,0,0,1) 
			\end{aligned}
		\end{equation*}
	
	Demos otro ejemplo que sirve para generar infinidad de matrices densidad. Para estados mezcla, podemos decir que un estado (o sistema) es separable cuando su matriz densidad se puede expresar como producto tensorial de las matrices densidad de cada subsistema (cada qubit). Cada subsistema admite una representación muy sencilla: los estados básicos $\ket{0}$ y $\ket{1}$ tendrán a $\op{0}{0}$ y $\op{1}{1}$ por matrices densidad, mientras que el estado de máxima mezcla está asociado a la matriz $\frac{1}{2}\left( \op{0}{0}+\op{1}{1} \right) =\frac{I}{2}$.
	
	Tomamos, como ejemplo, un sistema de $n=7$ qubits que se encuentra inicialmente en estado separable. Los primeros 3 qubits estáran en estado $\ket{0}$, el cuarto y el quinto en estado de máxima mezcla y los dos últimos en estado $\ket{1}$. Entonces, su matriz densidad vendrá dada por
		\begin{equation*}
			\begin{aligned}
				\rho &= \op{0}{0} \otimes \op{0}{0} \otimes \op{0}{0} \otimes \frac{I_4}{2} \otimes \frac{I_5}{2} \otimes \op{1}{1} \otimes \op{1}{1} \\
				&= \op{000}{000} \otimes \frac{I_4}{2} \otimes \frac{I_5}{2} \otimes \op{11}{11}
			\end{aligned}
		\end{equation*}
	
	Para llevar a cabo nuestra simulación, situamos los $r$ generadores del estabilizador en las filas $n+1,...,n+r$ y sus correspondientes generadores destabilizadores como las filas $1,...,r$. Las $2(n-r)$ filas restantes se rellenan con una colección de operadores $\overline{X_i}$ y $\overline{Z_i}$, que conmutan tanto con el estabilizador como con el destabilizador. Elegimos estos operadores de forma que cumplan las siguientes relaciones de conmutatividad: $[\overline{X_i},\overline{X_j}]=[\overline{Z_i},\overline{Z_j}]=[\overline{X_i},\overline{Z_j}]=0$ si $i\ne j$, y $\{\overline{X_i},\overline{Z_i}\}=0$\footnote{Se entiende que la notación no ha sido arbitraria: los operadores $\overline{X_i}$ y $\overline{Z_i}$ mantienen las mismas relaciones de conmutatividad que las que presentan $X_i$ y $Z_i$}. Colocaremos $\overline{X_i}$ en las filas $r+i$ y $\overline{Z_i}$ en las filas $n+r+i$, para todo $i \in \{1,...,n-r\}$.

	Se empezará la simulación desde el estado mezcla inicial $\op{0...0}{0...0}\otimes \frac{I_{n-r+1}}{2} \otimes ... \otimes \frac{I_{n}}{2}$ ($0$ en los $n-r$ primeros qubits y el estado completamente mezclado en los últimos $r$ qubits). En tal caso, se eligen los operadores $\overline{X_i}=X_{i+r}$ y $\overline{Z_i}=Z_{i+r}$. Notaremos $\overline{i}=i-n$ si $i\ge n+1$,  e $\;\overline{i}=i+n\;$ si $i\le n$. En tal caso, la proposición \ref{prop: proposicion Aaronson} se traducirá en que las filas $R_i$ y $R_j$ conmutarán a no ser que $i = \overline{j}$, en cuyo caso anticonmutarán. La matriz estabilizadora correspondiente a este estado mezcla sería una ``matriz identidad'' como la que usamos para estados puros.
		
	Pasemos a estudiar el algoritmo en esta situación algo más compleja. Con respecto a la acción de puertas unitarias, podemos decir que se mantiene todo lo dicho anteriormente para los estados puros. Lo que sí cambia es la $medida$ de un qubit $a$, que resulta algo más difícil, apareciendo ahora tres casos distintos:
		
			\begin{itemize}[leftmargin=0.6cm]
				
				\item[] \textbf{Caso I: } $x_{pa}=1$ para algún $p \in \{n+1,...,n+r\}$. 
				
					Lo que ocurre es que $Z_a$ anticonmuta con (al menos) un elemento del estabilizador, de forma que la medida es aleatoria. Actualizamos el estado de igual forma que en el Caso I de estados puros.
					
				\item[] \textbf{Caso II: } $x_pa=0$ en todas las filas $p>r$.
				
					Ahora $Z_a$ conmuta con todo el estabilizador y es, de hecho, un elemento suyo. El output de la medida tendrá un resultado determinista, que averiguamos como hicimos en el Caso II para estados puros, sumando todas las filas que anticonmutan con $Z_a$. La medida no altera el estado del sistema, dejándolo dentro del código $\mathcal{V}$. 
					
				\item[] \textbf{Caso III: } $x_{pa}=0$ para todo $p\in \{n+1,...,n+r\}$, pero $x_{ma}=1$ para algún $m\in \{r+1,...,n\}\bigcup\{n+r+1,...,2n\}$. 
				
					Ahora $Z_a$ conmuta con todos los generadores del estabilizador, pero no forma parte de él. Por ello, el estado cambia pero se queda dentro del código estabilizador. La medida da lugar a un resultado aleatorio, pero la forma de actualizar el estado del sistema difiere respecto a lo que hacemos en el Caso I:
					
					$R_m$ anticonmuta con $Z_a$ y por ello, tomará el papel de $R_p$ en el Caso I: guardamos la fila $R_m$ en la fila $R_{\overline{m}}$ y asignamos a la fila $m$ las etiquetas correspondientes a $Z_a$, dejando el valor de la fase $r_m$ en manos del azar. Una vez hemos hecho esto, intercambiamos las filas $R_{n+r+1}$ y $R_m$, y también las filas $R_{r+1}$ y $R_{\overline{m}}$. Por último, lo que antes era $r$ pasa a ser $r+1$: el estabilizador ha ganado un nuevo generador, $R_{n+r+1} \equiv Z_a$, y el correspondiente destabilizador $R_{r+1}$ (el antiguo $R_m$) anticonmuta con él.
					
					Se sigue cumpliendo lo que vimos en \ref{sec: GottesmanKnill}: a cada paso, el nuevo grupo estabilizador es $\mathcal{S}'=\left\langle \pm Z_a, \mathcal{C}_\mathcal{S}(Z_a) \right\rangle$, solo que ahora $\mathcal{C}_\mathcal{S}(Z_a)$ coincide con el estabilizador previo, $\mathcal{S}$. Es evidente que el número de medidas que podemos hacer está limitado, ya que a partir de la $(n-r)$-ésima medida, el estabilizador ya tendría los $n$ generadores que le permitimos tener como máximo.
			\end{itemize}
		
	\noindent Estudiando el algoritmo, nos damos cuenta de que está diseñado para que se verifiquen continuamente las condiciones que da la proposición \ref{prop: proposicion Aaronson}, también para esta situación. Garantizamos así que el tanto el grupo estabilizador como el grupo de Pauli estén correctamente generados por las correspondiente filas de la matriz.
		
	\subsection{Eficiencia en el algoritmo de Aaronson-Gottesman}
		\label{subsec: eficiencia AaronsonGottesman}
	
	Hemos repetido varias veces que este algoritmo suponía una mejora respecto al de Gottesman-Knill en términos de eficiencia. ¿A qué se debe?
	
	El proceso más costoso en el ``viejo'' algoritmo era la medida de un qubit cuando ésta era determinista: la eliminación gaussiana que necesitábamos para resolver el sistema asociado era un proceso de $O(n^3)$. Ahora, se ha propuesto un algoritmo que se ahorra este paso: aprovechando el producto simpléctico de etiquetas para detectar las relaciones de conmutatividad bajamos el número de operaciones a $O(n^2)$ \cite{Aaronson}. Este nuevo algoritmo iguala, pues, el tiempo para medir un qubit en cualquiera de los casos: no importa si la medida es determinista o aleatoria, ambas tomarán un tiempo $O(n^2)$.
	
	En su \textit{paper}, Aaronson y Gottesman completan este estudio sobre la eficiencia poniendo a prueba su algoritmo con un programa en C que implementa las cuatro operaciones fundamentales que están permitidas en los circuitos estabilizadores: cNOT, Hadamard, fase y medida. El experimento que realizan, simulando un sistema de $n$ qubits, es el siguiente:
	
		\begin{center}
			\textit{Fijado un parámetro $\beta>0$, se seleccionan aleatoriamente $\lfloor \beta n \log_2 n \rfloor$ puertas del conjunto \{cNOT, H, P\}, aplicándose a un(dos) qubit(s) aleatorio(s) en $\{1,...,n\}$. Una vez aplicadas todas las puertas, se miden todos los qubits desde el primero hasta el $n$-ésimo por orden.}
		\end{center}
	
	En la figura \ref{fig: Aaronson} se muestra la comparativa de los tiempos de ejecución como función de $n$ para distintos valores de $\beta$.
	
		\begin{figure}[h!]
			\centering
			\includegraphics[width=8cm]{medium.png}				
			\caption{ Tiempo medio para simular la medida de un qubit tras aplicar $\lfloor \beta n \log_2 n \rfloor$ puertas sobre $n$ qubits, en un Pentium III de 650MHz con 256MB de RAM \cite{Aaronson}.}
			 \label{fig: Aaronson}
		\end{figure}
	
	¿Cómo interpretar los resultados de dicha gráfica? Vemos que para $\beta$ bajos, el tiempo de medida es lineal con el número de qubits, pero que las curvas se van transformando en parábolas a medida que crece $\beta$. Esto es debido a que este tiempo aumenta a medida que tenemos que llamar más veces a la subrutina \textit{rowsum}: en el estado inicial no hay generadores de $\mathcal{S}$ tales que $x_{ia}=1$; pero conforme vamos aplicando más y más puertas de Clifford, hay más operadores que cumplen dicha condición. Como un valor de $\beta$ grande eleva el número de puertas aplicadas, la medida será también más trabajosa. Se termina, pues, concluyendo que el tiempo por medida se encontrará entre las dependencias lineal y  cuadrática, sin poder dar información más detallada.
	
	\vspace{0.35cm}
	
	No obstante, esta mejora tiene un precio. Para poder desarrollar la medida como sugieren estos autores, necesitamos considerar $n$ filas más en la matriz estabilizadora (más otra auxiliar), además de la columna que tiene en cuenta la fase de cada generador. Esto conlleva un aumento de la memoria ocupada: la matriz primitiva tenía dimensión $n \times 2n$, ocupando $2n^2$ bits; por su parte, la nueva versión es una matriz $(2n+1)\times(2n+1)\approx4n^2$.
	
	Este incremento de la memoria en un factor 2 es más que asumible, teniendo en cuenta que, de no querer asumirlo, tendríamos que lidiar con un tiempo que aumenta en un factor $n$, como impone el  antiguo algoritmo. Pensemos en la simulación de un sistema de 3000 qubits: $4\cdot 3000^2 = 36\cdot10^6\, \text{bits} \approx 4.25\, \text{MB}$ es una memoria insignificante respecto a un ordenador de mesa, mientras que un proceso 3000 veces más rápido sí supone una mejora enorme. 
	
	Un último apunte que \cite{Aaronson} da sobre la eficiencia de su algoritmo tiene que ver con la preparación del estado inicial, esto es, con la matriz estabilizadora de partida:
	
	La representación de estados puros no necesita ningún cálculo adicional: sólo necesitamos introducir las etiquetas, ya conocidas, de los generadores que conforman el grupo estabilizador. En cambio, para la representación de estados mezcla estabilizadores, se han de calcular los $2(n-r)$  operadores $\overline{X_i}$, $\overline{Z_i}$, que acompañan a los elementos estabilizadores y destabilizadores en la matriz. Para ello, se resuelve un sistema de ecuaciones que recoge las relaciones de conmutatividad que existen entre todas las filas de la matriz. Este es un proceso que tarda un tiempo $O(n^3)$.
	
	En definitiva, la representación de estados puros es un proceso $O(n^2)$ en su conjunto, mientras que la de estados mezcla sube sus tiempos a $O(n^3)$. De todas formas, este segundo proceso no estaba siquiera contemplado por el algoritmo de Gottesman-Knill, por lo que el algoritmo de Aaronson-Gottesman supone ganancias, se mire por donde se mire.


\newpage
\section{Conclusiones}

Este trabajo constituye una buena forma de ver cómo Física y Matemática son ciencias inseparables: para adentrarnos en la simulación clásica de circuitos cuánticos, nos vimos en la necesidad de recurrir a viejos conocidos como son la teoría de grupos o los espacios de Hilbert, pero también de aprender estructuras nuevas para un estudiante de Matemáticas como son los tensores o el producto simpléctico. 

Aunque uno podría pensar en la teoría de grupos como un campo demasiado abstracto y puro para buscar aplicaciones, ésta ha demostrado ser una gran aliada en el estudio de las simulaciones: no sólo permite formular una representación completamente nueva de ciertos estados cuánticos, sino que también aparece a la hora de mejorar la eficiencia de los algoritmos estudiados.

Fue en esta mejora donde brilló el producto simpléctico. Éste nos otorgaba una manera muy eficiente de comprobar las relaciones de conmutatividad entre operadores, que sería el motivo principal para que el algoritmo de Aaronson-Gottesman \cite{Aaronson} se impusiera sobre el algoritmo de Gottesman-Knill\cite{NielsenChuang}, pues se pasa de una complejidad $O(n^3)$ a $O(n^2)$ al implementar las novedades.

Estos pequeños avances nos permiten hacernos una idea del tipo de cosas en las que se trabaja en el ámbito de la Computación Cuántica: con un breve vistazo a algunos artículos de la bibliografía, se aprecia que estas herramientas suponen la clave de algunas investigaciones bastante actuales. En publicaciones más contemporáneas que las aquí revisadas, como \cite{Vitch} y \cite{Raussendorf}, se emplean funciones de quasi-probabilidad y representaciones nuevas para conseguir simulaciones eficientes de estados aún más generales.

En definitiva, simulando clásicamente los sistemas cuánticos, podemos entender el futuro desempeño de los ordenadores cuánticos, y esto es un paso necesario antes de plantearse la construcción de los mismos.
 
Sin embargo, puede que llegue el día en que los ordenadores cuánticos sobrepasen con creces las capacidades de cualquier ordenador clásico. Es por ello que se debe plantear un debate acerca de qué se debe y qué no se debe hacer con estas, a priori, potentes máquinas. Las posibilidades de un poder computacional aún mayor que el actual son muy diversas: desde mejoras en la comunicación y en la cyberseguridad hasta la resolución de cálculos de complejidad inimaginable a día de hoy. 

Como todo avance precedente, habrá aplicaciones que conlleven nuevos descubrimientos en Ciencia, fines militares e incluso repercusiones en nuestro día a día; de ahí que tengamos que evitar que se convierta en una amenaza y quedarnos con sus bondades, que prometen ser muchas.







% Referencias %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\addcontentsline{toc}{section}{Referencias} % Elige según idioma
%\addcontentsline{toc}{section}{References} % Elige según idioma

\begin{thebibliography}{100}
	
	\bibitem{NielsenChuang}
		N. Nielsen, I.L. Chuang, \\
		{\em Quantum Computation and Quantum Information}, Cambridge U. P., Cambridge, 2010

	\bibitem{Aaronson}
		S. Aaronson, D. Gottesman, \\
		{\em Improved Simulation of Stabilizer Circuits}, Phys. Rev. A 70, 052328 (2004), \\
		arXiv: quant-ph/0406196
		
	\bibitem{Grupos}
		D. Dummit, R. Foote, \\    	
		{\em Abstract algebra} (3rd ed.), John Wiley \& Sons, 2004

	\bibitem{Hilbert}
		F.J. Pérez González, \\    	
		{\em Análisis Funcional en Espacios de Banach}, Departamento de Análisis Matemático, \\
		Universidad de Granada (2019).\\
		\url{https://www.ugr.es/~fjperez/textos/Analisis_Funcional_en_Espacios_de_Banach.pdf}
	
	\bibitem{Dirac}
		R.B. Griffiths,  \\
		{\em Consistent Quantum Theory}. Cambridge University Press, 2002.
		
	\bibitem{deGosson}
		M.A. de Gosson, \\
		{\em Symplectic methods in Harmonic Analysis and in Mathematical Physics}. \\
		26 Birkhauser, Basel 2011.

	\bibitem{AlgebraII}
		J. Pérez Muñoz, \\    	
		{\em Álgebra Lineal y Geometría II}, Departamento de Geometría y Geometría, \\
		Universidad de Granada (2020)
		
	\bibitem{Tensores}
		W. Hackbusch, \\    	
		{\em Tensor Spaces and Numerical Tensor Calculus},  \\
		Springer International Publishing, 2019. 

	\bibitem{Kronecker}
		B.J. Broxson, \\    	
		{\em The Kronecker Product} , UNF Graduate Theses and Dissertations 25, 2006. \\
		\url{https://digitalcommons.unf.edu/etd/25}	

	\bibitem{Zettili}
		N. Zettili, \\
		{\em Quantum Mechanics: Concepts and Applications} (2nd Revised ed.), Wiley, 2009

	\bibitem{Sakurai}	
		J. Sakurai, J. Napolitano, \\
		{\em Modern quantum mechanics} (2nd ed.). Addison Wesley, 2011			
		
	\bibitem{Bermejo VandenNest}
		J. Bermejo-Vega, M. Van den Nest, \\
		{\em Classical simulations of Abelian-group normalizer circuits with intermediate \\ measurements}, Quantum Information \& Computation, Volume 14, Issue 3-4, \\
		Marzo 2014, arXiv: 1210.3637 [quant-ph]		
	
	\bibitem{Entanglement I}
		G. Tóth, O. Gühne, \\    	
		{\em Entanglement detection in the stabilizer formalism}, Phys. Rev. A 72, 022340 (2005), \\
		arXiv: quant-ph/0501020
		
	\bibitem{Teleportacion}
		C.H. Bennett, G. Brassard, C.Crépeau, R. Jozsa, A. Peres, W.K. Wootters,   \\    	
		{\em Teleporting an unknown quantum state via dual classical and \\
			Einstein-Podolsky-Rosen channels}, Phys. Rev. Lett. 70, 1895 (1993), \\
		DOI: \url{https://doi.org/10.1103/PhysRevLett.70.1895}	

	\bibitem{QEC}
		S.J. Devitt, K. Nemoto, W.J. Munro,   \\    	
		{\em Quantum Error Correction for Beginners}, Rep. Prog. Phys. 76 (2013) 076001, \\
		arXiv: 0905.2794 [quant-ph]
		
	\bibitem{QEC II}
		T. Brun, I. Devetak, M. Hsieh,   \\    	
		{\em Correcting Quantum Errors with Entanglement}, Science 314, 436-439 (2006), \\
		arXiv: quant-ph/0610092
		
%	\bibitem{QEC III}
%		K. Fujii,   \\    	
%		{\em Correcting Quantum Computation with Topological Codes: From Qubit to \\
%		 Topological Fault-Tolerance },  1st ed. 2015., Springer Singapore, 2015.
		
	\bibitem{YouTube}
		Artur Ekert, \\
		{\em Introduction to Quantum Information Science} (Lectures) \\
		\url{https://www.youtube.com/c/ArturEkert/featured}

	\bibitem{Gottesman}	
		D. Gottesman, \\
		{\em Theory of fault-tolerant quantum computation}, Phys. Rev. A, 57 (1). pp. 127-137 \\
		DOI: \url{https://doi.org/10.1103/PhysRevA.57.127}			
	
	\bibitem{VandenNest}
		J. Bermejo-Vega, M. Van den Nest, \\
		{\em Classical simulations of Abelian-group normalizer circuits with intermediate\\ measurements},  Quantum Information \& Computation, Volume 13, Issue 11-12,\\ Septiembre 2018, arXiv:1201.4867 [quant-ph]
			
	\bibitem{qcircuit}
		B.Eastin, S.T. Flammia,  \\    	
		{\em qcircuit 2.6.0 Tutorial} , Department of Physics and Astronomy, \\
		University of New Mexico.
		
	\bibitem{Vitch}	
		V. Vitch, C. Ferrie, D. Gross, J. Emerson, \\
		{\em Negative Quasi-Probability as a Resource for Quantum Computation}, \\
		New J. Phys. 14, 113011 (2012), arXiv: 1201.1256 [quant-ph]
		
	\bibitem{Raussendorf}
		R. Raussendorf, J. Bermejo-Vega, E. Tyhurst, C. Okay,  M. Zurel, \\
		{\em Phase space simulation method for quantum computation with magic states on qubits}, \\ 
		Phys. Rev. A 101, 012350 (2020), arXiv:1905.05374 [quant-ph]	
 
\end{thebibliography}


%%%%%%%% APÉNDICES %%%%%%%%%%%%%%%%%%%5
\newpage
\appendix





\end{document}
